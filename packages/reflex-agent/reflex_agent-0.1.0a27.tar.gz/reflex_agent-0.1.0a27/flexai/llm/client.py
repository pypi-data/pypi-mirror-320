from __future__ import annotations

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import AsyncGenerator

from flexai.message import AIMessage, Message, SystemMessage
from flexai.tool import Tool


@dataclass(frozen=True)
class Client(ABC):
    """Abstract base class for language model clients.

    Defines the interface for interacting with various language models.
    Subclasses should implement the necessary methods for specific LLM providers.
    """

    @abstractmethod
    async def get_chat_response(
        self,
        messages: list[Message],
        system: str | SystemMessage = "",
        tools: list[Tool] | None = None,
    ) -> AIMessage:
        """Retrieve a response from the chat model.

        Args:
            messages: Conversation history to send to the model.
            system: Optional system message to set the behavior of the AI.
            tools: Optional list of tools available for the model to use.

        Returns:
            A list of AI-generated messages in response to the input.
        """

    @abstractmethod
    async def stream_chat_response(
        self,
        messages: list[Message],
        system: str | SystemMessage = "",
        tools: list[Tool] | None = None,
    ) -> AsyncGenerator[str | AIMessage, None]:  # type: ignore
        """Stream the response from the chat model in real-time.

        Args:
            messages: Conversation history to send to the model.
            system: Optional system message to set the behavior of the AI.
            tools: Optional list of tools available for the model to use.

        Yields:
            AI-generated messages as they are generated by the model.
        """

    async def stream_chat_lines(
        self,
        messages: list[Message],
        system: str | SystemMessage = "",
    ) -> AsyncGenerator[AIMessage, None]:
        """Stream the response from the chat model line by line.

        This method processes the chat response and yields complete lines of text.
        It buffers incoming content until a full line is available, then yields it.

        Args:
            messages: Conversation history to send to the model.
            system: Optional system message to set the behavior of the AI.

        Yields:
            An async generator yielding complete lines of the AI-generated response.

        Note:
            This method relies on the `stream_chat_response` method, which should be implemented by subclasses.
        """
        buffer = ""
        async for delta in self.stream_chat_response(messages, system=system):  # type: ignore
            # Ignore interim responses.
            if isinstance(delta, str):
                continue
            buffer += delta.content
            while "\n" in buffer:
                line, buffer = buffer.split("\n", 1)
                yield AIMessage(content=f"{line}\n")
