def help():
    print(''' q1() -  код, q1_th() - теория
    q1 - Наивное умножение матрицы на вектор и умножение матриц
    q2 - Иерархия памяти, план кеша и LRU, промахи в обращении к кешу
    q3 - Алгоритм Штрассена
    q4 - Собственные векторы, собственные значения (важность, Google PageRank)
    q5 - Разложение Шура и QR-алгоритм
    q6 - Степенной метод
    q7 - Круги Гершгорина
    q8 - Разложение Шура, теорема Шура
    q9 - Нормальные матрицы, эрмитовы матрицы, унитарно диагонализуемые матрицы, верхне-гессенбергова форма матриц
    q10 - Спектр и псевдоспектр
    q11 - Неявный QR алгоритм (со сдвигами)
    q12 - Алгоритм на основе стратегии "разделяй и властвуй"
    q13 - Разреженные матрицы, форматы хранения разреженных матриц, прямые методы для решения больших разреженных систем
    q14 - Обыкновенные дифференциальные уравнения, задача Коши
    q15 - Локальная, глобальная ошибки
    q16 - Метод центральной разности
    q17 - Метод Эйлера
    q18 - Метод предиктора-корректора
    q19 - Метод Рунге-Кутты 1-4 порядков
    q20 - Методы Адамса-Мултона, методы Адамса-Бэшфорта
    q21 - Метод Милна
    q22 - Согласованность, устойчивость, сходимость, условия устойчивости
    q23 - Моделирование волны с использованием математических инструментов (амплитуда, период, длина волны, частота, Герц, дискретизация, частота дискретизации, фаза, угловая частота)
    q24 - Дискретное преобразование Фурье, обратное дискретное преобразование Фурье их ограничения, симметрии в дискретном преобразовании Фурье
    q25 - Быстрое преобразование Фурье, его принципы, фильтрация сигнала с использованием быстрого преобразования Фурье
    q26 - Операции свёртки, связь с быстрым преобразованием Фурье, операции дискретной свёртки
    q27 - Дискретная свёртка и Тёплицевы матрицы (Ганкелевы матрицы)
    q28 - Циркулянтные матрицы. Матрицы Фурье.
    q29 - Быстрый матвек с циркулянтом''')

def q29_th(): 
    print('''
    Для проведения быстрого матвека может понадобится следующее важное свойство Циркулянта, связывающее его с матрицей Фурье:

$$C = \frac{1}{n} F_n^* diag(F_n \cdot c) F_n$$

$$(c - \text{столбец матрицы} \  C)$$

Для **быстрого матвека** с циркулянтом:

1. Вложим Теплицеву матрицу, построенную по необходимому вектору, в циркулянт:

$$C = \begin{pmatrix} c_0 & c_{-1} & c_{-2} & c_2 & c_1 \\ c_1 & c_0 & c_{-1} & c_{-2} & c_2 \\ c_2 & c_1 & c_0 & c_{-1} & c_{-2} \\ c_{-2} & c_2 & c_1 & c_0 & c_{-1} \\ c_{-1} & c_{-2} & c_2 & c_1 & c_0 \end{pmatrix}$$

2. Получаем произведение:

$$\begin{pmatrix} y_1 \\ y_2 \\ y_3 \\ * \\ * \end{pmatrix} = \begin{pmatrix} c_0 & c_{-1} & c_{-2} & c_2 & c_1 \\ c_1 & c_0 & c_{-1} & c_{-2} & c_2 \\ c_2 & c_1 & c_0 & c_{-1} & c_{-2} \\ c_{-2} & c_2 & c_1 & c_0 & c_{-1} \\ c_{-1} & c_{-2} & c_2 & c_1 & c_0 \end{pmatrix} \cdot \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ 0 \\ 0 \end{pmatrix}$$

3. Умножение на теплицеву => умножение на циркулянт. Из связи циркулянта с преобразованием Фурье получаем ($\circ$ - поэлементное множение):

$$\begin{pmatrix} y_1 \\ y_2 \\ y_3 \\ * \\ * \end{pmatrix} = ifft\bigg( fft(\begin{pmatrix} c_0 \\ c_1 \\ c_2 \\ c_{-2} \\ c_{-1} \end{pmatrix}) \circ fft(\begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ 0 \\ 0 \end{pmatrix}) \bigg)$$''')
def q28_th(): 
    print('''
    **Циркулянт** или **циркулянтная матрица** — это матрица вида $$C = \begin{pmatrix} c_1 & c_n & ... & c_2 \\ c_2 & c_1 & ... & c_3 \\ \vdots & \vdots & ... & \vdots \\ c_n & c_{n - 1} & ... & c_1 \end{pmatrix}$$ Т.е. матрица, в которой любая следующая строка (столбец), начиная с первой (с первого) получается циклической алфавитной перестановкой элементов предыдущей строки (столбца).

**Матрица Фурье**

Дискретное преобразование Фурье является линейным преобразованием, которое переводит вектор временных отсчётов в вектор спектральных отсчётов той же длины. Таким образом преобразование может быть реализовано как умножение симметричной квадратной матрицы на вектор:

$$X = \mathcal{F}x$$

Где $\mathcal{F}$ - **матрица Фурье**

**Матрица Фурье** задаётся следующим образом: $\mathcal{F}(i, k) = w^{(i-1)(k-1)}, w = \exp\{-j \cdot \frac{2\pi}{N}\}$ и выглядит:

$$\mathcal{F} = \frac{1}{\sqrt{N}}\begin{pmatrix} 1 & 1 & 1 & ... & 1 \\ 1 & w & w^2 & ... & w^{N-1} \\ 1 & w^2 & w^4 & ... & w^{2(N-1)} \\ \vdots & \vdots & \vdots & ... & \vdots \\ 1 & w^{N-1} & w^{2(N-1)} & ... & w^{(N-1)^2} \end{pmatrix}$$''')
    
def q27_th(): 
    print('''
    **Дискретная свёртка и Тёплицевы матрицы**

Дискретную свёртку можно представить как умножение матрицы на вектор:

$$z_i = \sum_{j=0}^{n-1} x_j y_{i-j}, \Leftrightarrow z = Ax,$$

где элементы матрицы $A$ равны $a_{ij} = y_{i-j}$, то есть они зависят только от разности между индексами строки и столбца(и получается, что матрица A - Теплицева матрица)

**Тёплицевы матрицы**

Матрица называется Тёплицевой, если её элементы определены как $a_{ij} = t_{i-j}$

* Тёплицева матрица полностью определяется первой строкой и первым столбцом (то есть $2n-1$ параметр).

* Это плотная матрица, однако она имеет структуру, то есть определяется $O(n)$ параметрами (сравните с разреженными матрицами)

* Основная операция для вычисления дискретной свёртки – это произведение Тёплицевой матрицы на вектор.
''')
def q26_th(): 
    print('''
    **Свёртка**

* Одна из основных операций в обработке сигналов/машинном обучении – это свёртка двух функций

* Пусть $x(t)$ и $y(t)$ две данные функции. Их свёртка определяется как $$(x * y)(t) = ∫_{-∞}^{∞} x(τ) y(t - τ) dτ$$



**Теорема о свёртке и преобразование Фурье**(связь свертки и Фурье)

Широко известный факт: свёртка во временном пространстве (time domain) эквивалентна произведению в частотном пространстве (frequency domain).

* Преобразование из одного пространства в другое осуществляется с помощью преобразования Фурье:

$$\hat{x}(w) = (F(x))(w) = ∫_{-∞}^{∞}
 e^{iwt} x(t) dt$$

* Тогда $$F(x*y) = F(x)F(y)$$

* Таким образом, алгоритм вычисления свёртки можно записать следующим образом:

  1. Вычислить преобразование Фурье от $x(t)$ и $y(t)$
  2. Вычислить их произведение
  3. Применить к результату обратное преобразование Фурье


**Операция дискретной свёртки**

* Если приблизим интеграл $(x * y)(t) = ∫_{-∞}^{∞} x(τ) y(t - τ) dτ$ с помощью суммы значений подынтегрального выражения на равномерной сетке, тогда нам останется просуммировать выражение $$z_i = \sum_{j=0}^{n-1} x_j y_{i-j},$$

  которое называется **дискретной свёрткой**. Его можно рассматривать как применение фильтра с коэффициентами $x$ к сигналу $y$.''')
    
def q25_th(): 
    print('''
    Быстрое преобразование Фурье — алгоритм ускоренного вычисления дискретного преобразования Фурье, позволяющий получить результат за время, меньшее чем $O(N^2)$.

В основе алгоритма лежат идеи о симметрии ДПФ и принципы динамического программирования. Для снижения времени вычисления исходная последовательность делится на две подпоследовательности, работать с которыми значительно проще.

**Алгоритм БПФ:**

1. Разделение последовательности на две подпоследовательности из элементов на чётных и нечётных позициях.

$$x_{even}(n)=x(2n), \quad x_{odd}(n)=x(2n + 1)$$

2. Для каждой подпоследовательности рекурсивно выполняется алгоритм БПФ, пока длина последовательности не станет достаточно маленькой для прямого вычисления преобразования Фурье или пока длина последовательности не станет равна 1, в таком случае значение ДПФ равно самому элементу последовательности.

3. Комбинирование результатов. Для учёта вклада нечётных компонент рассчтывается величина $W_k=\exp\{-j \cdot \frac{2\pi k}{N}\}$. Результаты комбинируются следующим образом:

$$X(k)=X_{even}​(k)+W_k\cdot​X_{odd​}(k), \quad X(𝑘+𝑁/2)=X_{even}​(k)-W_k\cdot​X_{odd​}(k)$$

**Фильтрацяи сигнала с использованием БПФ:**

1. Сигнал разбивается на фрагменты
2. К каждому фрагменту применяется БПФ
3. Определяются частоты, которые необходимо отфильтровать. Их мощность ставится равной 0.
4. К каждому фрагменту применяем обратное преобразование Фурье, чтобы получить очищенный исходный сигнал.''')
def q24_th(): 
    print('''
    Для некоторой периодической последовательности отсчётов $\{x(k)\}$ с периодом $N$ верны следующие выражения.

Дискретное преобразование Фурье является спектром дискретного периодического сигнала, то есть, его разложением на гармоники.

**Дискретное преобразование Фурье:**

$$X(n) = \sum\limits_{k=0}^{N-1} x(k) \exp\{-j \cdot \frac{2\pi n k}{N}\}$$

**Обратное дискретное преобразование Фурье:**

$$x(k) = \frac{1}{N}\sum\limits_{k=0}^{N-1} X(n) \exp\{j \cdot \frac{2\pi n k}{N}\}$$

**Симметрии в дискретном преобразовании Фурье:**

1. Для вещественнозначных сигналов ДПФ является Эрмитовым (то есть имеет место симметрия:) $$X(-n) = X^*(n)$$
(Т.е. положительные частоты являются комплексно-сопряжёнными соответствующим отрицательным частотам)
2. Чётная и нечётная симметрии

    Если сигнал чётный ($x(k) = x(N-k)$), то его ДПФ будет вещественнозначным и чётным.

    Если сигнал нечётный ($x(k) = -x(N-k)$), то его ДПФ будет комплексным и нечётным.

**Ограничения и недостатки ДПФ:**
1. Алиасинг или наложение частот

    Неправильная дискретизация аналогового сигнала приводит к тому, что высокочастотные его составляющие накладываются на низкочастотные, в результате чего восстановление сигнала во времени приводит к его искажениям. Для предотвращения этого эффекта частота дискретизации должна быть достаточно высокой, а сигнал должен быть надлежащим образом отфильтрован перед оцифровкой.

2. Сложность вычислений

    Из выражений ДПФ можно видеть, что для вычисления каждой гармоники нужно $N$ операций комплексного умножения и сложения и соответственно $N^2$ операций на полное выполнение ДПФ.''')
def q23_th(): 
    print('''
    Моделирование волны основывается на математическом описании периодических колебаний, которые можно выразить с помощью тригонометрических функций, например, синуса или косинуса:  

y(t) = A * sin(ω * t + φ)

где:  
- A — амплитуда  
- ω — угловая частота  
- t — время  
- φ — фаза  

**Амплитуда (A)** - Максимальное отклонение волны от её среднего значения (обычно нуля). Определяет высоту волны.  

**Период (T)** - Время, за которое волна совершает один полный цикл.  

T = 1 / f, где f — частота.  

**Длина волны (lambda)** - Расстояние, которое волна проходит за один полный период.  

λ = v * T = v / f, где v — скорость распространения волны.  

**Частота (f)** - Количество циклов волны в единицу времени. Измеряется в Герцах.  

f = 1 / T

**Герц (Hz)** - Единица измерения частоты. 1 Герц равен одному циклу в секунду.  **Пример**: Если f = 2 Hz, это означает, что волна совершает два полных колебания за одну секунду.  

**Дискретизация** - Процесс преобразования непрерывного сигнала в последовательность дискретных точек.  **Зачем это нужно?** Для моделирования волны на компьютере, где все данные хранятся в цифровом формате.  

**Частота дискретизации**  - Количество измерений (сэмплов) волны в единицу времени.  

**Фаза (phi)** - Начальное смещение волны относительно нуля во времени.  


**Угловая частота (omega)** - Измеряется в радианах в секунду и показывает скорость изменения фазы волны.  ''')
def q22_th(): 
    print('''
    Условия сходимости:

1. Для неявного метода
$$y_{n + i} = h \frac{\boldsymbol{\beta}_k}{\boldsymbol{\alpha}_k} f\left(\mathbf{x}_{n + k}, y\left(\mathbf{x}_{n + k}\right)\right) + \mathbf{g}_n,\quad \boldsymbol{\beta}_k \neq 0.$$

если $\lim_{h \to 0} (y_n - y(x_n))=0$ - сходится

2. Метод класса $\sum{i=0}^k \alpha_i y_{n+i} - h \sum_{i=0}^n \beta_i f\left(x_{n+i}, y_{n+i}\right)$, $n=0,1,2,...$ сходится, если для каждой задачи Коши $y_n \rightarrow y\left(x_0\right)$ при $h \rightarrow 0$, $n=\frac{x-x_0}{h}$. Для любых $x \in [x_0, x_k]$.

Метод должен удовлетворять условию минимального уровня локальной
точности

---
Невязка $\rho_{n+k}$ которая получается после подстановки точного решения $y(x)$ дифференциального уравнения в разностное,
$$
\rho_{n+k}=\sum_{i=0}^k \alpha_i y_{n+i}-h \sum_{i=0}^n \beta_i f\left(x_{n+i}, y\left(x_{n+i}\right)\right)
$$
имеет порядок $O(h^{s+1})$ и называется погрешностью аппроксимации. Число s называется порядком аппроксимации или степенью разностного уравнения, а $r_{n+k}=(\rho_{n+k})/h$ – погрешностью дискретизации.

Метод является согласованным, если
$$
\max _{0 \leq n \leq N} \frac{r_{n+k}}{h_n} \rightarrow 0 \quad \text { при } h \rightarrow 0
$$
и имеет порядок согласованности $S$,
$$
\max _{0 \leq n \leq N} \frac{\| r_{n+k} \|}{O(h^i)} = O\left(h^i\right)
$$

---

Метод из класса $\sum_{i=0}^{n} \alpha_i y_{n+i}=h\sum_{i=0}^n \beta_i f(x_{n+i},y(x_{n+i}))$, $n=0,1,2,\ldots$ удовлетворяет корневую условность, если все корни характеристического полинома $\rho(\theta)$ лежат внутри единичной окружности или на самой окружности, причем те корни, которые лежат на единичной окружности, являются простыми.

Если метод согласован, то $\rho(\theta)$ обязательно имеет корень $\theta_1 = +1$.

Корни характеристического полинома классифицируются следующим образом:

$\theta_1 = +1$ - главный корень;

$|\theta| \leq 1$, $i=2,3,\ldots$, $k$- посторонние корни.

---

Метод удовлетворяющий корневому условию называют **нуль-устойчивым.**

**Согласованность** – определяет величину погрешности аппроксимации, **нуль-устойчивость** – определяет характер развития этой и других погрешностей в пределе при $h \rightarrow 0$, $Nh = x_k - x_0$.

Метод из класса
$\sum_{i=0}^k \alpha_i y_{n+i} = h \sum_{i=0}^{\infty} \beta_i f\left(x_{n+i}, y\left(x_{n+i}\right)\right)$, $n = 0,1,2,...$
сходится тогда и только тогда, когда он является согласованным и нуль-устойчивым.

---
**Устойчивость** численного метода - непрерывная зависимость численных результатов от входных данных и ограниченность погрешности при заданных пределах изменения параметров метода (шагов сетки, числа итераций и т.д.)

**Сходимость** численного метода - стремление численных результатов к точному решению, при стремлении параметров метода к определенным предельным значениям, например, шага сетки к 0 или количества итераций к бесконечности.''')
def q21_th(): 
    print('''
    Метод Милна относится к многошаговым методам и представляет один из методов прогноза и коррекции. Для решения дифференциального уравнения с использованием метода Милна необходимо начать с выбора начального условия и шага интегрирования. Решение производится в два этапа. Первый – прогнозирование значения функции, второй – коррекция полученного значения. Если полученное значение после коррекции существенно отличается от спрогнозированного, то проводят еще один этап коррекции. Если такая ситуация повторяется, коррекция проводится до того момента, пока значение не будет удовлетворять требуемому. Однако очень часто ограничиваются одним этапом коррекции.

Метод Милна не является «самодостаточным», для его применения требуется получить исходные данные с помощью какого – либо одношагового метода.

Обычно для получения исходных значений для применения метода Милна используют метод Рунге-Кутты. С его помощью находят исходные значения.

Алгоритм:

1) По предсказывающей формуле вычисляется грубое значение y на правом конце интервала: yk + 1: yk + 1 = yk – 3 + 4/3 · (2 · fk – fk – 1 + 2 · fk – 2) · Δt.

2) Рассчитывается производная в k + 1 точке: fk + 1 = f(t + Δt, yk + 1).

3) Снова рассчитывается yk + 1 по уточненной формуле, используя уже новое значение производной в точке k + 1: yk + 1 = yk – 1 + 1/3 · (fk + 1 + 4 · fk + fk – 1) · Δt.

4) Рассчитывается производная в k + 1 точке с учетом вновь вычисленного более точного значения yk + 1: fk + 1 = f(t + Δt, yk + 1). Здесь же производится подсчет итераций счетчиком i: i := i + 1.

5) Проверка точности: |yk + 1i-я итерация – yk + 1(i + 1)-я итерация| ≤ ε. Если условие выполнено, и точность ε достигнута, то переходим на следующий шаг 6, иначе осуществляется переход на шаг 3 и процесс уточнения повторяется с новыми значениями y и f, причем их старое значение берется с предыдущей итерации.

6) Подготовка к новому шагу: изменение счетчика времени t, изменение номера шага k:
t := t + Δt
k := k + 1.

7) Проверка окончания расчета: t ≤ T. Если условие выполняется, то расчет продолжается для следующей точки, и осуществляется переход на шаг 1, иначе — конец.''')
def q20_th(): 
    print('''
    Сравнивая явные и неявные методы Адамса, можно отметить следующее:
 1. Недостаток неявных методов состоит в необходимости на каждом шаге решать уравнение относительно неизвестной величины $у_{n+1}$.
 2. Некоторое преимущество неявных методов состоит в точности: при одной и той же шаговости к неявные методы имеют порядок сходимости к + 1, в отличие от явных, у которых по рядок сходимости к.
 3. Главное преимущество неявных методов состоит в возможности решать жесткие системы


Сравнивая метод Адамса с методом Рунге-Кутта той же точности, отмечаем его экономичность, поскольку он требует вычисления лишь одного значения правой части на каждом шаге (метод Рунге-Кутта – четырех). При этом, метод Адамса неудобен тем, что невозможно начать счет по одному лишь известному значению y. Расчет может быть начат лишь с узла x3.

Явный метод Адама: Использует предыдущие значения $y_n$, $y_{n-1}$, ... для аппроксимации следующего значения $y_{n+1}$. Формула для трехшагового метода Адамса-Баффорта:
$$ y_{n+1} = y_n + \frac{h}{12}(23 f(t_n, y_n) - 16 f(t_{n-1}, y_{n-1}) + 5 f(t_{n-2}, y_{n-2})) $$


Неявный метод Адама: Использует текущие и будущие значения для более точного результата. Формула для трехшагового метода Адамса-Мултона:
$$ y_{n+1} = y_n + \frac{h}{12}((5f(t_{n+1}, y_{n+1}) + 8f(t_n, y_n) - f(t_{n-1}, y_{n-1})))$$

---

Явные методы

Преимущества: Простота реализации и вычислительная эффективность.
Недостатки: Ограниченная стабильность, особенно для жестких систем.

Неявные методы

Преимущества: Более высокая стабильность, подходящая для жестких систем.
Недостатки: Более сложная реализация и необходимость решения нелинейных уравнений на каждом шаге.''')
    
    
    
def q19_th(): 
    print('''
    Классический **метод Рунге-Кутты 2-го порядка**, он же Метод Эйлера с
пересчетом, описывается следующим уравнением:

$y_i = y_{i-1} + h \cdot f(x_i, y_i)$

Схема является неявной, так как искомое значение $y_i$
входит в обе части
уравнения.
Затем вычисляют значение производной в точке $(x_i, y_i)$ и окончательно
полагают:

$y_i = y_{i-1} + h \cdot \cfrac{f(x_{i-1}, y_{i-1}) + f(x_i, y_i^*)}{2}$

то есть усредняют значения производных в начальной точке и в точке “грубого
приближения”. Окончательно запишем рекуррентную формулу метода РунгеКутты 2-го порядка в следующем виде:

$y_i = y_{i-1} + \frac{h}{2} \cdot (k_1 + k_2)$

где:

$k_1 = f(x_{i-1}, y_{i-1})$

$k_2 = f(x_{i-1} + h, y_{i-1} + h \cdot k_1)$

Метод имеет второй порядок точности: Локальная погрешность метода Рунге–Кутты 2–го порядка $e_2 = C∙h^3$, где C –
некоторая постоянная, и пропорциональна кубу шага интегрирования: при
уменьшении шага в 2 раза локальная погрешность уменьшится в 8 раз.


**Метод Рунге-Кутты 3-го порядка:**

$y_{n+1} = y_n + \cfrac{(k_1 + 4k_2 + k_3)}{6}$

$k_1 = h * f(x_n, y_n)$

$k_2 = h * f(x_n + \frac{h}{2}, y_n + \frac{k_1}{2})$

$k_3 = h * f(x_n + h, y_n - k_1 + 2k_2)$

**Метод Рунге-Кутты 4-го порядка:**

$y_{n+1} = y_n + \cfrac{(k_1 + 2(k_2 + k_3) + k_4)}{6}$

$k_1 = h * f(x_n, y_n)$

$k_2 = h * f(x_n + \frac{h}{2}, y_n + \frac{k_1}{2})$

$k_3 = h * f(x_n + \frac{h}{2}, y_n + \frac{k_2}{2})$

$k_4 = h * f(x_n + h, y_n + k_3)$

Локальная ошибка - $O(h^5)$

Глобальная ошибка - $O(h^4)$''')
def q18_th(): 
    print('''
    Рассмотрим еще одно семейство многошаговых методов, которые
используют неявные схемы, – метод прогноза и коррекции (они
называются также методами **предиктор-корректор**). Суть этих
методов состоит в следующем.
На каждом шаге вводятся два этапа, использующих многошаговые
методы:

1) с помощью явного метода (**предиктора**) по известным значениям
функции в предыдущих узлах находится начальное приближение
$𝑦_{𝑖+1} = 𝑦_{𝑖+1}^{(0)}$
в новом узле.

2) используя неявный метод (**корректор**), в результате итераций
находятся приближения $𝑦_{𝑖+1}^{(1)}, 𝑦_{𝑖+1}^{(2)}, ...$


К методам «предиктор-корректор» относится, например, метод Эйлера – Коши, где мы вычисляем

$y_{i+1}^{(0)} = y_i + h \cdot f(x_i, y_i)$

начальное приближение, с помощью явного метода – Эйлера (предиктор), затем

$y_{i+1}^{(1)} = y_i + h \cdot \cfrac{f(x_i, y_i) + f(x_{i+1}, y_{i+1}^{(0)})}{2}$

– следующее приближение значения функции $y_{i+1}$ в $x_{i+1}$-ой точке, $y_{i+1}^{(1)}$(корректор).


Один из вариантов метода прогноза и
коррекции может быть получен на основе
метода Адамса четвертого порядка:

на этапе предиктора
$y_{i+1} = y_i + \frac{h}{24}(55f_i - 59f_{i-1} + 37f_{i-2} - 9f_{i-3})$

на этапе корректора
$y_{i+1} = y_i + \frac{h}{24}(9f_{i+1} + 19f_i - 5f_{i-1} + f_{i-2})$

Явная схема используется на каждом шаге
один раз, а с помощью неявной схемы
строится итерационный процесс вычисления
$y_{i+1}$
, поскольку это значение входит в правую часть выражения $f_{i+1} = f(x_{i+1},
y_{i+1})$. Расчет по этому методу может быть начат только со значения y4
.
Необходимые при этом y1
, y2
, y3 находятся по методу Рунге-Кутта, y0
задается начальным условием.''')
    
    
def q17_th(): 
    print('''
    $\textbf{Метод Эйлера}$ — это один из самых простых методов численного решения обыкновенных дифференциальных уравнений (ОДУ). Он основан на аппроксимации решения с использованием касательной к графику функции и позволяет шаг за шагом приближённо вычислять значение функции.

**Формулировка метода**

Рассмотрим задачу Коши для ОДУ первого порядка:

$\frac{dy}{dx} = f(x,y), \space \space \space \space  y(x_0) = y_0$

Метод Эйлера позволяет найти приближённое значение y в следующей точке $x_{n+1} = x_n+h(h-шаг)$ по формуле:

$y_{n+1} = y_n+h \cdot f(x_n, y_n)$,

где $y_n$ - приближенное значение функции в точке $x_n$

Геометрическая интерпретация
Метод Эйлера можно рассматривать как последовательное построение касательных к кривой $y= y(x)$. На каждом шаге рассчитывается наклон касательной (то есть значение производной $f(x,y))$, и вдоль этой касательной проводится линейное приближение на длину шага $h$.

**Плюсы метода Эйлера**

1) $\it\text{Простота реализации}$:

Метод легко реализовать программно, он не требует сложных вычислений.

2) $\it\text{Интуитивная понятность}$:

Метод основан на простых геометрических и алгебраических принципах.


**Минусы метода Эйлера**

1) $\it\text{Низкая точность}$:

Ошибка метода Эйлера имеет порядок $O(h)$, что означает, что точность решения сильно зависит от величины шага $h$. Для достижения приемлемой точности шаг должен быть очень маленьким, что увеличивает количество вычислений.

2) $\it\text{Накопление ошибок}$:

Поскольку метод основан на последовательных шагах, ошибки на каждом шаге суммируются, что приводит к значительному отклонению от истинного решения.''')
def q16_th(): 
    print('''
    $\textbf{Метод центральной разности}$ — это численный метод для приближённого вычисления производной функции. Он используется, когда аналитическое нахождение производной либо невозможно, либо затруднено.

**Плюсы метода:**

1) $\it\text{Более высокая точность}$:

По сравнению с методами односторонних разностей (прямой и обратной), метод центральной разности обладает более высокой точностью, поскольку ошибка аппроксимации составляет $𝑂(h^2)$, тогда как в методах односторонних разностей —  $𝑂(h)$

2) $\it\text{Cимметричность}$:

Метод симметричен относительно точки
𝑥, что делает его более устойчивым и точным для гладких функций.






**Минусы метода:**

1) $\it\text{Невозможность вычисления на краях интервала}$:

Если требуется вычислить производную на границе заданного интервала, метод центральной разности использовать нельзя, поскольку он требует значений функции с обеих сторон от точки 𝑥.

2) $\it\text{Чувствительность к шагу ℎ}$:

Слишком маленький шаг может привести к накоплению ошибок округления, а слишком большой шаг уменьшает точность аппроксимации.''')
def q15_th(): 
    print('''
    **Локальные ошибки** - погрешности, образовавшиеся на каждом шаге (разница между точным и вычисленным значением на каждом шаге) = невязка метода

**Глобальные ошибки (накопленные)** - погрешности, образовавшиеся за $n$ шагов

Порядок глобальной погрешности относительно шага интегрирования на единицу ниже, чем порядок локальной погрешности. Таким образом, глобальная ошибка метода Эйлера есть  $O(h)$, т. е. данный метод имеет первый порядок. Иными словами, размер шага и ошибка для метода Эйлера связаны линейно. Практическим следствием этого факта является ожидание того, что при уменьшении приближенное решение будет все более точным и при стремлении к нулю будет стремиться к точному решению с линейной скоростью ; т.е. ожидаем, что при уменьшении шага вдвое ошибка уменьшится примерно в два раза.

Порядок численного метода для решения ОДУ определяется порядком его глобальной погрешности. Он может быть также опрделён, как количество вычислений значения производной $f(x, y)$ искомой ф-ии на каждом шаге. В соответствии с этим метод Эйлера является методом первого порядка.

для методов Рунге-Кутты глобальная ошибка — $O(h^p)$, где $p$ зависит от порядка метода (например, для метода Рунге-Кутты 4-го порядка — это $O(h^4))$''')
    
    
    
def q14_th(): 
    print('''
    Обыкновенные дифференциальные уравнения (оду) - ур-я, содержащие одну или несколько производных от искомой ф-и:

$F(x,y, y^,, y^{,,}, ..., y^{(n)}) = 0$

x - независимая переменная, y = y(x) - искомая ф-я

Наисвысший порядок производной n, входящей в предыдущее уравнение, называют порядком дифференциального ур-я

Рассмотрим с-му ОДУ первого порядка, записанную в виде:

$y^{'} (x) = f(x, y(x))$

Решение: любая ф-я y(x), которая удовлетворяет ур-ю. Решением ОДУ на интервале (a,b) называется ф-я $y = Φ(x)$, которая при её подстановке в исходное уравнение обращает его в тождество на (a, b)

Решение ОДУ в неявном виде $\Phi (x, y) = 0$ называется интегралом ОДУ

Существует мн-во возможных решенийю Для одного уникального решения необходимо указать независимые условия (для с-мы размером n)

Например, когда n условий заданы для одной точки.

$y(0) = y_0$

Это задача Коши (задача с начальными условиями). Либо дифференциальная задача''')
    
    
def q13_th(): 
    print('''
    **Определение разреженных матриц**

* Разреженные матрицы – это матрицы, такие что количество ненулевых элементов в них существенно меньше общего числа элементов в матрице.

* Из-за этого вы можете выполнять базовые операции линейной алгебры (прежде всего решать линейные системы) гораздо быстрее по сравнению с использованием плотных матриц.


**Приложения разреженных матриц**
Разреженные матрицы возникают в следующих областях:

* математическое моделирование и решение уравнений в частных производных
* обработка графов(графы представляют в виде матриц смежности, которые чаще всего разрежены
), например анализ социальных сетей
* рекомендательные системы
* в целом там, где отношения между объектами "разрежены"

**Хранение разреженных матриц**

1. $\it\text{COO (координатный формат)}$
* Простейший формат хранения разреженной матрицы – координатный.
* В этом формате разреженная матрица – это набор индексов и значений в этих индексах -> $i, j, val$
где $i, j$ массивы индексов, $val$ массив элементов матрицы.
* Таким образом, нам нужно хранить $ 3\cdot nnz$ элементов, где $nnz$ обозначает число ненулевых элементов в матрице.

  $\it{Недостатки}:$
  * Он неоптимален по памяти
  * Он неоптимален для умножения матрицы на вектор
  * Он неоптимален для удаления элемента
  Первые два недостатка решены в формате CSR.

2. $\it\text{CSR - Compressed sparse row}$

  В формате CSR матрица хранится также с помощью трёх массивов, но других: $ia, ja, sa$, где:
  * $ia$ (начало строк) массив целых чисел длины
  * $ja$ (индексы столбцов) массив целых чисел длины $nnz$
  * sa (элементы матрицы) массив действительных чисел длины $nnz$

  Всего необходимо хранить $2 \cdot nnz + n + 1$
 элементов.

3. $\it\text{LIL (список списков)}$

4. $\it\text{CSC (compressed sparse column)}$
5. $\it\text{блочные варианты}$

В scipy представлены конструкторы для каждого из этих форматов, например

scipy.sparse.lil_matrix(A).

**Прямые методы для решения больших разреженных систем:**
(прямые методы - численные методы, которые находят точное решение систем линейных уравнений вида Ax=b)
* LU разложение (Для разреженных матриц часто используют модифицированные алгоритмы LU-разложения, которые минимизируют заполнение(добавление новых ненулевых элементов в ходе вычислений) с помощью перестановок строк и столбцов)
* Различные методы переупорядочивания для минимизации заполнения факторов''')
    
    
    
def q12_th(): 
    print('''
    **Метод разделяй и властвуй** вычисления собственных значений и векторов трёхдиагональной матрицы - наиболее быстрый из существующих методов вычисления всех собственных значений и собственных векторов трехдиагональной матрицы, начиная с порядка n, который примерно равен 26. (Точное значение этого порогового порядка в конкретном случае зависит от компьютера.)
Пусть у нас есть трёхдиагональная матрица и мы разделили её на блоки:

   $$
      T = \begin{pmatrix}
      T_1 & B  \\
      B^T & T_2
      \end{pmatrix}
  $$

Можем записать матрицу $T$ в виде

   $$
      T = \begin{pmatrix}
      T_1 & B  \\
      B^T & T_2
      \end{pmatrix} + \rho vv^*
  $$

где $v^*$ – эрмитово-сопряжённый вектор, $v = (0, ..., 0, 1, 1, 0, ..., 0)^T$

Пусть мы уже разложили матрицы $T_1$ и $T_2$:

$T_1 = Q_1 Ʌ_1 Q^*_1$

$T_2 = Q_2 Ʌ_2 Q^*_2$


Тогда (проверьте!),

   $$
       \begin{pmatrix}
      Q_1^* & 0  \\
      0 & Q_2^*
      \end{pmatrix} T    
      \begin{pmatrix}
      Q_1^* & 0  \\
      0 & Q_2^*
      \end{pmatrix} = D + \rho uu^*
  $$  
  
  $$
      D = \begin{pmatrix}
      Ʌ_1 & 0  \\
      0 & Ʌ_2
      \end{pmatrix}
  $$

то есть мы свели задачу к задаче вычисления собственных значений у матрицы вида "диагональная матрица плюс матрица малого ранга"

**Матрица вида диагональная матрица плюс матрица малого ранга**

Собственные значения матрицы вида $D + \rho uu^*$ вычислить не просто.
Характеристический многочлен имеет вид

$det(D + \rho uu^* - \lambda I) = det(D - \lambda I) det (I + \rho (D - \lambda I)^{-1}uu^*) = 0$

Тогда:

$det(I + \rho (D - \lambda I)^{-1}uu^*) = 1 + \rho \sum_{i=1}^n \frac{|u_i|^2}{d_i - \lambda} = 0$

**Характеристическое уравнение**

$1 + \rho \sum_{i=1}^n \frac{|u_i|^2}{d_i - \lambda} = 0$''')
    
def q11_th(): 
    print('''
    **Сходимость QR алгоритма**

Если у нас есть разложение вида:  
$$A = X \Lambda X^{-1}, \quad A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} $$  

и  
$$
\Lambda = \begin{bmatrix} \Lambda_1 & 0 \\ 0 & \Lambda_2 \end{bmatrix},  
\quad \lambda(\Lambda_1) = \{\lambda_1, \dots, \lambda_m\},  
\quad \lambda(\Lambda_2) = \{\lambda_{m+1}, \dots, \lambda_r\},
$$

а также есть зазор между собственными значениями в матрицах $ \Lambda_1 $ и  $ \Lambda_2 $:  
$$
|\lambda_1| \geq \dots \geq |\lambda_m| > |\lambda_{m+1}| \geq \dots \geq |\lambda_r| > 0,
$$

тогда блок $ A_{21}^{(k)} $ матрицы $ A_k $ сходится к нулевому в процессе работы QR алгоритма со скоростью  
$$
\|A_{21}^{(k)}\| \leq Cq^k, \quad q = \left| \frac{\lambda_{m+1}}{\lambda_m} \right|,
$$
где $ m $ — размер матрицы $ \Lambda_1 $.  

Таким образом, нам нужно увеличить зазор между $ \Lambda_1 $ и $ \Lambda_2 $. Это можно сделать с помощью **QR алгоритма со сдвигами**.

QR-алгоритм со сдвигами – это модификация базового QR-алгоритма, которая ускоряет его сходимость. В ней вводится сдвиг $s_k$
 , чтобы быстрее выделить собственные значения матрицы.

**QR алгоритм со сдвигами**

$A_k - s_kI = Q_kR_k$

$A_{k+1} = R_kQ_k+s_kI$

Сходимость такого алгоритма линейная с фактором $$\big|\frac{\lambda_{m+1} - s_k}{\lambda_m - s_k}\big|$$

где $\lambda_m$– $m$-ое большее по модулю собственное значение. Если сдвиг близок к собственному вектору, сходимость более быстрая - становится почти квадратичной

* Существуют различные стратегии выбора сдвигов.

* Использование сдвигов – это общий подход к ускорению сходимости итерационных методов вычисления собственных значений.''')
    
    
def q10_th(): 
    print('''
    **Спектр** — это спектр линейного оператора, который представляет собой множество собственных значений оператора.

**Псевдоспектр** матрицы $A$ — это обобщение спектра, которое учитывает влияние малых возмущений на собственные значения. Он определяется как множество комплексных чисел $\lambda$, для которых матрица $(A−\lambda I)$ становится почти вырожденной, то есть:
$$||(A−λI)^{-1}||\ge \frac{1}{\epsilon}$$

для некоторого малого
$\epsilon>0$, где $||\cdot||$ — операторная норма.

* Для динамических систем с матрицей $A$, спектр может много сообщить о поведении системы (например, о её устойчивости)

* Однако для не нормальных матриц, спектр может быть неустойчивым относительно малых возмущений матрицы

* Для измерения подобных возмущений было разработана концепция псевдоспектра.

Рассмотрим объединение всех возможных собственных значений для всевозможных возмущений матрицы $A$.

$$\Lambda_{\epsilon}(A) = \{ \lambda \in \mathbb{C}: \exists E, x \ne 0: (A + E) x = \lambda x, \quad \Vert E \Vert_2 \leq \epsilon. \}$$

Для малых $E$ и нормальных $A$ это круги вокруг собственных значений, для не нормальных матриц, структура может сильно отличаться.''')

def q9_th(): 
    print('''
    **Определение.** Нормальная матрица — это квадратная матрица $A$, которая коммутирует со своей эрмитово-сопряжённой матрицей $
A^*$, то есть матрица $A$ нормальная, если
$$ AA^* = A^* A. $$

**Нормальные матрицы**

**Теорема**: $A$ – **нормальная матрица**, тогда и только тогда, когда существует такая унитарная матрица $U$, что $$A = U \Lambda U^*$$, где $\Lambda$-диагональная матрица, содержащая собственные значения матрицы $A$ на диагонали.

**Важное следствие**
Любая нормальная матрица – **унитарно диагонализуема**. Это означает, что она может быть приведена к диагональному виду с помощью унитарной матрицы $U$. Другими словами, каждая нормальная матрица имеет ортогональный базис из собственных векторов.

**Эрмитова** (или самосопряжённая) ма́трица — квадратная матрица, элементы которой являются комплексными числами, и которая, будучи транспонирована, равна комплексно сопряжённой:
$A^T = \overline{A}$. То есть для любого столбца $i$ и строки $j$ справедливо равенство $a_{ij} = \overline{a_{ji}}$ где $\overline{a}$— комплексно сопряжённое число к $a$, или $A = (\overline{A})^T = A^*$,
где * — эрмитово сопряжение. То есть эрмитова матрица — это квадратная матрица, которая равна своей эрмитово-сопряжённой матрице.

**Эрмитово - сопряженная матрица(сопряженно-транспонированная)** - матрица $A^*$ с комплексными элементами, полученная из исходной матрицы $A$ транспонированием и заменой каждого эелемента комплексно-сопряженным ему.

Пример:

$$\begin{equation*}
A =
\begin{pmatrix}
3 & 2+i\\
2 - i & 1\\
\end{pmatrix}
\end{equation*}$$

$$\begin{equation*}
A^* =
\begin{pmatrix}
3 & 2+2i\\
2-i & 1
\\
\end{pmatrix}
\end{equation*}$$

**Унитарно - диагонализуемые матрицы**: матрица $A$ унитарно - диагонализуемая, если сущетвует унитарная матрица $U$ такая, что $U^*AU$ - диагональная матрица.

(Унитарная матрица - квадратная матрица с комплексными элементами, результат умножения которой на эрмитово - сопряженную равен единичной матрице $U^*U = UU^* = I$. Иначе говоря, матрица унитарна тогда и т.т., когда существует обратная к ней матрица, удовлетворяющая условию $U^{-1} = U^*$)

**Верхне-гессенбергова форма матрицы**
Матрица $A$ имеет верхне-гессенбергову форму, если $$a_{ij} = 0, при \space i\geq j+2.$$

$$H =
\begin{bmatrix}
* & * & * & * & * \\
* & * & * & * & * \\
0 & * & * & * & * \\  
0 & 0 & * & * & * \\
0 & 0 & 0 & * & *  
\end{bmatrix}
$$


**Приведение произвольной матрицы к верхне-гессенберговой форме**

С помощью отражений Хаусхолдера можно привести любую матрицу к верхне-гессенберговой форме:
$$U^*AU = H$$

* Единственное отличие от вычисления разложения Шура заключается в занулении последних $n-2, n-3,...$
 элементов в первом, втором и так далее столбцах

* Сложность такого приведения $O(n^3)$ операций

* Если матрица приведена к верхне-гессенберговой форме, то одна итерация QR алгоритма имеет сложность
$O(n^2)$ операций (например, используя вращения Гивенса)

* Также верхне-гессенбергова форма матрицы сохраняется после выполнения одной итерации QR алгоритма.


вся теория

**Определение.** Нормальная матрица — это квадратная матрица $A$, которая коммутирует со своей эрмитово-сопряжённой матрицей $
A^*$, то есть матрица $A$ нормальная, если
$$ AA^* = A^* A. $$

**Нормальные матрицы**

**Теорема**: $A$ – **нормальная матрица**, тогда и только тогда, когда существует такая унитарная матрица $U$, что $$A = U \Lambda U^*$$, где $\Lambda$-диагональная матрица, содержащая собственные значения матрицы $A$ на диагонали.

**Важное следствие**
Любая нормальная матрица – **унитарно диагонализуема**. Это означает, что она может быть приведена к диагональному виду с помощью унитарной матрицы $U$. Другими словами, каждая нормальная матрица имеет ортогональный базис из собственных векторов.

-----
**Эрмитова** (или самосопряжённая) ма́трица — квадратная матрица, элементы которой являются комплексными числами, и которая, будучи транспонирована, равна комплексно сопряжённой:
$A^T = \overline{A}$. То есть для любого столбца $i$ и строки $j$ справедливо равенство $a_{ij} = \overline{a_{ji}}$ где $\overline{a}$— комплексно сопряжённое число к $a$, или $A = (\overline{A})^T = A^*$,
где * — эрмитово сопряжение. То есть эрмитова матрица — это квадратная матрица, которая равна своей эрмитово-сопряжённой матрице.

**Эрмитово - сопряженная матрица(сопряженно-транспонированная)** - матрица $A^*$ с комплексными элементами, полученная из исходной матрицы $A$ транспонированием и заменой каждого эелемента комплексно-сопряженным ему.

Пример:

$$\begin{equation*}
A =
\begin{pmatrix}
3 & 2+i\\
2 - i & 1\\
\end{pmatrix}
\end{equation*}$$

$$\begin{equation*}
A^* =
\begin{pmatrix}
3 & 2+2i\\
2-i & 1
\\
\end{pmatrix}
\end{equation*}$$

-----
**Унитарно - диагонализуемые матрицы**: матрица $A$ унитарно - диагонализуемая, если сущетвует унитарная матрица $U$ такая, что $U^*AU$ - диагональная матрица.

(Унитарная матрица - квадратная матрица с комплексными элементами, результат умножения которой на эрмитово - сопряженную равен единичной матрице $U^*U = UU^* = I$. Иначе говоря, матрица унитарна тогда и т.т., когда существует обратная к ней матрица, удовлетворяющая условию $U^{-1} = U^*$)

----
**Верхне-гессенбергова форма матрицы**
Матрица $A$ имеет верхне-гессенбергову форму, если $$a_{ij} = 0, при \space i\geq j+2.$$

$$H =
\begin{bmatrix}
* & * & * & * & * \\
* & * & * & * & * \\
0 & * & * & * & * \\  
0 & 0 & * & * & * \\
0 & 0 & 0 & * & *  
\end{bmatrix}
$$


**Приведение произвольной матрицы к верхне-гессенберговой форме**

С помощью отражений Хаусхолдера можно привести любую матрицу к верхне-гессенберговой форме:
$$U^*AU = H$$

* Единственное отличие от вычисления разложения Шура заключается в занулении последних $n-2, n-3,...$
 элементов в первом, втором и так далее столбцах

* Сложность такого приведения $O(n^3)$ операций

* Если матрица приведена к верхне-гессенберговой форме, то одна итерация QR алгоритма имеет сложность
$O(n^2)$ операций (например, используя вращения Гивенса)

* Также верхне-гессенбергова форма матрицы сохраняется после выполнения одной итерации QR алгоритма.''')
    
    
def q8_th(): 
    print('''
    $\LargeТеорема\spaceШура$


**Теорема:** Пусть матрица $A \in \mathbb{C}^{n \times n}$. ТОгда существует матрица $U$ унитарная и матрица $T$ верхнетреугольная такие, что $$T = U^*AU$$

$A = UTU^* - $разложение Шура

**Набросок доказательства**.
1. Каждая матрица имеет как минимум один ненулевой собственный вектор (для корня характеристического многочлена матрица $(A-\lambda I)$ вырождена и имеет нетривиальное ядро). Пусть

$$Av_1 = \lambda_1 v_1, \quad \Vert v_1 \Vert_2 = 1.$$

2. Пусть $U_1 = [v_1,v_2,\dots,v_n]$, где $v_2,\dots, v_n$ любые векторы ортогональные $v_1$. Тогда
  
  $$
      U^*_1 A U_1 = \begin{pmatrix}
      \lambda_1 & *  \\
      0 & A_2
      \end{pmatrix},
  $$
  
  где $A_2$ матрица размера $(n-1) \times (n-1)$. Это называется **блочнотреугольной формой**. Теперь мы можем проделать аналогичную процедуру для матрицы $A_2$ и так далее.  
  
  
**Замечание**: Поскольку в доказательстве необходимы собственные векторы, оно не является практичным алгоритмом.

**Приложение теоремы Шура**

Важное приложение теоремы Шура связано с так называемыми **нормальными матрицами**.  

**Определение.** Матрица $A$ называется **нормальной матрицей**, если  

$$ AA^* = A^* A. $$

**Q:** Какие примеры нормальных матриц вы можете привести?

Примеры: эрмитовы матрицы, унитарные матрицы.

$\LargeРазложение\spaceШура$
- Нужно найти унитарную матрицу $U$ и верхнетреугольную матрицу $T$, такие что для данной матрице $A$ выполнено

$$ A = U T U^*. $$

- <font color='red'> **Не путайте** QR алгоритм и QR разложение! </font>

- QR разложение – это представление матрицы в виде произведения двух матриц, а QR алгоритм использует QR разложение для вычисления разложения Шура.

Зачем нужно разложение Шура?

- Численные методы:
Разложение Шура часто используется как основа для численного поиска собственных значений. В частности, QR-алгоритм сводится к нахождению разложения Шура.

- Контроль собственных значений:
Так как разложение Шура даёт матрицу $T$, где на диагонали стоят собственные значения, оно полезно для анализа спектра матрицы.

**Путь к QR алгоритму**

Рассмотрим выражение

$$A = Q T Q^*,$$

и перепишем его в виде

$$
   Q T = A Q.
$$

Слева замечаем QR разложение матрицы $AQ$.

Используем его чтобы записать одну итерацию метода неподвижной точки для разложения Шура.

**Вывод QR алгоритма из уравнения неподвижной точки**

Запишем следующий итерационный процесс

$$
    Q_{k+1} R_{k+1} = A Q_k, \quad Q_{k+1}^* A = R_{k+1} Q^*_k
$$

Введём новую матрицу

$$A_k = Q^* _k A Q_k = Q^*_k Q_{k+1} R_{k+1} = \widehat{Q}_k R_{k+1}$$

тогда аппроксимация для $A_{k+1}$ имеет вид

$$A_{k+1} = Q^*_{k+1} A Q_{k+1} = ( Q_{k+1}^* A = R_{k+1} Q^*_k)  = R_{k+1} \widehat{Q}_k.$$

Итак, мы получили стандартную форму записи QR алгоритма.

Финальные формулы обычно записывают в **QRRQ**-форме:

1. Инициализируем $A_0 = A$.
2. Вычислим QR разложение матрицы $A_k$: $A_k = Q_k R_k$.
3. Обновим аппроксимацию $A_{k+1} = R_k Q_k$.

Продолжаем итерации пока $A_k$ не станет достаточно треугольной (например, норма подматрицы под главной диагональю не станет достаточно мала).

**Что известно о сходимости и сложности**

**Утверждение**

Матрицы $A_k$ унитарно подобны матрице $A$

$$A_k = Q^*_{k-1} A_{k-1} Q_{k-1} = (Q_{k-1} \ldots Q_1)^* A (Q_{k-1} \ldots Q_1)$$

а произведение унитарных матриц – унитарная матрица.

Сложность одной итерации $\mathscr{O}(n^3)$, если используется QR разложение для общего случая.

Мы ожидаем, что $A_k$ будет **очень близка к треугольной матрице** для достаточно большого $k$.''')
    
    
def q7_th(): 
    print('''
    $\textbf{Круги Гершгорина}$

Есть интересная теорема, которая часто помогает локализовать собственные значения.
Она называется $\it\text{теоремой Гершгорина}$.

Она утверждает, что все собственные значения $\lambda_i, i = \overline{1,n}$ находятся внутри объединения кругов Гершгорина $C_i$, где $C_i$– окружность на комплексной плоскости с центром в $a_{ii}$ и радиусом $r_i = \sum_{j \neq i} |a_{ij}|$


Более того, если круги не пересекаются, то они содержат по одному собственному значению внутри каждого круга.


**Доказательство**(на всякий случай)

Сначала покажем, что если матрица $A$ обладает строгим диагональным преобладанием, то есть $$|a_{ii}| > \sum_{j \neq i} |a_{ij}|,$$

тогда такая матрица невырождена.


Разделим диагональную и недиагональную часть и получим $$A = D+S = D(I+D^{-1}S),$$

где $||D^{-1}S||_1 < 1$. Поэтому, в силу теоремы о ряде Неймана, матрица $I + D^{-1}S$ обратима и, следовательно, матрица $A$ также обратима.

Теперь докажем утверждение теоремы от противного:

* если любое из собственных чисел лежит вне всех кругов, то матрица $(A - \lambda I)$ обладает свойством строгого диагонального преобладания
* поэтому она обратима
* это означает, что если $(A - \lambda I)x = 0$, то $x = 0$.''')
    
    
def q6_th():
    print('''
    $\it\textbf{Степенной метод}$

* Часто в вычислительной практике требуется найти не весь спектр, а только некоторую его часть, например самое большое или самое маленькое собственые значения.
* Также отметим, что для Эрмитовых матриц $(A = A^*)$
 собственные значения всегда действительны.

* Степенной метод – простейший метод вычисления $\it\text{максимального по модулю}$ собственного значения. Это также первый пример итерационного метода и Крыловского метода.

**Что необходимо помнить о степенном методе**

* Степенной метод даёт оценку для максимального по модулю собственного числа или спектрального радиуса матрицы
* Одна итерация требует одного умножения матрицы на вектор. Если можно умножить вектор на матрицу зa $O(n)$(например, она разреженная), тогда степенной метод можно использовать для больших $n$
* Сходимость может быть медленной
* Для грубой оценки максимального по модулю собственного значения и соответствующего вектора достаточно небольшого числа итераций

**Степенной метод: вид**

Задача на собственые значения $$Ax = \lambda x, ||x||_2 = 1 \text{(для устройчивости)}$$

может быть записана как итерации с неподвижной точкой, которые называются $\it\text{степенным методом}$ и дают максимальное по модулю собственное значение матрицы $A$.

Степенной метод имеет вид
$$x_{k+1} = Ax_k, x_{k+1} := \frac{x_{k+1}}{||x_{k+1}||_2}$$
и $x_{k+1} → v_1$, где $Av_1 = \lambda_1v_1 $ и $\lambda_1$ максимальное по модулю собственное значение, и $v_1$ – соответствующий собственный вектор.

На $(k+1)$-ой итерации приближение для $\lambda_1$ может быть найдено следующим образом $$\lambda^{k+1} = (Ax_{k+1}, x_{k+1})$$

Заметим, что $\lambda^{(k+1)}$ не требуется для $(k+2)$-ой итерации, но может быть полезно для оценки ошибки на каждой итерации: $||Ax_{k+1} - \lambda^{(k+1)}x_{k+1}||$
.

Метод сходится со скоростью геометричекой прогрессии, с константой $q = |\frac{\lambda_2}{\lambda_1}| < 1$, где $\lambda_1 > \lambda_2 \geq ... \geq \lambda_n$
. Это означает, что сходимость может быть сколь угодно медленной при близких значениях у $\lambda_1$ и $\lambda_2$.

**Общая сложность степенного метода**

Пусть $k$ — число итераций, необходимых для достижения заданной точности $ε$. Тогда общая сложность метода будет равна:
$O(k⋅n^2)$''')
    
def q5_th():
    print('''
    $\LargeРазложение\spaceШура$
- Нужно найти унитарную матрицу $U$ и верхнетреугольную матрицу $T$, такие что для данной матрице $A$ выполнено

$$ A = U T U^*. $$
  * Собственные значения матрицы $A$ находятся на диагонали матрицы $T$.

 **Не путайте** QR алгоритм и QR разложение!

- QR разложение – это представление матрицы в виде произведения двух матриц, а QR алгоритм использует QR разложение для вычисления разложения Шура.

**Путь к QR алгоритму**

Рассмотрим выражение

$$A = Q T Q^*,$$

и перепишем его в виде

$$
   Q T = A Q.
$$

Слева замечаем QR разложение матрицы $AQ$.

Используем его чтобы записать одну итерацию метода неподвижной точки для разложения Шура.

**Вывод QR алгоритма из уравнения неподвижной точки**

Запишем следующий итерационный процесс

$$
    Q_{k+1} R_{k+1} = A Q_k, \quad Q_{k+1}^* A = R_{k+1} Q^*_k
$$

Введём новую матрицу

$$A_k = Q^* _k A Q_k = Q^*_k Q_{k+1} R_{k+1} = \widehat{Q}_k R_{k+1}$$

тогда аппроксимация для $A_{k+1}$ имеет вид

$$A_{k+1} = Q^*_{k+1} A Q_{k+1} = ( Q_{k+1}^* A = R_{k+1} Q^*_k)  = R_{k+1} \widehat{Q}_k.$$

Итак, мы получили стандартную форму записи QR алгоритма.

Финальные формулы обычно записывают в **QRRQ**-форме:

1. Инициализируем $A_0 = A$.
2. Вычислим QR разложение матрицы $A_k$: $A_k = Q_k R_k$.
3. Обновим аппроксимацию $A_{k+1} = R_k Q_k$.

Продолжаем итерации пока $A_k$ не станет достаточно треугольной (например, норма подматрицы под главной диагональю не станет достаточно мала).

**Что известно о сходимости и сложности**

**Утверждение**

Матрицы $A_k$ унитарно подобны матрице $A$

$$A_k = Q^*_{k-1} A_{k-1} Q_{k-1} = (Q_{k-1} \ldots Q_1)^* A (Q_{k-1} \ldots Q_1)$$

а произведение унитарных матриц – унитарная матрица.

Сложность одной итерации $\mathscr{O}(n^3)$, если используется QR разложение для общего случая.

Мы ожидаем, что $A_k$ будет **очень близка к треугольной матрице** для достаточно большого $k$.

**Сходимость и сложность QR алгоритма**

- QR алгоритм сходится от первого диагонального элемента к последнему.

- По крайней мере 2-3 итерации необходимо для определения каждого диагонального элемента матрицы $T$.

- Каждый шаг состоит в вычислении QR разложения и одного произведения двух матриц, в результате имеем сложность $\mathscr{O}(n^3)$.

**Q**: означает ли это итоговую сложность $\mathscr{O}(n^4)$?

**A**: к счастью, нет!

- Мы можем ускорить QR алгоритм, используя сдвиги, поскольку матрица $A_k - \lambda I$ имеет те же векторы Шура (столбцы матрицы $U$).''')
    
def q4_th():
    print('''
    Что такое собственный вектор?

**Определение**. Вектор $x \neq 0$ называется собственным для квадратной матрицы A, если найдётся такое число $\lambda$, что $$Ax = \lambda x$$


Число $\lambda$ называется $\it{собственным}$ значением.

Так как матрица $A - \lambda I$ должна иметь нетривиальное ядро (что такое ядро?), собственные значения являются корнями характеристического полинома $$det(A - \lambda I) = 0$$

**Важность**

$\it\text{Собственные значения – это частоты выбраций}$

Обычно вычисление собственных значений и собственных векторов необходимо для изучения:

* вибраций в механических структурах
* снижения сложности моделей сложных систем

**Для чего используют:**

1. Упрощение сложных преобразований
- Собственные векторы и собственные значения помогают разложить сложные преобразования на простые части. Например, поворот, масштабирование или сжатие в каком-то направлении проще понять, если выделить направления (собственные векторы), которые остаются неизменными.

2. Диагонализация матриц
- Собственные векторы и значения используются для представления матрицы в более удобной форме — диагональной. Это важно, потому что:

  - Вычисления с диагональными матрицами (например, возведение в степень) намного проще.
  - Это позволяет изучать матрицу с минимальными усилиями.

3. Машинное обучение и анализ данных

- PCA (метод главных компонент): Собственные векторы помогают найти главные направления в данных, чтобы снизить размерность и оставить только важные признаки.
- Кластеризация: Собственные значения используются в алгоритмах, таких как спектральная кластеризация.

4. Дифференциальные уравнения

- Собственные значения и векторы упрощают решение линейных дифференциальных уравнений, которые описывают многие явления в природе и технике.

**Google PageRank**

Одна из самых известных задач, сводящихся к вычислению собственного вектора, – это задача вычисления Google PageRank.

* Задача состои в ранжировании веб-страницы: какие из них являются важными, а какие нет
* В интернете страницы ссылаются друг на друга
* PageRank определяется рекурсивно.

  Обозначим $p_i$ за важность $i$-ой страницы. Тогда определим эту важность как усреднённую важность всех страниц, которые ссылаются на данную страницу. Это определение приводит к следующей линейной системе $$p_i = \sum_{j \in N(i)}\frac{p_j}{L(j)},$$



  где

  * $L(j)$ – число исходящих ссылок с $j$-ой страницы,
  * $N(i)$– число соседей $i$-ой страницы.

  Это может быть записано следующим образом $$p = Gp,   G_{ij} = \frac{1}{L(j)}$$

  

  или как задача на собственные значения $$Gp = 1p$$


  то есть мы уже знаем, что у матрицы $G$ есть собственное значение равное $1$. Заметим, что $G$ – левостохастичная матрица, то есть сумма в каждом столбце равна $1$.''')
    
def q3_th():
    print('''
    Алгоритм Штрассена — это эффективный алгоритм умножения квадратных матриц, снижающий сложность с $O(n^3)$ до $O(n^{log_27}) \approx O(n^{2.81})$. В стандартном методе умножения матриц используется 8 умножений подматриц, что приводит к сложности  $O(n^3)$. Алгоритм Штрассена заменяет 8 умножений на 7,поэтому сложность становится равной  $O(n^{log_27})$



Метод Штрассена становится быстрее наивного алгоритма, если

$$2n^3>7n^{log_27},$$ $$n>667$$

классическое понятие сходимости, как его применяют, например, к итерационным методам или методам численного интегрирования, напрямую нельзя применить к методу Штрассена, поскольку это не итерационный метод, а детерминированный алгоритм, который выполняет конечное число операций и всегда возвращает определённый результат.

Почему классическое понятие сходимости неприменимо?
* Отсутствие итеративности
  Сходимость обычно анализируют в контексте методов, которые последовательно приближаются к решению через итерации. В методе Штрассена нет итераций — он разово применяет набор рекурсивных операций для вычисления произведения матриц, поэтому понятие «приближения к решению» здесь неприменимо.

* Детерминированный результат
  Алгоритм Штрассена всегда возвращает точное произведение матриц (при отсутствии ошибок округления). В отличие от итерационных методов, он не генерирует последовательность приближений, которая может стремиться к точному решению.


Очевидный способ вычисления правой стороны — просто сделать 8 умножений и 4 сложения. Но представьте, что умножения намного дороже сложений, поэтому мы хотим уменьшить количество умножений, если это вообще возможно. Штрассен использует трюк, чтобы вычислить правую сторону с одним умножением меньше и намного большим количеством сложений (и нескольких вычитаний).

Вот 7 умножений (пока это просто хитрые умножения, тут можно не искать логику):

$
M1 = (A + D) * (E + H) = AE + AH + DE + DH \\
M2 = (C + D) * E = CE + DE \\
M3 = A * (F - H) = AF - AH \\
M4 = D * (G - E) = DG - DE \\
M5 = (A + B) * H = AH + BH \\
M6 = (C - A) * (E + F) = CE + CF - AE - AF \\
M7 = (B - D) * (G + H) = BG + BH - DG - DH \\
$


Теперь сделаем несколько простых сложений и умножений.

Для AE+BG:
1. Итак, чтобы вычислить AE+BG, начнем с M1+M7 (что дает нам члены AE и BG)
$$M1 + M7 = (AE + AH + DE + DH) + (BG + BH - DG - DH)$$
$$M1 + M7 = AE + AH + DE + BG + BH - DG$$
$$M1 + M7 = AE + BG + AH + DE  + BH - DG$$
$$M1 + M7 = (AE + BG) + (AH + DE  + BH - DG)$$
2. затем прибавим/вычтем некоторые другие M, пока AE+BG не останется всем.
$$M1 + M7 + M4 = (AE + BG) + (AH + DE  + BH - DG) + (DG - DE)$$
$$M1 + M7 + M4 = (AE + BG) + (AH + BH)$$
$$M1 + M7 + M4 - M5 = (AE + BG) + (AH + BH) - (AH + BH)$$
$$M1 + M7 + M4 - M5 = (AE + BG)$$
Чудесным образом M выбираются так, что $M1+M7+M4-M5$ работает. То же самое с другими тремя требуемыми результатами.

Для AF+BH:
$$M3 + M5 = (AF - AH) + (AH + BH)$$
$$M3 + M5 = AF + BH - AH + AH$$
$$M3 + M5 = AF + BH$$

Для CE+DG:
$$M2 + M4 = (CE + DE) + (DG - DE)$$
$$M2 + M4 = CE + DG + DE - DE$$
$$M2 + M4 = CE + DG$$

Для CF+DH:
$$M1 - M2 = (AE + AH + DE + DH) - (CE + DE)$$
$$M1 - M2 = AE + AH + DE + DH - CE - DE$$
$$M1 - M2 = AE + AH + DH - CE + DE - DE$$
$$M1 - M2 = AE + AH + DH - CE$$

$$M1 - M2 + M3 = (AE + AH + DH - CE) + (AF - AH)$$
$$M1 - M2 + M3 = AE + AH + DH - CE + AF - AH$$
$$M1 - M2 + M3 = AE + DH - CE + AF + AH - AH$$
$$M1 - M2 + M3 = AE + DH - CE + AF$$

$$M1 - M2 + M3 + M6 = (AE + DH - CE + AF) + (CE + CF - AE - AF)$$
$$M1 - M2 + M3 + M6 = AE + DH - CE + AF + CE + CF - AE - AF$$
$$M1 - M2 + M3 + M6 = DH + CF + AE - AE - CE + CE + AF - AF$$
$$M1 - M2 + M3 + M6 = DH + CF$$

Теперь просто надо понять, что это работает не только для матриц 2x2, но и для любых (четных) матриц. При этом мы рекурсивно уменьшаем каждую матрицу.
''')

def q1_th():
    print('''**Определение**. Произведение матрицы $A$ размера $n×k$ и матрицы $B$ размера $k×m$– это матрица $C$ размера $n×m$ такая что её элементы записываются как $$c_{ij}=∑_{s=1}^{k}a_{is}b_{sj},i=1,…,n,j=1,…,m$$

Для $m=k=n$ сложность наивного алгоритма составляет $2n^3−n^2=O(n^3)$:

Почему рукописная(наивная) реализация такая медленная?

1) не используется параллелилизм

2) не используются преимущества быстрой памяти, в целом архитектуры памяти

**Определение**. Произведение матрицы $A$ размера $n×k$ и вектора $B$ размера $1×k$– это вектор $C$ длины $n$, такой, что его элементы записываются как $$c_{i}=∑_{j=1}^{k}a_{ij}b_{j},i=1,…,n$$''')
    
def q1():
    print('''def matmul(a, b): #наивное перемножение матриц
        n = a.shape[0]
        k = a.shape[1]
        m = b.shape[1]
        c = np.zeros((n, m))
        for i in range(n):
            for j in range(m):
                for s in range(k):
                    c[i, j] += a[i, s] * b[s, j]
        return c


    def mat_vec_mult(matrix, vector): # Наивное перемножение матрицы на вектор
        num_rows = len(matrix)
        num_cols = len(matrix[0])
        result = [0] * num_rows

        for i in range(num_rows):
            for j in range(num_cols):
                result[i] += matrix[i,j] * vector[j]

        return result


    mat_vec_mult(np.array([[1,2],[2,3],[4,7]]),[4,5]), matmul(np.array([[1,2],[2,3]]),np.array([[1,2],[2,3]]))''')
    
    
def q2():
    print('''
    from collections import OrderedDict

    class LRUCache:
        def __init__(self, capacity: int):
            """
            Инициализация кэша с заданной ёмкостью.
            """
            self.cache = OrderedDict()
            self.capacity = capacity

        def get(self, key: int) -> int:
            """
            Получить значение из кэша по ключу.
            Если ключа нет, вернуть -1.
            """
            if key in self.cache:
                # Переместить используемый элемент в конец (считается недавно использованным)
                self.cache.move_to_end(key)
                return self.cache[key]
            return -1

        def put(self, key: int, value: int):
            """
            Добавить элемент в кэш. Если кэш заполнен, удалить наименее используемый элемент.
            """
            if key in self.cache:
                # Если ключ уже существует, обновить значение и переместить в конец
                self.cache.move_to_end(key)
            self.cache[key] = value
            if len(self.cache) > self.capacity:
                # Удалить первый (наименее недавно использованный) элемент
                self.cache.popitem(last=False)

    # Пример использования
    cache = LRUCache(2)

    # Операции
    cache.put(1, 1)  # Кэш: {1: 1}
    cache.put(2, 2)  # Кэш: {1: 1, 2: 2}
    print(cache.get(1))  # Вернет 1, Кэш: {2: 2, 1: 1}
    cache.put(3, 3)  # Кэш: {1: 1, 3: 3} (удален 2)
    print(cache.get(2))  # Вернет -1 (так как 2 удален)
    cache.put(4, 4)  # Кэш: {3: 3, 4: 4} (удален 1)
    print(cache.get(1))  # Вернет -1
    print(cache.get(3))  # Вернет 3, Кэш: {4: 4, 3: 3}
    print(cache.get(4))  # Вернет 4, Кэш: {3: 3, 4: 4}''')


def q2_th():
    print('''Иерархия памяти — это способ организации различных типов памяти компьютера так, чтобы ускорить работу процессора. Память делится на уровни по скорости доступа и размеру: быстрые уровни маленькие и дорогие, а медленные — большие и дешевые.

**Уровни иерархии:**

**1. Регистр процессора:**

- Самый верхний уровень.
- Находится внутри процессора.
- Скорость: сверхбыстрая, доступ за один такт процессора.
- Объем: крошечный (несколько килобайт).
- Стоимость: очень высокая.

**2. Кэш-память процессора (CPU Cache):**
Состоит из 3 уровней:
- L1 (уровень 1):
  - Самая быстрая и дорогая кэш-память.
  - Очень маленький объем (16–128 КБ).
-L2 (уровень 2):
  - Чуть медленнее, но больше (256 КБ – несколько МБ).
- L3 (уровень 3):
  - Медленнее L1 и L2, общий для всех ядер процессора.
  - Объем до десятков МБ.
- Назначение: хранение данных, часто используемых процессором, для минимизации задержек.

**3. Оперативная память (RAM):**

- Скорость: медленнее кэша, но быстрее SSD.
- Объем: значительно больше (гигабайты).
- Стоимость: относительно дорогая.
- Используется для хранения данных и инструкций программ во время выполнения.

**4. Твердотельные накопители (Solid State Drives):**

- Включают неэнергозависимую флэш-память.
- Скорость: медленнее RAM, но быстрее, чем механические жесткие диски.
- Объем: большие (терабайты).
- Стоимость: средняя.
- Используются для долговременного хранения данных.
- Механические жесткие диски (HDD):

**5. Самый нижний уровень.**
- Скорость: самая медленная.
- Объем: очень большой (терабайты).
- Стоимость: самая низкая.
- Используются для долговременного хранения данных, к которым доступ требуется редко.


**План кеша (Cache Planning)**
Кэш работает как промежуточный буфер между процессором и оперативной памятью для ускорения доступа к часто используемым данным. Процесс организации кэша включает следующие аспекты:

**1. Кэш-линии:**

- Данные организуются в блоки фиксированного размера (обычно 32–128 байт).
- При кэшировании загружается вся кэш-линия, а не только отдельный байт.

**2. Ассоциативность кэша:**

- Определяет, как строки памяти сопоставляются с блоками в кэше.
  - Прямое отображение: каждая строка памяти может храниться только в определенном блоке кэша.
  - Полностью ассоциативный кэш: каждая строка памяти может находиться в любом блоке кэша.
  - N-канальный ассоциативный кэш: компромисс между двумя подходами.
  
**3. Алгоритмы замещения:**

- Когда кэш заполняется, нужно освободить место для новых данных.
- Пример: LRU (Least Recently Used), FIFO (First In, First Out), Random Replacement.


**Алгоритм LRU (Least Recently Used)**
LRU — один из самых распространенных алгоритмов замещения данных в кэше.

**Принцип работы:**

- При кэш-промахе (отсутствие данных в кэше) заменяется тот блок данных, который дольше всех не использовался.
- Данные, к которым был последний доступ, считаются самыми "свежими".

**Реализация:**

- Используется структура данных (обычно связанный список или стек).

**При каждом доступе к данным:**

- Если данные уже в кэше — переместить их в начало списка.
- Если данных нет в кэше:
- Если кэш заполнен, удалить последний элемент списка (самый "старый").
- Добавить новые данные в начало списка.

**Плюсы:**

- Эффективно для данных с локальностью запросов.
- Снижает частоту промахов.

**Минусы:**

Увеличенные затраты на обновление структуры (в худшем случае $\mathscr{O}(n)$)''')
