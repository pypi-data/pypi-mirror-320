import torch
from dataclasses import dataclass
from typing import Optional, List, Union, Tuple, Callable
import platform
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.parametrize as P
from tqdm import tqdm
from transformers import LlamaModel, LlamaConfig
from transformers.cache_utils import Cache
from transformers.modeling_outputs import BaseModelOutputWithPast
from transformers.utils import is_flash_attn_2_available

from ChatTTS.utils import del_all
from ChatTTS import model
import logging

import torch._dynamo

torch._dynamo.config.suppress_errors = True


class GPT(nn.Module):
    def __init__(
        self,
        gpt_config: dict,
        embed: model.Embed,
        use_flash_attn=False,
        use_vllm=False,
        device=torch.device("cpu"),
        device_gpt=torch.device("cpu"),
        logger=logging.getLogger(__name__),
    ):
        super().__init__()

        self.logger = logger

        self.device = device
        self.device_gpt = device_gpt

        self.generator = torch.Generator(device=device)

        self.num_vq = int(gpt_config["num_vq"])
        self.num_audio_tokens = int(gpt_config["num_audio_tokens"])
        self.num_text_tokens = int(gpt_config["num_text_tokens"])

        self.use_flash_attn = use_flash_attn
        self.is_te_llama = False
        self.is_vllm = use_vllm

        if self.is_vllm:
            return

        self.llama_config = self._build_llama_config(gpt_config)

        self.emb_code = [ec.__call__ for ec in embed.emb_code]
        self.emb_text = embed.emb_text.__call__
        self.head_text = embed.head_text.__call__
        self.head_code = [hc.__call__ for hc in embed.head_code]

    def load_pretrained(
        self, gpt_folder: str, embed_file_path: str, gpu_memory_utilization: float = 0.5
    ):
        if self.is_vllm and platform.system().lower() == "linux":
            from ChatTTS.model.velocity import LLM

            self.llm = LLM(
                model=gpt_folder,
                num_audio_tokens=self.num_audio_tokens,
                num_text_tokens=self.num_text_tokens,
                post_model_path=embed_file_path,
                gpu_memory_utilization=gpu_memory_utilization,
            )
            self.logger.info("vLLM model loaded")
            return

        self.gpt: LlamaModel = LlamaModel.from_pretrained(gpt_folder).to(
            self.device_gpt
        )
        del self.gpt.embed_tokens

    class Context:
        def __init__(self):
            self._interrupt = False

        def set(self, v: bool):
            self._interrupt = v

        def get(self) -> bool:
            return self._interrupt

    def _build_llama_config(
        self,
        config: dict,
    ) -> Tuple[LlamaModel, LlamaConfig]:
        if self.use_flash_attn and is_flash_attn_2_available():
            llama_config = LlamaConfig(
                **config,
                attn_implementation="flash_attention_2",
            )
            self.logger.warning(
                "enabling flash_attention_2 may make gpt be even slower"
            )
        else:
            llama_config = LlamaConfig(**config)

        return llama_config

    def prepare(self, compile=False):
        if self.use_flash_attn and is_flash_attn_2_available():
            self.gpt = self.gpt.to(dtype=torch.float16)
        if compile and not self.is_te_llama and not self.is_vllm:
            try:
                self.compile(backend="inductor", dynamic=True)
                self.gpt.compile(backend="inductor", dynamic=True)
            except RuntimeError as e:
                self.logger.warning(f"compile failed: {e}. fallback to normal mode.")

    @dataclass(repr=False, eq=False)
    class _GenerationInputs:
        position_ids: torch.Tensor
        cache_position: torch.Tensor
        use_cache: bool
        input_ids: Optional[torch.Tensor] = None
        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
        attention_mask: Optional[torch.Tensor] = None
        inputs_embeds: Optional[torch.Tensor] = None

        def to(self, device: torch.device, dtype: torch.dtype):
            if self.attention_mask is not None:
                self.attention_mask = self.attention_mask.to(device, dtype=dtype)
            if self.position_ids is not None:
                self.position_ids = self.position_ids.to(device, dtype=dtype)
            if self.inputs_embeds is not None:
                self.inputs_embeds = self.inputs_embeds.to(device, dtype=dtype)
            if self.cache_position is not None:
                self.cache_position = self.cache_position.to(device, dtype=dtype)

    @torch.no_grad()
    def _prepare_generation_inputs(
        self,
        input_ids: torch.Tensor,
        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        cache_position: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        use_cache=True,
    ) -> _GenerationInputs:
        # With static cache, the `past_key_values` is None
        # TODO joao: standardize interface for the different Cache classes and remove of this if
        has_static_cache = False
        if past_key_values is None:
            if hasattr(self.gpt.layers[0], "self_attn"):
                past_key_values = getattr(
                    self.gpt.layers[0].self_attn, "past_key_value", None
                )
            has_static_cache = past_key_values is not None

        past_length = 0
        if past_key_values is not None:
            if isinstance(past_key_values, Cache):
                past_length = (
                    int(cache_position[0])
                    if cache_position is not None
                    else past_key_values.get_seq_length()
                )
                max_cache_length = past_key_values.get_max_length()
                cache_length = (
                    past_length
                    if max_cache_length is None
                    else min(max_cache_length, past_length)
                )
            # TODO joao: remove this `else` after `generate` prioritizes `Cache` objects
            else:
                cache_length = past_length = past_key_values[0][0].shape[2]
                max_cache_length = None

            # Keep only the unprocessed tokens:
            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where
            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as
            # input)
            if (
                attention_mask is not None
                and attention_mask.shape[1] > input_ids.shape[1]
            ):
                start = attention_mask.shape[1] - past_length
                input_ids = input_ids.narrow(1, -start, start)
            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard
            # input_ids based on the past_length.
            elif past_length < input_ids.shape[1]:
                input_ids = input_ids.narrow(
                    1, past_length, input_ids.size(1) - past_length
                )
            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.

            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.
            if (
                max_cache_length is not None
                and attention_mask is not None
                and cache_length + input_ids.shape[1] > max_cache_length
            ):
                attention_mask = attention_mask.narrow(
                    1, -max_cache_length, max_cache_length
                )

        if attention_mask is not None and position_ids is None:
            # create position_ids on the fly for batch generation
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask.eq(0), 1)
            if past_key_values:
                position_ids = position_ids.narrow(
                    1, -input_ids.shape[1], input_ids.shape[1]
                )

        input_length = (
            position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]
        )
        if cache_position is None:
            cache_position = torch.arange(
                past_length, past_length + input_length, device=input_ids.device
            )
        else:
            cache_position = cache_position.narrow(0, -input_length, input_length)

        if has_static_cache:
            past_key_values = None

        model_inputs = self._GenerationInputs(
            position_ids=position_ids,
            cache_position=cache_position,
            use_cache=use_cache,
        )

        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step
        if inputs_embeds is not None and past_key_values is None:
            model_inputs.inputs_embeds = inputs_embeds
        else:
            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise
            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114
            # TODO: use `next_tokens` directly instead.
            model_inputs.input_ids = input_ids.contiguous()

        model_inputs.past_key_values = past_key_values
        model_inputs.attention_mask = attention_mask

        return model_inputs

    @dataclass(repr=False, eq=False)
    class GenerationOutputs:
        ids: List[torch.Tensor]
        attentions: List[Optional[Tuple[torch.FloatTensor, ...]]]
        hiddens: List[torch.Tensor]

        def destroy(self):
            del_all(self.ids)
            del_all(self.attentions)
            del_all(self.hiddens)

    @torch.no_grad()
    def _prepare_generation_outputs(
        self,
        inputs_ids: torch.Tensor,
        start_idx: int,
        end_idx: torch.Tensor,
        attentions: List[Optional[Tuple[torch.FloatTensor, ...]]],
        hiddens: List[torch.Tensor],
        infer_text: bool,
    ) -> GenerationOutputs:
        inputs_ids = [
            inputs_ids[idx].narrow(0, start_idx, i) for idx, i in enumerate(end_idx)
        ]
        if infer_text:
            inputs_ids = [i.narrow(1, 0, 1).squeeze_(1) for i in inputs_ids]

        if len(hiddens) > 0:
            hiddens = torch.stack(hiddens, 1)
            hiddens = [
                hiddens[idx].narrow(0, 0, i) for idx, i in enumerate(end_idx.int())
            ]

        return self.GenerationOutputs(
            ids=inputs_ids,
            attentions=attentions,
            hiddens=hiddens,
        )

    @torch.no_grad()
    def generate(
        self,
        emb: torch.Tensor,
        inputs_ids: torch.Tensor,
        temperature: torch.Tensor,
        eos_token: Union[int, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        max_new_token=2048,
        min_new_token=0,
        logits_processors: Tuple[
            Callable[[torch.LongTensor, torch.FloatTensor], torch.FloatTensor]
        ] = (),
        infer_text=False,
        return_attn=False,
        return_hidden=False,
        stream=False,
        show_tqdm=True,
        ensure_non_empty=True,
        stream_batch=24,
        manual_seed: Optional[int] = None,
        context=Context(),
    ):
        attentions: List[Optional[Tuple[torch.FloatTensor, ...]]] = []
        hiddens = []
        stream_iter = 0

        start_idx, end_idx = (
            inputs_ids.shape[1],
            torch.zeros(
                inputs_ids.shape[0], device=inputs_ids.device, dtype=torch.long
            ),
        )
        finish = torch.zeros(inputs_ids.shape[0], device=inputs_ids.device).bool()

        old_temperature = temperature

        temperature = (
            temperature.unsqueeze(0)
            .expand(inputs_ids.shape[0], -1)
            .contiguous()
            .view(-1, 1)
        )

        attention_mask_cache = torch.ones(
            (
                inputs_ids.shape[0],
                inputs_ids.shape[1] + max_new_token,
            ),
            dtype=torch.bool,
            device=inputs_ids.device,
        )
        if attention_mask is not None:
            attention_mask_cache.narrow(1, 0, attention_mask.shape[1]).copy_(
                attention_mask
            )

        progress = inputs_ids.size(1)
        # pre-allocate inputs_ids
        inputs_ids_buf = torch.zeros(
            inputs_ids.size(0),
            progress + max_new_token,
            inputs_ids.size(2),
            dtype=inputs_ids.dtype,
            device=inputs_ids.device,
        )
        inputs_ids_buf.narrow(1, 0, progress).copy_(inputs_ids)
        del inputs_ids
        inputs_ids = inputs_ids_buf.narrow(1, 0, progress)

        pbar: Optional[tqdm] = None

        if show_tqdm:
            pbar = tqdm(
                total=max_new_token,
                desc="text" if infer_text else "code",
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}(max) [{elapsed}, {rate_fmt}{postfix}]",
            )

        past_key_values = None

        for i in range(max_new_token):
            model_input = self._prepare_generation_inputs(
                inputs_ids,
                past_key_values,
                attention_mask_cache.narrow(1, 0, inputs_ids.shape[1]),
                use_cache=not self.is_te_llama,
            )

            if i > 0:
                del emb
                inputs_ids_emb = model_input.input_ids.to(self.device_gpt)
                if infer_text:
                    emb: torch.Tensor = self.emb_text(inputs_ids_emb[:, :, 0])
                else:
                    code_emb = [
                        self.emb_code[i](inputs_ids_emb[:, :, i])
                        for i in range(self.num_vq)
                    ]
                    emb = torch.stack(code_emb, 3).sum(3)
                del inputs_ids_emb, model_input.input_ids
            model_input.inputs_embeds = emb

            model_input.to(self.device_gpt, self.gpt.dtype)

            outputs: BaseModelOutputWithPast = self.gpt(
                attention_mask=model_input.attention_mask,
                position_ids=model_input.position_ids,
                past_key_values=model_input.past_key_values,
                inputs_embeds=model_input.inputs_embeds,
                use_cache=model_input.use_cache,
                output_attentions=return_attn,
                cache_position=model_input.cache_position,
            )
            del_all(model_input)
            attentions.append(outputs.attentions)
            hidden_states = outputs.last_hidden_state.to(
                self.device, dtype=torch.float
            )  # ğŸ»
            past_key_values = outputs.past_key_values
            del_all(outputs)
            if return_hidden:
                hiddens.append(hidden_states.narrow(1, -1, 1).squeeze_(1))

            with P.cached():
                if infer_text:
                    logits: torch.Tensor = self.head_text(hidden_states)
                else:
                    # logits = torch.stack([self.head_code[i](hidden_states) for i in range(self.num_vq)], 3)
                    logits = torch.empty(
                        hidden_states.size(0),
                        hidden_states.size(1),
                        self.num_audio_tokens,
                        self.num_vq,
                        dtype=torch.float,
                        device=self.device,
                    )
                    for num_vq_iter in range(self.num_vq):
                        x: torch.Tensor = self.head_code[num_vq_iter](hidden_states)
                        logits[..., num_vq_iter] = x
                        del x

            del hidden_states

            # logits = logits[:, -1].float()
            logits = logits.narrow(1, -1, 1).squeeze_(1).float()

            if not infer_text:
                # logits = rearrange(logits, "b c n -> (b n) c")
                logits = logits.permute(0, 2, 1)
                logits = logits.reshape(-1, logits.size(2))
                # logits_token = rearrange(inputs_ids[:, start_idx:], "b c n -> (b n) c")
                inputs_ids_sliced = inputs_ids.narrow(
                    1,
                    start_idx,
                    inputs_ids.size(1) - start_idx,
                ).permute(0, 2, 1)
                logits_token = inputs_ids_sliced.reshape(
                    inputs_ids_sliced.size(0) * inputs_ids_sliced.size(1),
                    -1,
                ).to(self.device)
                del inputs_ids_sliced
            else:
                logits_token = (
                    inputs_ids.narrow(
                        1,
                        start_idx,
                        inputs_ids.size(1) - start_idx,
                    )
                    .narrow(2, 0, 1)
                    .to(self.device)
                )

            logits /= temperature

            for logitsProcessors in logits_processors:
                logits = logitsProcessors(logits_token, logits)

            del logits_token

            if i < min_new_token:
                logits[:, eos_token] = -torch.inf

            scores = F.softmax(logits, dim=-1)

            del logits

            if manual_seed is None:
                idx_next = torch.multinomial(scores, num_samples=1).to(finish.device)
            else:
                idx_next = torch.multinomial(
                    scores,
                    num_samples=1,
                    generator=self.generator.manual_seed(manual_seed),
                ).to(finish.device)

            del scores

            if not infer_text:
                # idx_next = rearrange(idx_next, "(b n) 1 -> b n", n=self.num_vq)
                idx_next = idx_next.view(-1, self.num_vq)
                finish_or = idx_next.eq(eos_token).any(1)
                finish.logical_or_(finish_or)
                del finish_or
                inputs_ids_buf.narrow(1, progress, 1).copy_(idx_next.unsqueeze_(1))
            else:
                finish_or = idx_next.eq(eos_token).any(1)
                finish.logical_or_(finish_or)
                del finish_or
                inputs_ids_buf.narrow(1, progress, 1).copy_(
                    idx_next.unsqueeze_(-1).expand(-1, -1, self.num_vq),
                )

            if i == 0 and finish.any():
                self.logger.warning(
                    "unexpected end at index %s",
                    str([unexpected_idx.item() for unexpected_idx in finish.nonzero()]),
                )
                if ensure_non_empty and manual_seed is None:
                    if show_tqdm:
                        pbar.close()
                    self.logger.warning("regenerate in order to ensure non-empty")
                    del_all(attentions)
                    del_all(hiddens)
                    del (
                        start_idx,
                        end_idx,
                        finish,
                        temperature,
                        attention_mask_cache,
                        past_key_values,
                        idx_next,
                        inputs_ids_buf,
                    )
                    new_gen = self.generate(
                        emb,
                        inputs_ids,
                        old_temperature,
                        eos_token,
                        attention_mask,
                        max_new_token,
                        min_new_token,
                        logits_processors,
                        infer_text,
                        return_attn,
                        return_hidden,
                        stream,
                        show_tqdm,
                        ensure_non_empty,
                        stream_batch,
                        manual_seed,
                        context,
                    )
                    for result in new_gen:
                        yield result
                    del inputs_ids
                return

            del idx_next
            progress += 1
            inputs_ids = inputs_ids_buf.narrow(1, 0, progress)

            not_finished = finish.logical_not().to(end_idx.device)
            end_idx.add_(not_finished.int())
            stream_iter += not_finished.any().int()
            if stream:
                if stream_iter > 0 and stream_iter % stream_batch == 0:
                    self.logger.debug("yield stream result, end: %d", end_idx)
                    yield self._prepare_generation_outputs(
                        inputs_ids,
                        start_idx,
                        end_idx,
                        attentions,
                        hiddens,
                        infer_text,
                    )
            del not_finished

            if finish.all() or context.get():
                break

            if pbar is not None:
                pbar.update(1)

        if pbar is not None:
            pbar.close()

        if not finish.all():
            if context.get():
                self.logger.warning("generation is interrupted")
            else:
                self.logger.warning(
                    f"incomplete result. hit max_new_token: {max_new_token}"
                )

        del finish, inputs_ids_buf

        yield self._prepare_generation_outputs(
            inputs_ids,
            start_idx,
            end_idx,
            attentions,
            hiddens,
            infer_text,
        )


@dataclass(repr=False, eq=False)
class RefineTextParams:
    prompt: str = ""
    top_P: float = 0.7
    top_K: int = 20
    temperature: float = 0.7
    repetition_penalty: float = 1.0
    max_new_token: int = 384
    min_new_token: int = 0
    show_tqdm: bool = True
    ensure_non_empty: bool = True
    manual_seed: Optional[int] = None


class Embed(model.Embed):
    def __init__(
        self,
        hidden_size,
        num_audio_tokens,
        num_text_tokens,
        num_vq,
        model_path,
        device="cpu",
    ):
        super().__init__(hidden_size, num_audio_tokens, num_text_tokens, num_vq)
        self.load_pretrained(model_path, device=device)


class StreamGPT(GPT):
    def __init__(
        self,
        config,
        embed_path,
        model_path,
        tokenizer_path,
        top_k=20,
        top_p=0.7,
        temperature=0.1,
        repetition_penalty=1.05,
        min_new_token=0,
        max_new_token=2048,
        manual_seed=None,
        compile=False,
        device="cpu",
        use_vllm=False,
        gpu_memory_utilization=0.5,
    ):
        spk_stat = "æ„ç©¤å·©å™…å»·æˆ‡ç¬‰å±ˆç™åª„å¹å§å¸¶çˆ²æ¼ˆå¡€æ®æ…„äº…å€´åº²èˆ´çŒ‚ç‘ˆåœç‹´å¤¥åœ“å¸æˆ›æŒ è…‰è€åŠ¤å½å–³å¹¾æˆ˜è¬‡è€å´’æ „å‘¥å€¸åº­ç‡¡æ¬ˆæè¥è¤„ä¹­åŸ—å¹ºçˆƒå¼”æ‘æ–æ”å…•ä½–å»èˆç«¾è±ƒç£å§“è¶¡ä½„å¹’çˆšæ¬„è±„è®çš³è¨µä»©å¸†æŠ•è¬Œèƒèå„åœä¼†å¹¦æŠ‚èŒå‘„æ‘æ–ƒè®¹å‚®åºçˆ£èœ€æ©åç¥„äº¥å…¡å¸¸çˆ‚æ¬æ‰‰ä¸æµ”ä½±åƒˆå¼·æ‰•ä¼…æ‰‚è›å¾´æ†å‚å·€æˆºæ¬€è‰‚çå—´å•¥å€¤å½·åˆ‚æ¬Šç©ˆæ‰’å¤ä¿”è´²åº›åˆç¬‚å„è´æ´ä»­äºåº›å‰çŒ¢æ‰ƒç¼è¶¤åˆåµå¹ªèˆä¼Œç…å©æ½¤æ™ä½å¼¾èˆ™èŒ¥ç©è‘è £è¨‘ä¼åº¤åˆŠç¬æ©æº‘åƒ”äº‘ååº¯æˆšä¼æ½‰è†è„´åƒµå™”å»ƒè‰…åŒŠç¥‚å”æ†´å£å—™å¸­çˆ¥æ¬è™è°ç‰´å¸½åŠ¿å¼¿ç‰³èœå…€è›å‚„å–©ä¸¿å¸”åˆ”åœ†è¡å»ç½¤åºä¿ƒå¸™åŠ¢ä¼ˆæ±„æ¨æª„å‹µä¼´å¼èˆ‘æ¬ç½…è™æ˜´åŠ­å‹…å¸œåˆ¼æœŠè•è™è“´æ¨‘ä¼«å¹¨æ‰‘è¬ªå‰€å ç¨´ä¸µä¼±å¼èˆ®è«¸èµç¿’ä¿”å®¹å±å¹«ç‰¶è¬ƒå­„ç³ç­”å—åƒŠå¸œç‡²ç¬„çµ‚ç€’åˆ¤ä¹…åƒ¤å¸˜çˆ´èŒ‡åƒå­‘å†„å‡•ä½³å¼•æ‰èœæ­ç¼è£„å‰½å„ºæ˜çˆ‹æœçœ¿å»å‘„å¡å˜‡å¹»çˆ±èŒ è©è¨å‰´å”­ä¿å¹¾æˆŠæ¬€ç¡èè´„æ¥•å’å·¡çˆ€å¼å±„èç³è³™å‡¶å½åˆ…æ¼„å€å”æº´å‰‘åŠ‹åº½èˆ½çŒ„ç…ƒè·å¤”æƒ¥ä¼¾åº®èˆä¼ˆç½å‘å„æ€…ä¸šæ€¯åˆæœ‡çå¶è¦”å©ä¿³å·¶çˆœæœæ½å´è„ä¿¹å‡›å¸¸çˆºç¬Œç©€èæ­¤å¤¡å€›å¸¡åˆ€åŒ‰çµ‚çªèˆ£è²©ä¾½æ€¿æ‰‰ä¼¥è´¿æ†å¿“è¬©å§†å¹ŒçŠŠæ¼‚æ…†ç™’å´ç”å…å¸¼æˆæ¬…è©‚æµæœ”ä»¹å£­å¸°è‡·å¼æ‡èç¤å¸¡å–å¸˜çˆä¼…è…‚çšçº¤å›…å……å¹“æˆ ä¼¥ç‚ä¸è¨¤æˆ±å€±å¼‹çˆ®å¬Œç™æå­„ä¾¥åŠ¬å¿¶åˆ“åœ‹è©€æ¡’å¤å©å˜„åº¬æˆšèŒèµ‚ç›‘ç‡¤å˜‘å‹Œå¹¦èˆ½æŒå‘‚è«æ£¤å§‘å†åº•èˆ¡ç¬è‰ƒç€å­´å€‰å‚”å¼‹çˆ”çŒ ä¹æ¿‘å¡„å½å˜§æ‚èˆ›ç¼‡è¥ƒåçª´ä»¡åˆ±å¿•åˆ¥æ¼‡ç©å²ç¼´å»½ä»·åºŒçˆŠè¬ˆç¡„è®‘æƒ¤å€å„‚åº­çˆ‹ä¼‡è‚å¶è”æ‘å‚ åº“åˆèŒ„æ­ƒæˆè–¤ä¼ä¼¯å»®åˆ›ç¬ å¡„ç†å…´å‹½ä¿„å¸…å‰‰æœ€è…€ç æ•¤åä¾å¼†æˆºæœ’è™ƒæ—èš„æ¢•äº–å¹”ç‰»æœ£æ‰…è´ç”å å™…å¸¡å‰Œåœ…æ‘€å´å½¤æµåƒ³åº™çˆ–å¬‡å•æ¸æ‚¤å ä¸›å¹†åˆ§æŒœå½ƒæ‚å¹¤åˆ¹åšŸæ•èŠçœ‹è€æ‘ç„”å‘ä¹å¸–çˆ­æ¬ç™ƒç³’åœ„å¼™ä½±å»œæˆ¤è¬å©€å’æ˜´ç„äº©å»¦è‰æ‹¼è¬¿èŠç™¤æ€¹å…½å¹¸èˆ³æœ‡ç•å–ç¨”æ¯ä¸¼å¼ˆæ‡²æŒ€è­‚å‹‘å“´å•ä¼å¸¸èˆ­ç¬¯æ™å ‘ä¿„å©å‰”å»Ÿçˆæ¬¦çµå¤’ä¼¤ä¼‘å‚‘å»³æˆŒèœ…æ½†ç™å½´æ‘‘å‹¯åºŠåˆ½æ¬…è‰ç å¿„æ‰ä»å»¡èˆŠçŒ¥æ½‚å”å§”ä»±åƒœå»¼çˆ¤æœ„å‘ƒå¼ç¤”æ»µå“å¹©çˆ„æŒ‚ç­ä¹ç±¤åˆ•å‡Ÿå¹µçˆ å¼‰ç™…ä¹‘å´å‹¥ä¼–å¸ªèˆ©èŒ†å©ç¢å¹¤å­ä¹¢å·œè‰³çŒæ¡€æ¡å•„å”©ä¿Šå¹èˆ®çŒ€è‰…ç„è”ç½äº€å¸‹çˆœç¼…å™ƒå’æ–¤å–©äºˆå¹©çˆ›ç¬†æ‘€æµçŒ´ä¾ä¾¹å¹ƒåˆ•åœ’æ…„è›æ ¤æ¾¹ä»‘åº§çˆ¼è¬‰æ¡ƒæ…æµ”æ–•å»å¹›æ‡°å¬“è¡æ„æ°„æ‚…ä»¿åº”èŠ”æ¼„è¡ƒæ•è¬¤å‚åŒ©å¹¹æŠƒåœ‰ç™„å»è£„å±µå™‰å¹åˆ©è¬è‚æè›”åš™åæ€—èˆåœç•ƒè†æ „åˆµä¸œå·†æˆ¤è«¾å‘ƒå‘åª¤å—¨è·å¿¶çˆçœ„ç¥‚æœ’å¶”åƒ­åŠ‰å¿¾åˆåŒ‹ç™„è¢ç¿´ç…åƒ·å»²èŠ„èŒˆæˆçšæ“„å´‘ä¼„å»‰ç‰åŒƒå‰ƒçŠæ¾¤å”‘ä¸„åººæˆƒä¼ƒç…€æŸæ„å™äº½å¸´åˆ‡ç¼Œç½„æŒå°´å™™å€°å¸¦èˆæ¼„æ©„å¡ç³´ä¿©åƒ¯å¸€èˆ¬æ¼€å‚æ æ›´ä¸¡ä¿‡å»±èˆŒçŒæ…‚æ‹å¤å¶±å¶åº”åˆªçœ‰çèŒä¼”å˜…åºå¸ŸèˆŠæ¼‚æ€æ æš„å–¡ä¹åº™èˆ†åŒ‚æ•€æ½‘æ”åŠ‘ä¾–å»¶æˆ¦ç›½æ€¶å”¯æ…³è˜èŸƒå­«å¨ç›Šè¢°çå±ƒç—¶ç¿®ç¬ªå„šè£€å€¹æ¤Œç»ç¿€è©µç­½èˆ˜æƒ¯å ¿æŸä¾°æ™ˆè—ç¼®è©—å»¦å¤¸å¦ç‘»ç€’è£”åª€æ†å”ƒå†¶ç’­ç‹»æ¸ è‘å¥¬ç†¹èŒ…æ„ºæ°°è£æ» ç¿¦å²“è¤Œæ³£å´²åš­æ¬“æ¹’è™å®ºçˆ„è›…æ„¸åºåŒƒå¸†èª”ç©®æ‡Œè“ªç·æ¾Œæ°‹æŠŒè¨™å±Œè‡å»›ç¸å¬å±ºå¸Œç–­å­å‡‚ç´‹æ–°ç…å½ƒè†²è·±å°ªæ‡çœ†çª´çå“æ¨è¸ç´­æ¦‚å›¥æ˜¾å£Œæ¦„å«å˜®å¬­è¦¤åª¸ä¾µä½®çƒ’è€¸è§Œå©€ç§‹ç‹ƒå¸¹è‘¯è¨¤æ¡œç³¨ç¬¾è…¢ä¼€è‚¶æ‚ç‚‚è‰¤ç¦–å²…è‡ºæƒ˜æ¢·çå‹ç›ä½¨å²§æ†³ç“§å˜´æ±¬è—Šæ„Œè˜¤å¶ ç¡´ç»¤èœ²è¥æ‹¬å‹¾è°‚ç¸¨å¦¥è“ªæ¾­ç«­è¢è—œçºç³²ç…®æ„†ç€¯å­¯ç“ç½‚è«ºå¡¿ç‡—ç‹Ÿå¼™è¡¯æ»ç¸·ä¸±ç³…è‡„æ¢±ç€®æ°å·³çŒ™äºŠç¬¦èƒ åŒƒæ³€å»åœƒè†‚è’ƒç±ç¤©å²ˆç°¹ç¼ŒåŠºç‡²è¤¡å­“è†œæ‹”è ¿è§®å‘‹ç…£åŒå°·ç†œè«–å¼²ç‰­ç´«å¯Šèªƒç´€æ©´è³¬å‚¸ç®å¼šçªƒä¾«ç°²æ…¯çƒ£æ¸½ç¥Œå£“åª¥å™œå¤½å¤›è«›ç¹ç–®ç¦„å†ªè¬‡åª½è¡¤ç›°ç¼ºç¹‘è–«å…¾è§åµ±æ‰“æ»½ç®ºåš¯å‡£ç‹¢è œå´¼è¦½çƒ¸ç°¶ç›¯ç±“æ‘€è‹¶å³¸æ‡—æ³²æ¶»å‡®æ„³ç·—å‰‹ç¬”æ‡†å»¡ç¿æ¤ç¤¤æƒè—¥å´è…ˆçƒ„ä¼¹äº¯æ˜£ç¿¬è¤çµ‹æ¡«åƒ¨å¨èŒä¸›çŸ„èœå¨ˆæ†Šè‹†å¡è“åš¢å«¼ç»»å´±å©‹å›±è ¸ç¯¯æ™£èŠ€ç¹¼ç´¢å…“åƒ–èª¹å²¯åœªè¤°è ‡å”“å¦·èƒ…å·æ¸®ç ›å‚ˆè·åµšå†ƒè³¼èµå³è£‹è‚èˆ¾ç¬¦ç†»å²³å¢©å¯®ç²ƒå‡²è¢‘å½šå¤ªç»²å¤´æ‘¯ç¹³ç‹ä¿¥ç±Œå†è«è¨»åå¹«æ“¤è©’å®’å‡•è³å”¶æ¢å™”å¼¼èª²å±¿è¦å›¨ç„¬æ«±æ’ªè®è¬ç°¸æ‡°æ««æ¶ºåµç»å±ªç¿”å³æ…˜æ»Ÿç†²æ˜±å†›çƒŠèˆ¿å°¦èˆ„ç³–å¥æºå‡‚å½†è²ç³´ç¦å›°çš»çç‰‹ç’è¯™å¶±è‡€å¼€è“ˆçœè…¼ä¸¢çº»å»æ†¤å«–æš­è¢­å´²è‚¸è›å¦’æ¦—ç´‰è°¨çª®è¢ƒç‘ èç»Šè…†äº¿å†²è‘å–‹ç¸”è©–å²‘å…¾ç»™å ¸èµæ—»æ¡€è›¨åª†è¨‚å³¦ç´·æ•¯å›¬åç­¨å²¸ç„¸æ‹­ç¬µæ®’å“œå¢’èå±“å¨“è«™æ¢°è‡®æœ›æ‘°èŠ‘å¯­å‡†åƒè°¹æ°æ—‹æ†¢è®å±ƒåˆ’æ¬£ç˜«è°è˜»å“ç¹ç±¥ç¦¦åƒ¿èªµçš¯å¢“ç‡€ç¸¿ç¬ç†¦ç»—ç¨¹æ¦çŸ»ç¶è““å¸¡æˆ“æ²ºåŒºæ‰ç•ƒæ´Šè©ªç³è£¶ç›°çª¶è€åŒåŠ‚èªåº©æƒæ»œæ²ºå“®å‘ƒç…è­ å´„æ§€çŒ„è‚¼è”æ“‹æ¹Œè ºç¯ƒæ¥è«Œç¦å®å «æŒªè£•å´‘æ…©ç‹²æ‚ ç…‹ä»›æ„ç ˆç²µå…«æ£å®³æ¥å¦‹è”è²¨å°µå¥‚è‹°æ€«èªå‚«å²†è•¯å±‡è„‰å¤ˆä»†èŒåˆ“ç¹¸èŠºå£¸ç¢—æ››æ±æˆ­ç‚»ç»å‡‰åªå…ç‹œçˆ´æ€°è³ƒçºè¢å¨·ç¦ƒè“¥è†¹è–ªæ¸»ç½¸çª¿ç²«å‡¾è¤„èˆºçª®å¢«å¹²è‹Šç¹å†åƒ®è¨¸å¤¯ç»›è“ªè™›ç¾½æ…²çƒæ†·è¶çŠè °èå¡æˆå»ç›æ¬å–“èœ®è­¤å´†æ¥å›˜çŸ‡è–­ä¼£è‰˜è™å¸´å¥®è‹¢æ¸¶è™æš£ç¿èƒå°¾ç¨ˆç³¶ç€´ç½åµšæ°®è‘¯ç¬«æ…æ£Œæ‚¶ç‚¯ç«»çˆ…ä»¬åª¡å§¢å«ºçª·åˆ®æ­«åŠˆè£©å±¬æ¤•è³‘èœ¹è–Šåˆ²ç¾©å“¯å°—è¤¦ç“€ç¨¾ç¤‹æ£çª¼èˆ«å°‹å§æ¤„ä¾¸å—«çºä¿®çº˜åªƒè…½è››ç¨¹æ¢­å‘›ç€ˆè˜Ÿç¸€ç¤‰è«–å¤µå”®ä¸»æ¢®è ‰å¨…å¨­è£€èª¼å¶­è¦³æ³å€Šç°ˆè¤ƒæ“ç¶¿å‚¬çƒæº¶è‹Šç¬›è¥¹æ«²ç›…å…­å›«ç©ä½ƒç²¨æ…¯ç“¢çœ¸æ—±èƒå©¨è”å²‹ç¥—å¢¼ç„»ç½‘ç‰»ç–è©†å³‹ç§‰èƒ³åª´è¢­æ¾“è³¢çµŒç¨Ÿå£©èƒ«ç¢¯åå›«å¶çº†çªˆæ§Šè³æ’¹ç’¬èƒç¼˜èª¾å®­æ„Šçœ—å–·ç›‘åŠ‹è˜è¨¯ç¸½æ§¿æ£­æˆ¾å¢®çŠ„æŒç¸ˆç°æ¨¥è›”æè¢­å«›æ†«å€†ç¯å¢µè³ˆç¾¯èŒè§³è’œè‡´å¨¢æ…„å‹’è¦¸è˜æ›²æ ‚è‘­å®†å¦‹çš½ç¼½å…ç›³çŒ¼è”‚ç³¥è§§çƒ³æª¸ä½¯æ†“ç…¶è”ç­¼ç§ç¹·ç²è†Œå¡„å‰°è®å¯¾è…•æ£¥æ¸½å¿²ä¿›æµªè­¬ç§›æƒ›å£’å˜¸æ·«å†»æ›„ç»ç ƒå¥«è²¯åº´çˆ…ç²“è„®è„¡å¨å¦–å³µè˜²è¨æƒ‹æ³Šè €ã´†"
        embed = Embed(
            config["hidden_size"],
            config["num_audio_tokens"],
            config["num_text_tokens"],
            config["num_vq"],
            embed_path,
            device,
        )
        super().__init__(
            gpt_config=config,
            embed=embed,
            use_flash_attn=False,
            use_vllm=use_vllm,
            device=device,
            device_gpt=device,
        )
        self.eval()
        self.load_pretrained(
            model_path, embed_path, gpu_memory_utilization=gpu_memory_utilization
        )
        if compile and "cuda" in device:
            self.prepare(compile=compile)

        self.config = config
        self.device = device
        self.embed = embed
        self.speaker = model.Speaker(config["hidden_size"], spk_stat, device)
        self.tokenizer = model.Tokenizer(tokenizer_path)

        self.num_code = config["num_audio_tokens"] - 1
        logits_warpers, logits_processors = model.gen_logits(
            num_code=self.num_code,
            top_P=top_p,
            top_K=top_k,
            repetition_penalty=repetition_penalty,
        )
        self.logits_processors = (*logits_processors, *logits_warpers)
        self.temperature = torch.tensor([temperature] * config["num_vq"], device=device)
        self.max_new_token = max_new_token
        self.min_new_token = min_new_token
        self.manual_seed = manual_seed
        self.random_spk_emb = self.speaker.sample_random()

    def generate(
        self, text, spk_emb=None, speech_tokens=None, prompt="", speed: int = 5
    ):
        prompt = f"{prompt}[speed_{speed}]"
        spk_emb = spk_emb or self.random_spk_emb
        text = self.speaker.decorate_code_prompts(
            [text], prompt=prompt, txt_smp=None, spk_emb=spk_emb
        )
        # speech_tokens: dvae.encode(wav)
        if speech_tokens is not None:
            speech_tokens = self.speaker.decode_prompt(speech_tokens)
        input_ids, attention_mask, text_mask = self.tokenizer.encode(
            text, self.config["num_vq"], speech_tokens, self.device
        )
        emb = self.embed(input_ids, text_mask)
        if spk_emb is not None:
            self.speaker.apply(
                emb,
                spk_emb,
                input_ids,
                self.tokenizer.spk_emb_ids,
                self.device,
            )

        num_tokens = 0
        for tokens in super().generate(
            emb,
            input_ids,
            temperature=self.temperature,
            eos_token=self.num_code,
            attention_mask=attention_mask,
            max_new_token=self.max_new_token,
            min_new_token=self.min_new_token,
            logits_processors=self.logits_processors,
            infer_text=False,
            return_hidden=True,
            stream=True,
            show_tqdm=True,
            ensure_non_empty=True,
            stream_batch=4,
            manual_seed=self.manual_seed,
        ):
            ids = tokens.ids[0].T[None, :, num_tokens:]
            num_tokens += ids.shape[2]
            yield ids

    def refine_text(
        self,
        text,
        params: RefineTextParams = RefineTextParams(
            prompt="[oral_7][laugh_0][break_6]"
        ),
    ):
        if not isinstance(text, list):
            text = [text]

        input_ids, attention_mask, text_mask = self.tokenizer.encode(
            text=self.speaker.decorate_text_prompts(text, params.prompt),
            num_vq=self.config["num_vq"],
            device=self.device_gpt,
        )

        logits_warpers, logits_processors = model.gen_logits(
            num_code=self.tokenizer.len,
            top_P=params.top_P,
            top_K=params.top_K,
            repetition_penalty=params.repetition_penalty,
        )

        emb = self.embed(input_ids, text_mask)

        del text_mask

        result = next(
            super().generate(
                emb,
                input_ids,
                temperature=torch.tensor([params.temperature], device=self.device),
                eos_token=self.tokenizer.eos_token,
                attention_mask=attention_mask,
                max_new_token=params.max_new_token,
                min_new_token=params.min_new_token,
                logits_processors=(*logits_processors, *logits_warpers),
                infer_text=True,
                stream=False,
                show_tqdm=params.show_tqdm,
                ensure_non_empty=params.ensure_non_empty,
                manual_seed=params.manual_seed,
            )
        )

        del emb, input_ids

        text_tokens = result.ids
        text_tokens = [i[i.less(self.tokenizer.break_0_ids)] for i in text_tokens]
        text = self.tokenizer.decode(text_tokens)
        result.destroy()
        return text[0]

    def load_pretrained(
        self, gpt_folder: str, embed_file_path: str, gpu_memory_utilization: float = 0.5
    ):
        if self.is_vllm and platform.system().lower() == "linux":
            from ChatTTS.model.velocity import LLM

            self.llm = LLM(
                model=gpt_folder,
                num_audio_tokens=self.num_audio_tokens,
                num_text_tokens=self.num_text_tokens,
                post_model_path=embed_file_path,
                gpu_memory_utilization=gpu_memory_utilization,
            )
            self.logger.info("vLLM model loaded")
            return

        self.gpt: LlamaModel = LlamaModel.from_pretrained(gpt_folder).to(
            self.device_gpt
        )
        del self.gpt.embed_tokens
