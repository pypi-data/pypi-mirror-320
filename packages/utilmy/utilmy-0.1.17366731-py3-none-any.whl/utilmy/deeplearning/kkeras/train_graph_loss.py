# -*- coding: utf-8 -*-
"""training_with_graph_loss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VN5yPkwDmK0rjBh5ZToCiPF7qmgIlqHx

# 0) Globals Params
"""

#!pip install python-box
#!pip install neural_structured_learning

"""
### STORED All variables inside special dict cc


"""
import os
import glob

from box import Box
cc = Box({})


### Output naming
cc.dname       = 'fashion_kaggle_full_40k'
cc.tag         = "test02_300epoch_"
tag = cc.tag


### Epoch
cc.epoch_start = 0
cc.epoch_end   = 300
num_epochs        = 300  # 80 overfits

cc.compute_mode   = ""  # "tpu"


### Input file
image_size     = 64   ### 64
xdim           = 64
ydim           = 64
cdim           = 3


#### Data Size
nmax           = 100000

#### Model Parameters
n_filters      = 12  #  12  # Base number of convolutional filters
latent_dim     = 200  # Number of latent dimensions


#### Training parmeter
batch_size     = 64
learning_rate  = 1e-3
schedule_type = 'poly'

cc.patience = 300
cc.kloop    = 200
# cc.kloop = 15
cc.class_dict = {'gender': 5, 'masterCategory': 7, 'subCategory': 45, 'articleType': 141, 'baseColour': 46}


#### Paths data  #####################
cc.root = "/content/"
cc.root2 = "/content/drive/MyDrive/v3/"
cc.root3 = ""
# os.makedirs(os.path.join(cc.root2, 'fashion_data'), exist_ok=True)

cc.path_img_all   = cc.root + "fashion_data/images/"
cc.path_img_train = cc.root + "fashion_data/images/"
cc.path_img_test  = cc.root + "fashion_data/images/"
cc.img_suffix = ".jpg"


# cc.path_label_raw   = cc.root + 'raw_fashion_data/styles.csv'
cc.path_label_raw   = cc.root2  + 'fashion_data/styles.csv'
cc.path_label_train = cc.root2 + 'fashion_data/styles_train.csv'
cc.path_label_test  = cc.root2 + 'fashion_data/styles_test.csv'
cc.path_label_val   = cc.root2 + 'fashion_data/styles_test.csv'

cc.model_dir     =  cc.root2 + 'saved_models/'


##### Output
model_dir  = cc.model_dir
model_dir2 = model_dir + f"/m_{tag}-{cc.dname}/"
cc.model_dir2 = model_dir2
# os.makedirs(model_dir2, exist_ok=True)

print('params', cc)

"""# 1) Connect the Google Drive"""

from google.colab import drive
drive.mount('/content/drive/')

"""# 1) Common"""

#!nvidia-smi

"""
Actually, Google Colab automatically disconnects the notebook if we leave it idle for more than 30 minutes. ðŸ•‘
Open your Chrome DevTools by pressing F12 or ctrl+shift+i on Linux and enter the following JavaScript snippet in your console:
function KeepClicking(){
console.log("Clicking");
document.querySelector("colab-connect-button").click()
}
setInterval(KeepClicking,60000)



"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install albumentations
# !pip install keras-adabound
# !pip install keras-rectified-adam
# !pip install tf-madgrad
# !pip install tf_sprinkles
# !pip install utilmy
# !pip install python-box

# Commented out IPython magic to ensure Python compatibility.
import os
import shutil
import glob
import sys
import time
import math
import string
import time
import json
from pathlib import Path
import random
import logging
import functools

import tensorflow as tf
from tensorflow.keras import layers, regularizers

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import cv2
import scipy
import pandas as pd
import scipy.stats
import h5py
import yaml
import seaborn as sns

from tensorflow import keras
from PIL import Image
from tqdm import tqdm
from scipy.stats import norm
from sklearn import manifold
from utilmy import pd_read_file
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas

from tf_sprinkles import Sprinkles
from madgrad import MadGrad

from sklearn.preprocessing import OneHotEncoder
from albumentations.core.transforms_interface import ImageOnlyTransform
from albumentations import (
    Compose, HorizontalFlip, CLAHE, HueSaturationValue,
    RandomBrightness, RandomContrast, RandomGamma,
    ToFloat, ShiftScaleRotate, Resize,
    GridDistortion, ElasticTransform, OpticalDistortion, Cutout
)
from scipy.spatial.distance import cdist

import numpy as np
import tensorflow as tf
import neural_structured_learning as nsl
from neural_structured_learning.keras import layers as nsl_layers
import neural_structured_learning.configs as nsl_configs


# %matplotlib inline

"""## 1) Define some helper functions"""

def log(*s):
    """Summary: Log decorator"""
    print(*s, flush=True)


def metric_accuracy(y_val, y_pred_head, class_dict):
    """Summary: Calculate validation accuracy
    Arguments: y_truth: numpy array
               y_predicted: numpy array
               class_dictionary: dict
               
    Return: Validation Accuracy"""
    # Val accuracy
    val_accuracies = {class_name: 0. for class_name in class_dict}
    for i, class_name in enumerate(class_dict):
        y_pred = np.argmax(y_pred_head[i], 1)
        y_true = np.argmax(y_val[i], 1)
        val_accuracies[class_name] = (y_pred == y_true).mean()
    print( f'\n {val_accuracies}')
    return val_accuracies
    
def metric_accuracy_2(y_test, y_pred, dd):
   """Summary: Calculate test accuracy
    Arguments: y_truth: numpy array
               y_redicted: numpy array
               class_dictionary: dict
               
    Return: Test Accuracy"""
   test_accuracy = {} 
   for k,(ytruei, ypredi) in enumerate(zip(y_test, y_pred)) : 
       ytruei = np.argmax(ytruei,         axis=-1)
       ypredi = np.argmax(ypredi.numpy(), axis=-1)
       # log(ytruei, ypredi ) 
       test_accuracy[ dd.labels_col[k] ] = accuracy_score(ytruei, ypredi )
        
   log('accuracy', test_accuracy)     
   return test_accuracy 
    


def cal_loss_macro_soft_f1(y, y_hat):
    """Compute the macro soft F1-score as a cost.
    Average (1 - soft-F1) across all labels.
    Use probability values instead of binary predictions.
    Args:
        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)
        y_hat (float32 Tensor): probability matrix of shape (BATCH_SIZE, N_LABELS)
    Returns:
        cost (scalar Tensor): value of the cost function for the batch
    """
    y     = tf.cast(y, tf.float32)
    y_hat = tf.cast(y_hat, tf.float32)
    tp    = tf.reduce_sum(y_hat * y, axis=0)
    fp    = tf.reduce_sum(y_hat * (1 - y), axis=0)
    fn    = tf.reduce_sum((1 - y_hat) * y, axis=0)
    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)
    cost    = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1
    macro_cost = tf.reduce_mean(cost) # average on all labels
    
    return macro_cost
    

def plot_original_images(test_sample):
    """Plot original images"""
    fig1 = plt.figure(figsize=(8, 8))

    for i in range(len(test_sample)):
        plt.subplot(4, 4, i + 1)
        plt.imshow(test_sample[i])
        plt.axis('off')

    print('Original Images:')
    plt.show()


def plot_reconstructed_images(model, test_sample):
    """Plot reconstructed images"""
    z_mean, z_logsigma, x_recon, _ = model.call(np.array(test_sample))

    fig1 = plt.figure(figsize=(8, 8))

    for i in range(x_recon.shape[0]):
        plt.subplot(4, 4, i + 1)
        plt.imshow(x_recon[i])
        plt.axis('off')

    print('\n Reconstructed Images (Perceptual):')
    plt.show()


def valid_image_check(img_list, path="", tag="", y_labels="", n_sample=3, renorm=True):    
    """Assess image validity"""
    os.makedirs(path, exist_ok=True)
    if n_sample is not None and isinstance(n_sample, int):
        img_list = img_list[:n_sample]
        y_labels = [y[:n_sample].tolist() for y in y_labels]

    for i in range(len(img_list)) :
        img = img_list[i]
        if not isinstance(img, np.ndarray) :
            img = img.numpy()
        
        if renorm:     
            img = (img * 0.5 + 0.5) * 255
        
        label_tag = 'label_{' + '-'.join([str(y[i]) for y in y_labels]) + '}'
        save_path = f"{path}/img_{cc.tag}_nimg_{i}_{tag}_{label_tag}_recon.png"
        img = img[:, :, ::-1]
        cv2.imwrite(save_path, img)
        img = None
        

def save_best(model, model_dir2, valid_loss, best_loss, counter):
    """Save the best model"""
    curr_loss = valid_loss
    if curr_loss < best_loss:
        save_model_state(model, model_dir2 + f'/best/')
        # dd = {"pars" : [ learning_rate, latent_dim, num_epochs ]}
        # json.dump(dd, open(model_dir2 +"/best/info.json" , mode='w'))
        print(f"Model Saved | Loss impoved from {best_loss} -----> {curr_loss}")
        best_loss = curr_loss
        counter   = 0
    else:
        counter += 1    
    return best_loss, counter    



def save_model_state(model, model_dir2):
    """Save the model"""
    os.makedirs(model_dir2 , exist_ok=True)
    model.save_weights( model_dir2 + f'/model_keras_weights.h5')


def train_stop(counter, patience):
    """Stop the training if meet the condition"""
    if counter == patience :
        log(f"Model not improved from {patience} epochs...............")
        log("Training Finished..................")
        return True
    return False

"""## 1-3) Define DFC-VAE model"""

class DFC_VAE(tf.keras.Model):
    """Deep Feature Consistent Variational Autoencoder Class"""
    def __init__(self, latent_dim, class_dict):
        """
        Initializes class instance"""
        super(DFC_VAE, self).__init__()
        self.latent_dim = latent_dim
        self.encoder = make_encoder()
        self.decoder = make_decoder()
        
        self.classifier = make_classifier(class_dict)

    def encode(self, x) -> tuple:
        """
        Encodes data to latent variables: Z-mean and Z-logsigma.
        Args:
            x: input data
        Returns:
            (Tuple): z-mean, z-logsigma
        """
        z_mean, z_logsigma = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
        return z_mean, z_logsigma
  
    def reparameterize(self, z_mean, z_logsigma):
        eps = tf.random.normal(shape=tf.shape(z_mean))
        return eps * tf.exp(z_logsigma * 0.5) + z_mean

    def decode(self, z, apply_sigmoid: bool=False):
        """
        Decodes variable.
        Args:
            z: latent variable;
            apply_sigmoid: flag of whether apply or not activation function;

        Returns:
            re-constracted variable.
        """
        x_recon = self.decoder(z)
        if apply_sigmoid:
            new_x_recon = tf.sigmoid(x_recon)
            return new_x_recon
        return x_recon

    def call(self, x,training=True, mask=None):
        # out_classes = None
        z_mean, z_logsigma = self.encode(x)
        z = self.reparameterize(z_mean, z_logsigma)
        x_recon = self.decode(z)
        
        #### Classifier
        out_classes = self.classifier(z)
        
        return z_mean, z_logsigma, x_recon, out_classes


def make_encoder(n_outputs=1):
    """Creates encoder model"""
    #Functionally define the different layer types
    Input = tf.keras.layers.InputLayer
    Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu',
                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001), 
                                activity_regularizer=regularizers.l2(1e-5))
    BatchNormalization = tf.keras.layers.BatchNormalization
    Flatten = tf.keras.layers.Flatten
    Dense = functools.partial(tf.keras.layers.Dense, activation='relu',
                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                bias_regularizer=regularizers.l2(1e-4), activity_regularizer=regularizers.l2(1e-5))

    ##### Build the encoder network using the Sequential API
    encoder = tf.keras.Sequential([
        Input(input_shape=(xdim, cc.ydim, 3)),

        Conv2D(filters=n_filters, kernel_size=5,  strides=2),
        BatchNormalization(),

        Conv2D(filters=2*n_filters, kernel_size=5,  strides=2),
        BatchNormalization(),

        Conv2D(filters=4*n_filters, kernel_size=3,  strides=2),
        BatchNormalization(),

        Conv2D(filters=6*n_filters, kernel_size=3,  strides=2),
        BatchNormalization(),

        Flatten(),
        Dense(512*1, activation='relu'),
        layers.Dropout(0.2),  
        Dense(512*1, activation='relu'),
        Dense(2*latent_dim, activation=None),
    ])

    return encoder


def make_decoder():
    """
    ValueError: Dimensions must be equal, but are 3 and 4 
    for '{{node sub}} = Sub[T=DT_FLOAT](x, sequential_1/conv2d_transpose_3/Relu)' with input shapes: [8,256,256,3], [8,256,256,4].

    """    
    #Functionally define the different layer types
    Input = tf.keras.layers.InputLayer

    # bias_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001)
    Dense = functools.partial(tf.keras.layers.Dense, activation='relu', 
                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                bias_regularizer=regularizers.l2(1e-4), activity_regularizer=regularizers.l2(1e-5))
    Reshape = tf.keras.layers.Reshape
    Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu',
                                        kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001), 
                                        activity_regularizer=regularizers.l2(1e-5))
    BatchNormalization = tf.keras.layers.BatchNormalization
    Flatten = tf.keras.layers.Flatten

    #Build the decoder network using the Sequential API
    if xdim == 64 :   #### 64 x 64 img         
        decoder = tf.keras.Sequential([
            Input(input_shape=(latent_dim,)),

            Dense(units= 4*4*6*n_filters),
            Dense(units= 4*4*6*n_filters),
            layers.Dropout(0.2),   
            Dense(units= 4*4*6*n_filters),    
            Reshape(target_shape=(4, 4, 6*n_filters)),
            #### ValueError: total size of new array must be unchanged, input_shape = [2304], output_shape = [7, 4, 144]

            #### Conv. layer      
            Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),
            Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),
            Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),

            Conv2DTranspose(filters=3, kernel_size=5,  strides=2),      
            # Conv2DTranspose(filters=4, kernel_size=5,  strides=2),

        ])

    if ydim == 256 :  ### 256 8 256 img          
        decoder = tf.keras.Sequential([
            Input(input_shape=(latent_dim,)),

            Dense(units=16*16*6*n_filters),
            Dense(units=16*16*6*n_filters),
            layers.Dropout(0.2),   
            Dense(units=16*16*6*n_filters),    
            Reshape(target_shape=(16, 16, 6*n_filters)),

            #### Conv. layer      
            Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),
            Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),
            Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),
            Conv2DTranspose(filters=3, kernel_size=5,  strides=2),      

        ])
    return decoder


def make_classifier(class_dict):
    """ Supervised multi class
            self.gender         = nn.Linear(self.inter_features, self.num_classes['gender'])
            self.masterCategory = nn.Linear(self.inter_features, self.num_classes['masterCategory'])
            self.subCategory    = nn.Linear(self.inter_features, self.num_classes['subCategory'])
            self.articleType    = nn.Linear(self.inter_features, self.num_classes['articleType'])

    """    
    Input = tf.keras.layers.InputLayer
    Dense = functools.partial(tf.keras.layers.Dense, activation='relu', 
                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                bias_regularizer=regularizers.l2(1e-4), 
                                activity_regularizer=regularizers.l2(1e-5))
    Reshape = tf.keras.layers.Reshape
    BatchNormalization = tf.keras.layers.BatchNormalization

    # if xdim == 64 :   #### 64 x 64 img          
    base_model = tf.keras.Sequential([
        Input(input_shape=(latent_dim,)),
        Dense(units=512),
        layers.Dropout(0.10),         
        Dense(units=512),
        layers.Dropout(0.10), 
        Dense(units=512),
    ])

    x = base_model.output
    ## x = layers.Flatten()(x) already flatten
    
    #### Multi-heads
    outputs = [Dense(num_classes, activation='softmax', name= f'{class_name}_out')(x) for class_name, num_classes in class_dict.items()]
    clf = tf.keras.Model(name='clf', inputs=base_model.input , outputs=outputs)  

    return clf

"""## 1-4) Build loss function"""

percep_model = tf.keras.applications.EfficientNetB0(
    include_top=False, weights='imagenet', input_tensor=None,
    input_shape=(xdim, ydim, cdim), pooling=None, classes=1000,
    classifier_activation='softmax'
)


from utilmy.deeplearning.keras.train_vqvae_loss import *;


"""# 3) Kaggle Fashion dataset

## Code for generating custom data from the Kaggle dataset
"""

# Download data
os.chdir(cc.dir_local)
os.system("gdown --id 1AExPSGhWOGPA79McvAOEFKXO_MCgkAVY -O raw_fashion_data.zip")
os.system("unzip -qq raw_fashion_data.zip")

"""### Old"""

import os
import glob



df = pd.read_csv('raw_fashion_data/styles.csv', error_bad_lines=False, warn_bad_lines=False)
df = df.dropna()
df_img_ids = set(df['id'].tolist())
img_ids = set([int(os.path.splitext(filename)[0]) for filename in os.listdir(os.path.join('raw_fashion_data/images'))])
available_img_ids = df_img_ids & img_ids
print('Total valid images:', len(available_img_ids))

df = df[df['id'].isin(available_img_ids)]
print(df.shape)

# Edit these categories as you need
value_map = {
    'gender': ['men', 'women'],
    'masterCategory': ['apparel', 'accessories'],
    'subCategory': ['topwear', 'shoes'],
    'articleType': ['tshirts', 'shirts'],
    'baseColour': ['black', 'white']
}
cols = ['id'] + list(value_map.keys())
df = df[cols]
for col, values in value_map.items():
    df[col] = df[col].apply(apply_func, args=(values,))

    print(df[col].unique())
shuffled = df.sample(frac=1)

n = df.shape[0]
num_train = int(0.65 * n)
num_val = n - num_train

df_train = df.iloc[:num_train, :]
df_val = df.iloc[num_train:, :]
for col in df_train.columns:
    print(df_train[col].unique())
for col in df_val.columns:
    print(df_val[col].unique())

#!mkdir -p fashion_data
#!cp -r raw_fashion_data/images fashion_data
df_train.to_csv('fashion_data/styles_train.csv', index=False)
df_val.to_csv('fashion_data/styles_val.csv', index=False)

"""### New"""

import os
import glob


df = pd.read_csv('raw_fashion_data/styles.csv', error_bad_lines=False, warn_bad_lines=False)
df = df.dropna()
df_img_ids = set(df['id'].tolist())
img_ids = set([int(os.path.splitext(filename)[0]) for filename in os.listdir(os.path.join('raw_fashion_data/images'))])
available_img_ids = df_img_ids & img_ids
print('Total valid images:', len(available_img_ids))

df = df[df['id'].isin(available_img_ids)]
print(df.shape)

# # Edit these categories as you need
# def apply_func(s, values):
#     if s.lower() in values:
#         return s.lower()
#     return 'other'


# value_map = {
#     'gender': ['men', 'women'],
#     'masterCategory': ['apparel', 'accessories'],
#     'subCategory': ['topwear', 'shoes'],
#     'articleType': ['tshirts', 'shirts'],
#     'baseColour': ['black', 'white']
# }
# cols = ['id'] + list(value_map.keys())
# df = df[cols]
# for col, values in value_map.items():
#     df[col] = df[col].apply(apply_func, args=(values,))

#     print(df[col].unique())
shuffled = df.sample(frac=1)

n = df.shape[0]
num_train = int(0.65 * n)
num_val = n - num_train

df_train = df.iloc[:num_train, :]
df_val = df.iloc[num_train:, :]
# for col in df_train.columns:
#     print(df_train[col].unique())
# for col in df_val.columns:
#     print(df_val[col].unique())

#!mkdir -p fashion_data
#!cp -r raw_fashion_data/images fashion_data
df_train.to_csv('fashion_data/styles_train.csv', index=False)
df_val.to_csv('fashion_data/styles_val.csv', index=False)

#!zip -r fashion_data.zip fashion_data

from google.colab import drive
drive.mount('/content/drive')

#!cp fashion_data.zip /content/drive/MyDrive/v3/

"""## Download the clean Kaggle dataset"""

# Download data
# !gdown --id 16i8Y7yQXkOdv-QwMcPusrKOd9ICyT07i -O fashion_data.zip
# !unzip -qq fashion_data.zip
# os.chdir('/content/')
# os.system('gdown --id 1Jf2XOJb078Mu75oUCJjBfxM36TGZ8SFv -O fashion_data.zip')
# os.system('unzip -qq fashion_data.zip')
#!cp {cc.root2}fashion_data.zip  /content/fashion_data.zip
#!unzip -o -qq  /content/fashion_data.zip   -d /content

#### https://www.kaggle.com/paramaggarwal/fashion-product-images-dataset

"""#### Visualization"""

df_train = pd.read_csv('fashion_data/styles_train.csv', error_bad_lines=False, warn_bad_lines=False)
df_val = pd.read_csv('fashion_data/styles_val.csv', error_bad_lines=False, warn_bad_lines=False)

sns.countplot(data=df_train, x='gender')

print('Total: ', df_train.masterCategory.nunique())

plt.figure(figsize=(10, 10))
sns.countplot(data=df_train, x='masterCategory')

# print(df_train.articleType.value_counts())
print('Total: ', df_train.articleType.nunique())

plt.figure(figsize=(20, 10))
sns.countplot(data=df_train, x='articleType', order=df_train.articleType.value_counts().iloc[:20].index)

# print(df_train.baseColour.value_counts())
# print(df_train.baseColour.nunique())
print('Total: ', df_train.baseColour.nunique())

#plt.figure(figsize=(20, 10))
# sns.countplot(data=df_train, x='baseColour', order=df_train.baseColour.value_counts().iloc[:20].index)

# print(df_train.subCategory.value_counts())
# print(df_train.subCategory.nunique())
print('Total: ', df_train.subCategory.nunique())

#plt.figure(figsize=(20, 10))
# sns.countplot(data=df_train, x='subCategory', order=df_train.subCategory.value_counts().iloc[:20].index)

"""## Data loader"""
from utilmy.deeplearning.keras.util_dataloader import  RealCustomDataGenerator9;


df = pd.read_csv(cc.path_label_train)
df = pd.concat((df,  pd.read_csv(cc.path_label_test )))
df = pd.concat((df,  pd.read_csv(cc.path_label_val )))
df = df.fillna('')
print( df.dtypes )

for ci in df.columns :
    if 'id' not in ci:
       df[ci] = df[ci].astype(str).str.lower()

       print(ci, df[ci].unique(), len(df[ci].unique()))


df.to_csv( cc.path_label_raw , index=False )

print(df.head(3).T)

df = pd.read_csv( cc.path_label_raw )

df = pd.read_csv(cc.path_label_raw, error_bad_lines=False, warn_bad_lines=False)
df = df.fillna('')
df = df.dropna()
df['id'] = df['id'].astype('int')
df = df.drop_duplicates('id')
df_img_ids  = set(df['id'].tolist())

image_list    = glob.glob(cc.path_img_all + "/*" + cc.img_suffix )
image_list    = [filename.split("/")[-1] for filename in image_list]
log('N images', len(image_list))

img_ids = set([int(filename.split(".")[0]) for filename in image_list])
available_img_ids = df_img_ids & img_ids
print('Total valid images:', len(available_img_ids))

df = df[df['id'].isin(available_img_ids)]
df = df.drop_duplicates('id')
shuffled  = df.sample(frac=1)

n         = df.shape[0]
num_train = int(0.9 * n)
num_val   = n - num_train

dftmp = df.drop_duplicates(list(cc.class_dict.keys()))
df_train  = pd.concat( (df.iloc[:num_train, :], dftmp))
df_val    = pd.concat( (df.iloc[num_train:, :], dftmp))
num_train = len(df_train)
num_val   = len(df_val)      


train_transforms = Compose([
    Resize(image_size, image_size, p=1),
    HorizontalFlip(p=0.5),
    RandomContrast(limit=0.2, p=0.5),
    RandomGamma(gamma_limit=(80, 120), p=0.5),
    RandomBrightness(limit=0.2, p=0.5),
    HueSaturationValue(hue_shift_limit=5, sat_shift_limit=20,
                       val_shift_limit=10, p=.9),
    ShiftScaleRotate(
        shift_limit=0.0625, scale_limit=0.1, 
        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), 
    ToFloat(max_value=255),
    SprinklesTransform(num_holes=10, side_length=10, p=0.5),
])

test_transforms = Compose([
    Resize(image_size, image_size, p=1),
    ToFloat(max_value=255)
])

# data_dir         = Path('./fashion_data')
image_dir        = cc.path_img_all # str(data_dir / 'images')
train_label_path = cc.path_label_train # str(data_dir / 'styles_train.csv')
val_label_path   = cc.path_label_val  # str(data_dir / 'styles_val.csv')

raw_train_data       = RealCustomDataGenerator(image_dir, train_label_path, cc.class_dict,
                                     split='train', batch_size=batch_size, transforms=train_transforms)
raw_val_data         = RealCustomDataGenerator(image_dir, val_label_path, cc.class_dict,
                                   split='val', batch_size=batch_size, transforms=test_transforms, shuffle=False)

# Build graph metadata
max_neighbors = 3
graph_reg_config = nsl.configs.make_graph_reg_config(
    max_neighbors          = max_neighbors,
    neighbor_prefix        = 'NL_nbr_',
    neighbor_weight_suffix = '_weight',
)

# First, build a tfrecord file in which each example format like as follow.
# {'id': ID, 'embedding': EMBDDING}
# Then build the graph from these embeddings and save into a .tsv file.
print('Building tfrecords file...')
train_tfrecord_path = './train_embeddings.tfrecords'
train_graph_path = 'train_graph.tsv'
build_tfrecord(raw_train_data, train_tfrecord_path, max_records=20)
graph_builder_config = nsl.configs.GraphBuilderConfig()
nsl.tools.build_graph_from_config(
    [train_tfrecord_path],
    train_graph_path,
    graph_builder_config,
)

# Read the graph from the .tsv file
train_graph_dict = nsl.tools.read_tsv_graph(train_graph_path)
lens = set()
for k, v in train_graph_dict.items():
    lens.add(len(v))
print('Unique lens:', lens)

# Actually create final dataset
print('Creating dataset...')

class GraphDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, data_iter, graph_dict):
        """ GraphDataGenerator:__init__
        Args:
            data_iter:     
            graph_dict:     
        Returns:
           
        """
        self.data_iter = data_iter
        self.graph_dict = graph_dict
    
    def __len__(self):
        """ GraphDataGenerator:__len__
        Args:
        Returns:
           
        """
        return len(self.data_iter)
    
    def __getitem__(self, idx):
        """ GraphDataGenerator:__getitem__
        Args:
            idx:     
        Returns:
           
        """
        index, batch_x, *batch_y = self.data_iter[idx]
        return self._map_func(index, batch_x, *batch_y)
    
    def _map_func(self, index, x_batch, *y_batch):
        """ GraphDataGenerator:_map_func
        Args:
            index:     
            x_batch:     
            *y_batch:     
        Returns:
           
        """
        features_dict = {'feature': x_batch}
        for idx in range(index*batch_size, (index+1)*batch_size):
            neighbor_dict = self.graph_dict[str(idx)]
            print(neighbor_dict)
            for i, (neighbor_key, neighbor_weight) in enumerate(neighbor_dict.items()):
                print(len(self.data_iter[int(neighbor_key)]))
                print(self.data_iter[int(neighbor_key)][1].shape)
                features_dict[f'NL_nbr_{i}_feature'] = self.data_iter[int(neighbor_key)][1]
                features_dict[f'NL_nbr_{i}_weight'] = float(neighbor_weight)
        return features_dict, y_batch


train_data = GraphDataGenerator(raw_train_data, train_graph_dict)

for x, y in train_data:
    print(x, y)

"""### Augmentation Performance Test"""

all_transforms = {
    'resize': Resize(image_size, image_size, p=1),
    'hozirontal_flip': HorizontalFlip(p=0.5),
    'random_contrast': RandomContrast(limit=0.2, p=0.5),
    'random_gamma': RandomGamma(gamma_limit=(80, 120), p=0.5),
    'random_brightness': RandomBrightness(limit=0.2, p=0.5),
    'hue_saturation': HueSaturationValue(hue_shift_limit=5, sat_shift_limit=20,
                       val_shift_limit=10, p=.9),
    'shift_scale_rotate': ShiftScaleRotate(
        shift_limit=0.0625, scale_limit=0.1, 
        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), 
    'tofloat': ToFloat(max_value=255),
    'grid_distortion': GridDistortion(num_steps=5),
    'elastic_transform': ElasticTransform(sigma=10, alpha_affine=10),
    'optical_distortion': OpticalDistortion(distort_limit=0.1, shift_limit=0.1),
    # 'sprinkles_transform': SprinklesTransform(num_holes=10, side_length=10, p=0.5),
    'cutout': Cutout(num_holes=15),
}

def plot_grid(images, title=''):
    """function plot_grid
    Args:
        images:   
        title:   
    Returns:
        
    """
    num_samples = len(images)
    n = int(np.sqrt(num_samples))
    _, axes = plt.subplots(n, n, figsize=(10, 10))
    for i in range(n):
        for j in range(n):
            axes[i, j].imshow(images[i*n+j])
            axes[i, j].axis('off')
    plt.suptitle(title)
    plt.show()
    plt.close()


for name, transform in all_transforms.items():
    if name not in ('resize', 'tofloat'):
        test_perf_transforms = Compose([
            all_transforms['resize'],
            transform,
        ])
        test_perf_data = RealCustomDataGenerator(cc.path_img_all, val_label_path, cc.class_dict,
                                                    split='val', batch_size=batch_size, transforms=test_perf_transforms, shuffle=False)
        images, *labels = test_perf_data[0]
        plot_grid(images, title=name)

t0 = time.time()

transform_times = {}
for name, transform in all_transforms.items():
    if name not in ('resize', 'tofloat'):
        test_perf_transforms = Compose([
            all_transforms['resize'],
            transform,
        ])

        test_perf_data = RealCustomDataGenerator(cc.path_img_all, val_label_path, cc.class_dict,
                                                 split='val', batch_size=batch_size, transforms=test_perf_transforms, shuffle=False)
        # Run over 10 batches and get average running time
        times = []
        for i in range(10):
            t0 = time.time()
            test_perf_data[i]
            times.append(time.time() - t0)
        
        avg_time = np.mean(times)
        transform_times[name] = avg_time
        print('Transform type: {} - Average time: {} on batch size: {}'.format(name, avg_time, batch_size))

time_df = pd.DataFrame(data=list(transform_times.items()), columns=['Transform', 'Time (s)'])
time_df.plot(kind='barh', x='Transform', figsize=(15, 10))

"""## Train

### Learning rate scheduler
"""

# https://www.pyimagesearch.com/2019/07/22/keras-learning-rate-schedules-and-decay/

class LearningRateDecay:
    """## Summary: Learning rate decay plotting"""
    def plot(self, epochs, title="Learning Rate Schedule"):
        """ LearningRateDecay:plot
        Args:
            epochs:     
            title:     
        Returns:
           
        """
        # compute the set of learning rates for each corresponding
        # epoch
        lrs = [self(i) for i in epochs]
        # the learning rate schedule
        plt.style.use("ggplot")
        plt.figure()
        plt.plot(epochs, lrs)
        plt.title(title)
        plt.xlabel("Epoch #")
        plt.ylabel("Learning Rate")
    
class StepDecay(LearningRateDecay):
    """ Summary: Step-based learning rate decay """
    def __init__(self, init_lr=0.01, factor=0.25, drop_every=10):
        """ StepDecay:__init__
        Args:
            init_lr:     
            factor:     
            drop_every:     
        Returns:
           
        """
		# store the base initial learning rate, drop factor, and
		# epochs to drop every
        self.init_lr = init_lr
        self.factor = factor
        self.drop_every = drop_every

    def __call__(self, epoch):
        """ StepDecay:__call__
        Args:
            epoch:     
        Returns:
           
        """
        """ PolynomialDecay:__call__
        Args:
            epoch:     
        Returns:
           
        """
        # compute the learning rate for the current epoch
        exp = np.floor((1 + epoch) / self.drop_every)
        alpha = self.init_lr * (self.factor ** exp)
        # return the learning rate
        return float(alpha)


class PolynomialDecay(LearningRateDecay):
    """ Summary: Polynomial-based learning rate decay """
    def __init__(self, max_epochs=100, init_lr=0.01, power=1.0):
        """ PolynomialDecay:__init__
        Args:
            max_epochs:     
            init_lr:     
            power:     
        Returns:
           
        """
        # store the maximum number of epochs, base learning rate,
        # and power of the polynomial
        self.max_epochs = max_epochs
        self.init_lr = init_lr
        self.power = power

    def __call__(self, epoch):
        # compute the new learning rate based on polynomial decay
        decay = (1 - (epoch / float(self.max_epochs))) ** self.power
        alpha = self.init_lr * decay
        # return the new learning rate
        return float(alpha)

def visualize_imgs(img_list, path, tag, y_labels, n_sample=None):
    """Assess image validity"""
    os.makedirs(path, exist_ok=True)
    if n_sample is not None and isinstance(n_sample, int):
        img_list = img_list[:n_sample]
        y_labels = [y[:n_sample].tolist() for y in y_labels]

    for i in range(len(img_list)) :
        img = img_list[i]
        if not isinstance(img, np.ndarray) :
            img = img.numpy()
        
        img = img[:, :, ::-1]
        img = np.clip(img * 255, 0, 255).astype('uint8')
        label_tag = 'label_{' + '-'.join([str(y[i]) for y in y_labels]) + '}'
        save_path = f"{path}/img_{cc.tag}_nimg_{i}_{tag}_{label_tag}.png"
        cv2.imwrite(save_path, img)
        img = None


@tf.function
def train_step(x, model, y_label_list=None):
    """function train_step
    Args:
        x:   
        model:   
        y_label_list:   
    Returns:
        
    """
    """function train_step
    Args:
        x:   
        model:   
        y_label_list:   
    Returns:
        
    """
    with tf.GradientTape() as tape_w:

        # A separate GradientTape is needed for watching the input.
        with tf.GradientTape() as tape_x:
            tape_x.watch(x)

            # Regular forward pass.
            sample_features, nbr_features, nbr_weights = nbr_features_layer.call(x)
            base_output = model(sample_features, training=True)
            labeled_loss = loss_fn(y, base_output)

        has_nbr_inputs = nbr_weights is not None and nbr_features
        if (has_nbr_inputs and graph_reg_config.multiplier > 0):
            # Use logits for regularization.
            sample_logits = base_output
            nbr_logits = model(nbr_features, training=True)
            graph_loss = regularizer(
                sources=sample_logits, targets=nbr_logits, weights=nbr_weights)
        else:
            graph_loss = tf.constant(0, dtype=tf.float32)

        scaled_graph_loss = graph_reg_config.multiplier * graph_loss

        # Combines both losses. This could also be a weighted combination.
        total_loss = labeled_loss + scaled_graph_loss

    # Regular backward pass.
    gradients = tape_w.gradient(total_loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return base_output, total_loss, labeled_loss, scaled_graph_loss

    with tf.GradientTape() as tape:
        z_mean, z_logsigma, x_recon, out_classes = model(x, training=True)      #Forward pass through the VAE
        loss = perceptual_loss_function(x, x_recon, z_mean, z_logsigma,
            y_label_heads=y_label_list, 
            y_pred_heads=out_classes, 
            clf_loss_fn=clf_loss_crossentropy
        )  

    grads = tape.gradient(loss, model.trainable_variables)   #Calculate gradients
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss


@tf.function
def validation_step(x, model):
    """function validation_step
    Args:
        x:   
        model:   
    Returns:
        
    """
    """function validation_step
    Args:
        x:   
        model:   
    Returns:
        
    """
    """function validation_step
    Args:
        x:   
        model:   
    Returns:
        
    """
    z_mean, z_logsigma, x_recon, out_classes = model(x, training=False)  #Forward pass through the VAE
    loss = perceptual_loss_function(x, x_recon, z_mean, z_logsigma)
    return loss, x_recon, out_classes


##### Output
model_dir  = os.path.join(cc.dir_drive, 'saved_models')
model_dir2 = model_dir + f"/m_{cc.tag}-{cc.dname}/"
os.makedirs(model_dir2, exist_ok=True)
check_dir = os.path.join(model_dir2, 'check')

## Do not delete
##if os.path.isdir(check_dir):
###    shutil.rmtree(check_dir)  # Delete previous dir


#### Instantiate a new DFC_CVAE model and optimizer
log('Current directory:', os.getcwd())
log("\nBuild the DFC-VAE Model")
model = DFC_VAE(cc.latent_dim, class_dict)
optimizer = tf.keras.optimizers.Adam(cc.learning_rate)


### Training loop
kbatch = len(train_data)
train_loss_hist = []
valid_loss_hist = []
counter = 0
dostop = False
best_loss = 100000000.0

# Setup learning rate scheduler
if cc.schedule_type == 'step':
	print("Using 'step-based' learning rate decay")
	schedule = StepDecay(init_lr=cc.learning_rate, factor=0.25, drop_every=15)
elif cc.schedule_type == "linear":
	print("Using 'linear' learning rate decay")
	schedule = PolynomialDecay(max_epochs=cc.num_epochs, init_lr=cc.learning_rate, power=1)
elif cc.schedule_type == "poly":
	print("Using 'polynomial' learning rate decay")
	schedule = PolynomialDecay(max_epochs=cc.num_epochs, init_lr=cc.learning_rate, power=5)

# ==============================================================================
# ### Validation dataset
# # This might take long time
# for batch_idx, (x_val, *_) in enumerate(val_data):
#     x_val = np.clip(x_val * 255, 0, 255).astype('uint8')
#     visualize_imgs(x_val, path=model_dir2 + "/check/",
#                     batch_idx=batch_idx, n_sample=3)
# ==============================================================================



# Add layers
nbr_features_layer = nsl_layers.NeighborFeatures(graph_reg_config.neighbor_config)
regularizer = nsl_layers.PairwiseDistance(graph_reg_config.distance_config, name='graph_loss')

for epoch in range(cc.num_epochs):
    log(f"Starting epoch {epoch+1}/{cc.num_epochs}, in {kbatch} kbatches ")
    if dostop: break   

    # Set learning rate
    lr = schedule(epoch)
    optimizer.learning_rate = lr
    print('[Epoch {:03d}] Learning rate: {}'.format(epoch, optimizer.learning_rate.numpy()))
    for batch_idx, (x, *y_label_list) in enumerate(train_data):        
        if dostop:
            break

        # log("x", x)
        # log("y_label_list", y_label_list)
        train_loss = train_step(x, model, y_label_list=y_label_list)
        train_loss = np.mean(train_loss.numpy())
        train_loss_hist.append(train_loss)
        if (batch_idx + 1) % (kbatch // 10) == 0:
            log('[Epoch {:03d} batch {:04d}/{:04d}]'.format(epoch + 1, batch_idx+1, kbatch))

        # if (batch_idx + cc['kloop']) % cc['kloop']  == 0 :
        if (batch_idx + 1) % cc['kloop'] == 0:
            for (x_val, *y_val) in val_data:
                valid_loss, x_recon, y_pred = validation_step(x_val, model)    
                valid_loss = np.mean(valid_loss.numpy())

                # test_accuracy = metric_accuracy(y_test, y_pred, dd)

            valid_loss_hist.append(valid_loss)
        
            log(epoch+1, batch_idx, 'train,valid', train_loss, valid_loss)        
            best_loss, counter = save_best(model, model_dir2, valid_loss, best_loss, counter)
            dostop = train_stop(counter, cc['patience'])

        # if (batch_idx + 500) % 500 == 0:
        if (batch_idx + 1) % 200 == 0:
            for (x_val, *y_val) in val_data:
                _, x_recon, y_pred_head = validation_step(x_val, model)
                break

            y_pred     = [np.argmax(y, 1) for y in y_pred_head]
            valid_image_check(x_recon, path=model_dir2 + "/check/",
                              tag=f"e{epoch+1}_b{batch_idx+1}", y_labels=y_pred, n_sample=20, renorm=True)

            if epoch == 0 and batch_idx < 300 : 
              y_val_true = [np.argmax(y, 1) for y in y_val]
              visualize_imgs(x_val, path=model_dir2 + "/check/",
                            tag=f'e{epoch+1}_b{batch_idx+1}', y_labels=y_val_true, n_sample=20)

            # Val accuracy
            val_accuracies = metric_accuracy(y_val, y_pred_head, class_dict)
            # print('\n %s\n' % val_accuracies)


log('Final valid_loss', str(valid_loss_hist)[:200])

"""### Plot learning rate"""

schedule.plot(np.arange(cc.num_epochs))

"""## Save the model"""

log("\nSaving Model")
os.makedirs(model_dir, exist_ok=True)
tf.saved_model.save(model, model_dir2)
model.save_weights( model_dir2 + f'/model_keras_weights.h5')
dd = {"pars" : [ cc.learning_rate, cc.latent_dim, num_epochs ]}
json.dump(dd, open(model_dir2 +"/info.json" , mode='w'))
log(model_dir2)

"""## Reload the model"""

log('\nReload Model')
model2 = DFC_VAE(cc.latent_dim, class_dict)
input_shape = (cc.batch_size, cc.xdim, cc.ydim, cc.cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
model2.build(input_shape)
model2.load_weights( model_dir2 + f'/model_keras_weights.h5')
log('# Reloaded', model2)
# log('rerun eval', validation_step(x_val, model2))

"""## Plot"""

"""Plot Original Input Images vs. Reconstructed Images"""
logger = logging.getLogger()
old_level = logger.level
logger.setLevel(100)

# test_sample = random.sample(list(test_images), 1)
for test_images, *_ in val_data:
    break
plot_original_images([test_images[0]])
_, _, x_recon, _ = model2(tf.expand_dims(test_images[0], 0), training=False)
plot_reconstructed_images(model, x_recon)


logger.setLevel(old_level)
log('Plot Loss Over Time During the Training Process')
plt.figure()
plt.plot(train_loss_hist)
plt.plot(valid_loss_hist)
plt.title('Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()


log('Plot Tail-End of Logarithmic of Loss Over Time During the Training Process')
plt.figure()
plt.plot(np.log(train_loss_hist[5000:]))
plt.plot(np.log(valid_loss_hist[5000:]))
plt.title('Logarithmic Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()

"""## Classification Evaluation"""

val_accuracies = {class_name: 0. for class_name in class_dict}
for (x_val, *y_val) in val_data:
    _, _, _, predictions = model(x_val, training=False)

    for i, class_name in enumerate(class_dict):
        y_pred = np.argmax(predictions[i], 1)
        y_true = np.argmax(y_val[i], 1)
        val_accuracies[class_name] = (y_pred == y_true).mean()
print(val_accuracies)

"""# 4) Fashion MNIST TF"""

data_type = 'fashion_tf'
# if data_type == 'random' :
#     num_train = 100
#     num_val = 50
#     x_train = np.random.randint(0, 256, size=(num_train, image_size, image_size, 3), dtype='uint8')
#     y_train = [np.random.randint(0, 2,  size=(num_train, n_labels)) for _, n_labels in class_dict.items()]

#     x_val = np.random.randint(0, 256, size=(num_val, image_size, image_size, 3), dtype='uint8')
#     y_val = [np.random.randint(0, 2,  size=(num_val, n_labels)) for _, n_labels in class_dict.items()]

#     train_data = CustomDataGenerator0(x_train, y_train, augmentations=train_transforms)
#     val_data = CustomDataGenerator0(x_val, y_val, augmentations=test_transforms)
# elif data_type == 'fashion_tf':

# ==============================================================================
(x_train, train_labels), (x_val, val_labels) = tf.keras.datasets.fashion_mnist.load_data()

# Convert to RGB
x_train = np.stack([x_train, x_train, x_train], axis=-1)
x_val = np.stack([x_val, x_val, x_val], axis=-1)
image_size = 28
num_train = train_labels.shape[0]
num_val = val_labels.shape[0]
y_train = np.zeros((len(class_dict), num_train, 1), dtype='int32')
y_val = np.zeros((len(class_dict), num_val, 1), dtype='int32')

y_train[train_labels, :, :] = 1
y_val[val_labels, :, :] = 1
train_data = CustomDataGenerator0(x_train, y_train, augmentations=train_transforms)
val_data = CustomDataGenerator0(x_val, y_val, augmentations=test_transforms)
# ==============================================================================

# elif data_type == 'fashion':
#     data_dir = Path('./data')
#     image_dir = str(data_dir / 'images')
#     train_label_path = str(data_dir / 'labels' / 'train.txt')
#     val_label_path = str(data_dir / 'labels' / 'val.txt')
#     train_data = CustomDataGenerator(image_dir, train_label_path, class_dict, split='train', transforms=train_transforms)
#     val_data = CustomDataGenerator(image_dir, val_label_path, class_dict, split='val', transforms=test_transforms)


log("Train Model")
log('Number of training batches:', len(train_data))
log('Number of test batches:', len(val_data))

"""Sanity check"""

for images, *labels in train_data:
    break

plt.imshow(images[0])
plt.show()

@tf.function
def train_step_2(x, model, y_label_list=None):
    """function train_step_2
    Args:
        x:   
        model:   
        y_label_list:   
    Returns:
        
    """
    with tf.GradientTape() as tape:
        z_mean, z_logsigma, x_recon, out_classes = model(x, training=True)      #Forward pass through the VAE
        loss = perceptual_loss_function(x, x_recon, z_mean, z_logsigma,
            y_label_heads=y_label_list, 
            y_pred_heads=out_classes, 
            clf_loss_fn=clf_loss_crossentropy
        )  

    grads = tape.gradient(loss, model.trainable_variables)   #Calculate gradients
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss


@tf.function
def validation_step(x, model):
    z_mean, z_logsigma, x_recon, out_classes = model(x, training=False)  #Forward pass through the VAE
    loss = perceptual_loss_function(x, x_recon, z_mean, z_logsigma)
    return loss, x_recon, out_classes


##### Output
cc.dname = 'model_fashion_tf'
model_dir = './saved_models'
model_dir2 = model_dir + f"/m_{cc.tag}-{cc.dname}/"
os.makedirs(model_dir2, exist_ok=True)


#### Instantiate a new DFC_CVAE model and optimizer
log("\nBuild the DFC-VAE Model")
model = DFC_VAE(latent_dim, class_dict)
optimizer = tf.keras.optimizers.Adam(learning_rate)


### Training loop
kbatch = len(train_data)
train_loss_hist = []
valid_loss_hist = []
counter = 0 
dostop = False
best_loss = 100000000.0

### Validation dataset
# x_val = test_images
# valid_image_check(x_val, path= model_dir2 + "/check/", 
#                   tag= f"0ref", n_sample=10, renorm=True)


for epoch in range(num_epochs):
    log(f"Starting epoch {epoch+1}/{num_epochs}, in {kbatch} kbatches ")
    if dostop: break   

    for batch_idx, (x, *y_label_list) in enumerate(train_data):        
        if dostop:
            break

        # log("x", x)
        # log("y_label_list", y_label_list)
        log('[Epoch {:03d} batch {:04d}/{:04d}]'.format(epoch + 1, batch_idx+1, kbatch))

        train_loss = train_step(x, model, y_label_list=y_label_list)
        train_loss = np.mean(train_loss.numpy())
        train_loss_hist.append(train_loss)

        if (batch_idx + cc['kloop'] ) % cc['kloop']  == 0 :
            for (x_val, *_) in val_data:
                valid_loss, x_recon, _ = validation_step(x_val, model)    
                valid_loss = np.mean(valid_loss.numpy())
            valid_loss_hist.append(valid_loss)
        
            log(epoch+1, batch_idx, 'train,valid', train_loss, valid_loss)        
            best_loss, counter = save_best(model, model_dir2, valid_loss, best_loss, counter)
            dostop = train_stop(counter, cc['patience']) 

        if (batch_idx + 500) % 500 == 0 :
            for (x_val, *_) in val_data:
                _, x_recon, _ = validation_step(x_val, model)
                break
            valid_image_check(x_recon, path=model_dir2 + "/check/",
                              tag=f"e{epoch+1}_b{batch_idx+1}", n_sample=10, renorm=True)


log('Final valid_loss', str(valid_loss_hist)[:200])

"""Save the model"""

log("\nSaving Model")
os.makedirs(model_dir, exist_ok=True)
tf.saved_model.save(model, model_dir2)
model.save_weights(model_dir2 + f'/model_keras_weights.h5')
dd = {"pars" : [ learning_rate, latent_dim, num_epochs ]}
json.dump(dd, open(model_dir2 +"/info.json" , mode='w'))
log(model_dir2)

"""Reload the model"""

log('\nReload Model')
model2 = DFC_VAE(latent_dim, class_dict)
input_shape = (batch_size, xdim, ydim, cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
model2.build(input_shape)
model2.load_weights( model_dir2 + f'/model_keras_weights.h5')
log('# Reloaded', model2)
log('rerun eval', validation_step(x_val, model2))

"""Plot"""

"""Plot Original Input Images vs. Reconstructed Images"""
logger = logging.getLogger()
old_level = logger.level
logger.setLevel(100)

# test_sample = random.sample(list(test_images), 1)
for test_images, *_ in val_data:
    break
plot_original_images([test_images[0]])
_, _, x_recon, _ = model2(tf.expand_dims(test_images[0], 0), training=False)
plot_reconstructed_images(model, x_recon)


logger.setLevel(old_level)
log('Plot Loss Over Time During the Training Process')
plt.figure()
plt.plot(train_loss_hist)
plt.plot(valid_loss_hist)
plt.title('Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()


log('Plot Tail-End of Logarithmic of Loss Over Time During the Training Process')
plt.figure()
plt.plot(np.log(train_loss_hist[5000:]))
plt.plot(np.log(valid_loss_hist[5000:]))
plt.title('Logarithmic Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()

# def get_data_sample(batch_size, train_images, labels_val):
#     """Data sampler"""
#     i_select = np.random.choice(np.arange(len(labels_val['gender'])), size=batch_size, replace=False)
    
#     #### Images
#     x = np.array([train_images[i]  for i in i_select ])

#     #### y_onehot Labels  [y1, y2, y3, y4]
#     labels_col = ['gender', 'masterCategory', 'subCategory', 'articleType']
#     y_label_list = []
#     for ci in labels_col :
#         v =  labels_val[ci][i_select]
#         y_label_list.append(v)

#     return x, y_label_list

"""# 5) Random data"""

num_train = 100
num_val = 50
x_train = np.random.randint(0, 256, size=(num_train, image_size, image_size, 3), dtype='uint8')
y_train = [np.random.randint(0, 2,  size=(num_train, n_labels)) for _, n_labels in class_dict.items()]

x_val = np.random.randint(0, 256, size=(num_val, image_size, image_size, 3), dtype='uint8')
y_val = [np.random.randint(0, 2,  size=(num_val, n_labels)) for _, n_labels in class_dict.items()]

train_data = CustomDataGenerator0(x_train, y_train, augmentations=train_transforms)
val_data = CustomDataGenerator0(x_val, y_val, augmentations=test_transforms)

log("Train Model")
log('Number of training batches:', len(train_data))
log('Number of test batches:', len(val_data))

@tf.function
def train_step(x, model, y_label_list=None):
    with tf.GradientTape() as tape:
        z_mean, z_logsigma, x_recon, out_classes = model(x, training=True)      #Forward pass through the VAE
        loss = perceptual_loss_function(x, x_recon, z_mean, z_logsigma,
            y_label_heads=y_label_list, 
            y_pred_heads=out_classes, 
            clf_loss_fn=clf_loss_crossentropy
        )  

    grads = tape.gradient(loss, model.trainable_variables)   #Calculate gradients
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss


@tf.function
def validation_step(x, model):
    z_mean, z_logsigma, x_recon, out_classes = model(x, training=False)  #Forward pass through the VAE
    loss = perceptual_loss_function(x, x_recon, z_mean, z_logsigma)
    return loss, x_recon, out_classes


##### Output
cc.dname = 'model_random'
model_dir = './saved_models'
model_dir2 = model_dir + f"/m_{cc.tag}-{cc.dname}/"
os.makedirs(model_dir2, exist_ok=True)


#### Instantiate a new DFC_CVAE model and optimizer
log("\nBuild the DFC-VAE Model")
model = DFC_VAE(latent_dim, class_dict)
optimizer = tf.keras.optimizers.Adam(learning_rate)


### Training loop
kbatch = len(train_data)
train_loss_hist = []
valid_loss_hist = []
counter = 0 
dostop = False
best_loss = 100000000.0

### Validation dataset
# x_val = test_images
# valid_image_check(x_val, path= model_dir2 + "/check/", 
#                   tag= f"0ref", n_sample=10, renorm=True)


for epoch in range(num_epochs):
    log(f"Starting epoch {epoch+1}/{num_epochs}, in {kbatch} kbatches ")
    if dostop: break   

    for batch_idx, (x, *y_label_list) in enumerate(train_data):        
        if dostop:
            break

        # log("x", x)
        # log("y_label_list", y_label_list)
        log('[Epoch {:03d} batch {:04d}/{:04d}]'.format(epoch + 1, batch_idx+1, kbatch))

        train_loss = train_step(x, model, y_label_list=y_label_list)
        train_loss = np.mean(train_loss.numpy())
        train_loss_hist.append(train_loss)

        if (batch_idx + cc['kloop'] ) % cc['kloop']  == 0 :
            for (x_val, *_) in val_data:
                valid_loss, x_recon, _ = validation_step(x_val, model)    
                valid_loss = np.mean(valid_loss.numpy())
            valid_loss_hist.append(valid_loss)
        
            log(epoch+1, batch_idx, 'train,valid', train_loss, valid_loss)        
            best_loss, counter = save_best(model, model_dir2, valid_loss, best_loss, counter)
            dostop = train_stop(counter, cc['patience']) 

        if (batch_idx + 500) % 500 == 0 :
            for (x_val, *_) in val_data:
                _, x_recon, _ = validation_step(x_val, model)
                break
            valid_image_check(x_recon, path=model_dir2 + "/check/",
                              tag=f"e{epoch+1}_b{batch_idx+1}", n_sample=10, renorm=True)


log('Final valid_loss', str(valid_loss_hist)[:200])

"""Save the model"""

log("\nSaving Model")
os.makedirs(model_dir, exist_ok=True)
tf.saved_model.save(model, model_dir2)
model.save_weights( model_dir2 + f'/model_keras_weights.h5')
dd = {"pars" : [ learning_rate, latent_dim, num_epochs ]}
json.dump(dd, open(model_dir2 +"/info.json" , mode='w'))
log(model_dir2)

"""Reload the model"""

log('\nReload Model')
model2 = DFC_VAE(latent_dim, class_dict)
input_shape = (batch_size, xdim, ydim, cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
model2.build(input_shape)
model2.load_weights( model_dir2 + f'/model_keras_weights.h5')
log('# Reloaded', model2)
log('rerun eval', validation_step(x_val, model2))

"""Plot"""

"""Plot Original Input Images vs. Reconstructed Images"""
logger = logging.getLogger()
old_level = logger.level
logger.setLevel(100)

# test_sample = random.sample(list(test_images), 1)
for test_images, *_ in val_data:
    break
plot_original_images([test_images[0]])
_, _, x_recon, _ = model2(tf.expand_dims(test_images[0], 0), training=False)
plot_reconstructed_images(model, x_recon)


logger.setLevel(old_level)
log('Plot Loss Over Time During the Training Process')
plt.figure()
plt.plot(train_loss_hist)
plt.plot(valid_loss_hist)
plt.title('Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()


log('Plot Tail-End of Logarithmic of Loss Over Time During the Training Process')
plt.figure()
plt.plot(np.log(train_loss_hist[5000:]))
plt.plot(np.log(valid_loss_hist[5000:]))
plt.title('Logarithmic Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()











# -*- coding: utf-8 -*-
"""


from utilmy import pd_read_file
import tensorflow as tf, pandas as pd 
from sklearn.metrics import accuracy_score
print(tf, tf.keras)

from importall import *
from utils import *
from importall import xdim, ydim, cdim
from utils import log, valid_image_check
from util_train import *
from util_train import save_model_state, save_best, metric_accuracy

###########################################################################################
from box import Box
cc = Box({})

### Data Size
cc.nmax        = 100000
cc.max_batch   = 300
cc.data_type   = 'npz'



### model 
cc.n_filters  = 12    #  12  # Base number of convolutional filters
cc.latent_dim = 256   # Number of latent dimensions


### Train
cc.opt_name       = ''   # 'madgrad'  ### Optimizer 
cc.data_gen_name  = 'album1'  ###  'album1'  ### Data Generator Name 

cc.batch_size     = 32
cc.learning_rate  = 1e-3
cc.num_epochs     = 100  # 80 overfits
cc.patience       = 200
cc.kloop          = 200


### Output file
cc.tag        = "test17"



############################################################################
log("\n\n##### Load Data   ###############################################")
cc.dname = "train_test-alllabel_nobg-256_256-100000"




if cc.data_type == 'npz' :
    log("\n\n######## Image dataset   ####################################################")
    data    = np.load( data_train + f"/{cc.dname}.npz")

        
    log("##### Xtrain ")    
    x_train = data['train']
    # x_test  = data['test']
    i_test = np.random.randint(0, len(x_train), 1000)  
    
    x_test = x_train[i_test]
    
    log( f"{cc.dname}")
    log( 'dataset shape', x_train.shape, )

    log( 'img shape', x_test[0].shape )
    (xdim, ydim, cdim) = x_test[0].shape
 

    log("#### Labels into OneHot #########################################################")    
    dfref      = pd_read_file( data_train + "/preprocessed_df.csv" )
    labels_col = [  'gender', 'masterCategory', 'subCategory', 'articleType' ]
    

    df       = pd.DataFrame(data['train_label'], columns=['uri'])    
    # log5(df)
    df['id'] = df['uri'].apply(lambda x : int(x.split("/")[-1].split(".")[0])    ) 
    labels_val, cc.labels_cnt  = data_to_y_onehot_list(df, dfref, labels_col)     
    y_train  = [ labels_val[ci][:, :]  for ci in labels_col  ]  ### list of oneHot (kbatch, nlabels)
    log5(y_train)
    
    
    # df       = pd.DataFrame(data['test_label'], columns=['uri'])
    df = pd.DataFrame(data['train_label'], columns=['uri'])
    df = df.iloc[i_test, :]
    
    df['id']       = df['uri'].apply(lambda x : int(x.split("/")[-1].split(".")[0])    )    
    labels_val, _  = data_to_y_onehot_list(df, dfref, labels_col)         
    y_test         = [ labels_val[ci][:, :]  for ci in labels_col  ]  #### list of oneHot (kbatch, nlabels)
    log5(y_test)    
    
    del data; gc.collect()
    

elif cc.data_type == 'parquet' :
    log("\n\n######## Image dataset   ####################################################")
    data    = np.load( data_train + f"/{cc.dname}.npz")

    
    i_test = np.random.randint(0, len(x_train), 1000)
    
    
    log("#### Labels into OneHot #########################################################")    
    dfref      = pd_read_file( data_train + "/preprocessed_df.csv" )
    # dfref      = data['df_master']
    labels_col = [  'gender', 'masterCategory', 'subCategory', 'articleType' ]
    
    
    id_filter = dfref[dfref['masterCategory'] == 0]['id'].values
    #id_filter = dfref[dfref['masterCategory'] > -1 ]['id'].values
    

    df       = pd.DataFrame(data['train_label'], columns=['uri'])    
    # log5(df)
    df['id'] = df['uri'].apply(lambda x : int(x.split("/")[-1].split(".")[0])    ) 
    df       = df[df['id'].isin(id_filter) ]
    
    # i_train =
    
    labels_val, cc.labels_cnt  = data_to_y_onehot_list(df, dfref, labels_col)     
    y_train  = [ labels_val[ci][:, :]  for ci in labels_col  ]  #### list of oneHot (kbatch, nlabels)
    log5(y_train)
    
    
    # df       = pd.DataFrame(data['test_label'], columns=['uri'])
    df = pd.DataFrame(data['train_label'], columns=['uri'])
    df = df.iloc[i_test, :]
    
    df['id']       = df['uri'].apply(lambda x : int(x.split("/")[-1].split(".")[0])    )    
    labels_val, _  = data_to_y_onehot_list(df, dfref, labels_col)         
    y_test         = [ labels_val[ci][:, :]  for ci in labels_col  ]  #### list of oneHot (kbatch, nlabels)
    log5(y_test)    
    
    
    
    log("### Xtrain ")    
    x_train = data['train']
    # x_test  = data['test']
    
    x_test = x_train[i_test]
    
    log( f"{cc.dname}")
    log( 'dataset shape', x_train.shape, )

    log( 'img shape', x_test[0].shape )
    (xdim, ydim, cdim) = x_test[0].shape
 
    # time.sleep(3)
    #np.savez_compressed( data_train + f"/{cc.dname}_check2.npz" ,
    #                     train=x_train[:100], test=x_test[:100])


    
elif cc.data_type == 'direct' :
     from util_train import CustomDataGenerator_img, train_transforms, test_transforms
     dfref      = pd_read_file( data_train + "/preprocessed_df.csv" )
     labels_col = [  'gender', 'masterCategory', 'subCategory', 'articleType' ]
     cc.labels_cnt = {}
     for ci in labels_col :
        cc.labels_cnt[ci] = dfref[ci].nunique()

        
else :    ###### Fake
    log("####### Fake Data  ")    
    image_size   = 64; num_train= 100 ; num_val = 10
    num_classes  = [ 7, 8, 9, 10]

    x_train =  np.random.randint(0, 256, size=(num_train, image_size, image_size, 3), dtype='uint8')
    y_train = [np.random.randint(0, 2,   size=(num_train, n_classes)) for n_classes in num_classes]

    x_test  =  np.random.randint(0, 256, size=(num_val, image_size, image_size, 3), dtype='uint8')
    y_test  = [np.random.randint(0, 2,   size=(num_val, n_classes)) for n_classes in num_classes]


    (xdim, ydim, cdim) = x_test[0].shape
    #### Mapping classname : nb of labels
    labels_col = ['gender', 'masterCategory', 'subCategory', 'articleType']
    cc.labels_cnt = { labels_col[i]: num_classes[i] for i in range(len(labels_col))  }


    
log('##### Check data ########################################################')    
log('xtrain, xtest', len(x_train) , len(x_test), x_train[0].shape)    
log('ytrain', len(y_train) , len(y_train[0]) )
log('ytest',  len(y_test)  , len(y_test[0]) )

log('cc.labels_cnt'   , cc.labels_cnt)
train_size = min(cc.nmax, x_train.shape[0])
test_size  = min(cc.nmax, x_test.shape[0])
log('Actual train, test: ', train_size, test_size)
log('x_train:',      str(x_train[0])[:10])

log( 'xdim, ydim', xdim, ydim )
cc.xdim, cc.ydim, cc.cdim = xdim, ydim, cdim
time.sleep(4)

    


################################################################################
log("\n\n##### Build Model   #################################################")
from tensorflow.keras import layers,regularizers

#### Deep Feature Consistent Variational Autoencoder Class
class DFC_VAE(tf.keras.Model):
  def __init__(self, latent_dim, labels_cnt):
    super(DFC_VAE, self).__init__()
    self.latent_dim = cc.latent_dim
    self.encoder    = make_encoder()
    self.decoder    = make_decoder()
    
    self.classifier = make_classifier(labels_cnt)

  def encode(self, x):
    z_mean, z_logsigma = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
    return z_mean, z_logsigma
  
  def reparameterize(self, z_mean, z_logsigma):
    eps = tf.random.normal(shape=z_mean.shape)
    return eps * tf.exp(z_logsigma * 0.5) + z_mean

  def decode(self, z, apply_sigmoid=False):
    x_recon = self.decoder(z)
    if apply_sigmoid:
      new_x_recon = tf.sigmoid(x_recon)
      return new_x_recon
    return x_recon

  ### bug when saving https://github.com/tensorflow/tensorflow/issues/37439
  def call(self, x,training=True, mask=None): 
    # out_classes = None
    z_mean, z_logsigma = self.encode(x)
    z                  = self.reparameterize(z_mean, z_logsigma)
    x_recon            = self.decode(z)
    
    #### Classifier
    out_classes = self.classifier(z)
    return z_mean, z_logsigma, x_recon, out_classes


def make_encoder(n_outputs = 1):
  #Functionally define the different layer types
  Input              = tf.keras.layers.InputLayer
  Conv2D             = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu',
                                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001), 
                                         activity_regularizer=regularizers.l2(1e-5)
                                        )
  BatchNormalization = tf.keras.layers.BatchNormalization
  Flatten            = tf.keras.layers.Flatten
  Dense              = functools.partial(tf.keras.layers.Dense, activation='relu',
                                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                         bias_regularizer=regularizers.l2(1e-4), activity_regularizer=regularizers.l2(1e-5)
                                        )

  ##### Build the encoder network using the Sequential API
  encoder = tf.keras.Sequential([
    Input(input_shape=(xdim, ydim, 3)),

    Conv2D(filters=1*cc.n_filters, kernel_size=5,  strides=2),
    BatchNormalization(),
    
    Conv2D(filters=2*cc.n_filters, kernel_size=5,  strides=2),
    BatchNormalization(),

    Conv2D(filters=4*cc.n_filters, kernel_size=3,  strides=2),
    BatchNormalization(),

    Conv2D(filters=6*cc.n_filters, kernel_size=3,  strides=2),
    BatchNormalization(),

    Flatten(),
    Dense(512*1, activation='relu'),
    layers.Dropout(0.2),  
    Dense(512*1, activation='relu'),
    Dense(2*cc.latent_dim, activation=None),
  ])

  return encoder


def make_decoder():
  
  #Functionally define the different layer types
  Input              = tf.keras.layers.InputLayer

  # bias_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001)
  Dense              = functools.partial(tf.keras.layers.Dense, activation='relu', 
                                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                         bias_regularizer=regularizers.l2(1e-4), activity_regularizer=regularizers.l2(1e-5)
                                        )
  Reshape            = tf.keras.layers.Reshape
  Conv2DTranspose    = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu',
                                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001), 
                                         activity_regularizer=regularizers.l2(1e-5)
                                        )
  BatchNormalization = tf.keras.layers.BatchNormalization
  Flatten            = tf.keras.layers.Flatten

  #Build the decoder network using the Sequential API
  if xdim == 64 :   #### 64 x 64 img         
      decoder = tf.keras.Sequential([
        Input(input_shape=(cc.latent_dim,)),

        Dense(units= 4*4  *6*cc.n_filters),
        Dense(units= 4*4  *6*cc.n_filters),
        layers.Dropout(0.2),   
        Dense(units= 4*4  *6*cc.n_filters),    
        Reshape(target_shape=(4, 4, 6*cc.n_filters)),
        #### ValueError: total size of new array must be unchanged, input_shape = [2304], output_shape = [7, 4, 144]

        #### Conv. layer      
        Conv2DTranspose(filters=4*cc.n_filters, kernel_size=3,  strides=2),
        Conv2DTranspose(filters=2*cc.n_filters, kernel_size=3,  strides=2),
        Conv2DTranspose(filters=1*cc.n_filters, kernel_size=5,  strides=2),

        Conv2DTranspose(filters=3, kernel_size=5,  strides=2),      
        # Conv2DTranspose(filters=4, kernel_size=5,  strides=2),

      ])


  if ydim == 256 :  ### 256 8 256 img          
      decoder = tf.keras.Sequential([
        Input(input_shape=(cc.latent_dim,)),

        Dense(units=16*16  *6*cc.n_filters),
        Dense(units=16*16  *6*cc.n_filters),
        layers.Dropout(0.2),   
        Dense(units=16*16  *6*cc.n_filters),    
        Reshape(target_shape=(16, 16, 6*cc.n_filters)),

        #### Conv. layer      
        Conv2DTranspose(filters=4*cc.n_filters, kernel_size=3,  strides=2),
        Conv2DTranspose(filters=2*cc.n_filters, kernel_size=3,  strides=2),
        Conv2DTranspose(filters=1*cc.n_filters, kernel_size=5,  strides=2),
        Conv2DTranspose(filters=3, kernel_size=5,  strides=2),      

      ])
  return decoder



#### Classifier
def clf_loss_crossentropy(y_true, y_pred):
    return tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)


def make_classifier(nclass_dict):
   
  Input              = tf.keras.layers.InputLayer
  Dense              = functools.partial(tf.keras.layers.Dense, activation='relu', 
                                         kernel_regularizer   = tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                         bias_regularizer     = regularizers.l2(1e-4), 
                                         activity_regularizer = regularizers.l2(1e-5)
                                        )
  Reshape            = tf.keras.layers.Reshape
  BatchNormalization = tf.keras.layers.BatchNormalization


  label_list  = [  'gender', 'masterCategory', 'subCategory', 'articleType' ]
  if xdim == 64 or xdim == 256 :   #### 64 x 64 img                 
        base_model = tf.keras.Sequential([
            Input(input_shape=(cc.latent_dim,)),
            Dense(units= 256 ),
            layers.Dropout(0.15),         
            Dense(units= 128),
        ])

        x = base_model.output
        ## x = layers.Flatten()(x) already flatten
        
        #### Multi-heads
        outputs = [ Dense( nclass_dict[ci], activation='softmax', name= f'out_{ci}')(x)  for ci in label_list  ] 
        clf     = tf.keras.Model(name='clf', inputs= base_model.input , outputs= outputs)  
        
  return clf



#### Loss Function  ########################################################################
#### Pretrained EfficientNetB0 Network 
percep_model = tf.keras.applications.EfficientNetB0(
    include_top=False, weights='imagenet', input_tensor=None,
    input_shape=(xdim, ydim, cdim), pooling=None, classes=1000,
    classifier_activation='softmax'
)

from util_train import clf_loss_macro_soft_f1

def loss_total_function(x, x_recon, z_mean, z_logsigma, kl_weight=0.00005, 
                        y_label_heads=None, y_pred_heads=None, clf_loss_fn=None):
      ### log( 'x_recon.shae',  x_recon.shape )  
      ### VAE Loss
      reconstruction_loss = tf.reduce_mean(tf.reduce_mean(tf.abs(x-x_recon), axis=(1,2,3)))
      latent_loss         = 0.5 * tf.reduce_sum(tf.exp(z_logsigma) + tf.square(z_mean) - 1.0 - z_logsigma, axis=1)

      ### Efficient Head Loss
      a = 0.015  # 0.015
      perceptual_loss  = tf.reduce_sum(tf.square(tf.subtract(tf.stop_gradient(percep_model(x)),percep_model(x_recon))))
      loss_all         = kl_weight*latent_loss + reconstruction_loss + a*perceptual_loss  

      ### Classifier Loss 
      if y_label_heads is not None :
          loss_clf = []
          for i in range(len(y_pred_heads) ):
                # loss_clf.append( 0.0 )                        
                head_loss = clf_loss_fn(y_label_heads[i], y_pred_heads[i])
                # head_loss = clf_loss_macro_soft_f1(y_label_heads[i], y_pred_heads[i])
                loss_clf.append(head_loss)

          loss_clf = tf.reduce_mean(loss_clf) * 0.01 # + 10 # 000.0
          loss_all = loss_all * 2.0 + loss_clf    

      return loss_all



###########################################################################################
### Training setup
@tf.function
def train_step(x, model, optimizer, y_label_list=None):
  with tf.GradientTape() as tape:
    z_mean, z_logsigma, x_recon, y_pred = model.call(x)      #Forward pass through the VAE
    loss = loss_total_function(x, x_recon, z_mean, z_logsigma,
              y_label_heads = y_label_list, 
              y_pred_heads  = y_pred, 
              clf_loss_fn   = clf_loss_crossentropy
    )  

  grads = tape.gradient(loss, model.trainable_variables)   #Calculate gradients
  optimizer.apply_gradients(zip(grads, model.trainable_variables))
  return loss


@tf.function
def validation_step(x, model):
  z_mean, z_logsigma, x_recon, y_pred = model.call(x)  #Forward pass through the VAE
  loss                        = loss_total_function(x, x_recon, z_mean, z_logsigma)
  return loss, x_recon, y_pred



def model_reload(cc,):    
    model2      = DFC_VAE(cc.latent_dim, cc.labels_cnt)
    input_shape = (cc.batch_size, cc.xdim, cc.ydim, cc.cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
    model2.build(input_shape)
    model2.load_weights( cc.model_dir2 + f'/best/model_keras_weights.h5')
    return model2




    
###########################################################################################
####### Dataset ###########################################################################  
if cc.data_gen_name == 'album1':
   from util_train import CustomDataGenerator, train_augments, test_augments
   train_data = CustomDataGenerator(x_train, y_train, augmentations=train_augments)
   test_data  = CustomDataGenerator(x_test, y_test, augmentations=test_augments)


if cc.data_gen_name == 'direct':
    from util_train import CustomDataGenerator_img, train_transforms, test_transforms
   
    train_img_dir  = data_dir + '/train/*'
    test_img_dir   = data_dir + '/test/*'    
    label_file = data_label  +"/preprocessed_df.csv"
    labels_col = [ 'gender', 'masterCategory', 'subCategory', 'articleType' ]
    
    train_data = CustomDataGenerator_img(train_img_dir, label_file, labels_col, split='train', transforms= train_transforms)
    test_data  = CustomDataGenerator_img(test_img_dir,  label_file, labels_col, split='val',   transforms= test_transforms)

    


    
###########################################################################################
###### Optimizer ##########################################################################
if   cc.opt_name == 'ada-bound':  
  from keras_adabound import AdaBound
  optimizer = AdaBound(lr=1e-3, final_lr=0.1)

elif cc.opt_name == 'radam':      
  from keras_radam import RAdam
  optimizer = RAdam(learning_rate=0.001)

elif cc.opt_name == 'madgrad':    
  from madgrad import MadGrad
  optimizer = MadGrad(lr=0.01)
else :    optimizer = tf.keras.optimizers.Adam(learning_rate=cc.learning_rate)




log("\n\n### Train Model ###############################################################")
log('x_train',      str(x_train)[:10])
log('shape train, test', train_size, test_size)


##### Hyperparameters  ###################################################################
cc.model_dir2 = model_dir + f"/m_{cc.tag}-{cc.dname}/"
os.makedirs(cc.model_dir2, exist_ok=True)


#### Model Instance
try :
   model  = model_reload(cc)
   log('\n\n model RELOADED', model)
except :    
   model          = DFC_VAE(cc.latent_dim, cc.labels_cnt)
   log('\n\n model NEW', model)

    
#### Training loop
from box import Box
dd = Box({})
dd.kbatch          = train_size // cc.batch_size
dd.train_loss_hist = []
dd.valid_loss_hist = []
dd.counter         = 0 
dd.dostop          = False
dd.best_loss       = 100000000.0
dd.test_accuracy   = {}
dd.labels_col      = labels_col
### Validation dataset
valid_image_check(x_test, path= cc.model_dir2 + "/check/", 
                  tag= f"0ref", n_sample=10, renorm=True)
### Dump Data
json.dump(cc, open(cc.model_dir2 +"/info.json" , mode='w'))


for i in range(cc.num_epochs):
    log(f"Starting epoch {i+1}/{cc.num_epochs}, in {dd.kbatch} kbatches ")
    if dd.dostop: break   

    for j, (x, *y_label_list) in enumerate(train_data):        
        if j > cc.max_batch : dostop= True
        if dd.dostop:  break 
        #(x, *y_label_list) = get_data_sample(cc.batch_size, x_train, labels_val)                
        # log5(x) ; log5(y_label_list )

        train_loss = train_step(x, model, optimizer, y_label_list = y_label_list)
        dd.train_loss_hist.append( np.mean( train_loss.numpy() ) )        


        if (j + cc['kloop'] ) % cc['kloop']  == 0 :
           valid_loss, x_recon, y_pred  = validation_step(x_test, model)    
           dd.valid_loss_hist.append( np.mean( valid_loss.numpy()  ) )

           log(i, j, 'train,valid', dd.train_loss_hist[-1], dd.valid_loss_hist[-1] )                               
           dd.test_accuracy = metric_accuracy(y_test, y_pred, dd)

            
           dd.best_loss, dd.counter = save_best(model, cc.model_dir2, dd.valid_loss_hist[-1], dd.best_loss, dd.counter)
           dd.dostop                = train_stop(dd.counter, cc['patience']) 
           json.dump({'res' : str(dd)}, open(cc.model_dir2 +"/results.json" , mode='w'))           
                    

        if (j + 400 ) % 400 == 0 :
            valid_image_check(x_recon, path= cc.model_dir2 + "/check/", 
                              tag= f"e{i}_b{j}", n_sample=10, renorm=True)

        
log('counter', counter) 
log(dd)
log('Final valid_loss', str(valid_loss_hist)[:200] )





    
log("\n\n#### Saving Model ######################################################")    
os.makedirs(cc.model_dir2, exist_ok=True)
#model.save( model_dir + "/model.h5")
# input_shape = (cc.batch_size, xdim, ydim, cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
# model.build(input_shape)
# model.save( model_dir + "/model_tf", save_format='tf') 
tf.saved_model.save(model,   cc.model_dir2 )
model.save_weights( cc.model_dir2 + f'/model_keras_weights.h5')



log(cc.model_dir2)



log('\n\n##### Reload Model ####################################################',)
##### model2    = tf.saved_model.load(cc.model_dir2)
model2      = DFC_VAE(cc.latent_dim, cc.labels_cnt)
input_shape = (cc.batch_size, xdim, ydim, cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
model2.build(input_shape)
model2.load_weights( cc.model_dir2 + f'/model_keras_weights.h5')
# model2  = tf.keras.models.load_model(cc.model_dir2) 
log('##### Reloaded', model2)
log('rerun eval', validation_step(x_test, model2))


"""

















"""
# OLD"""

# Dummy data
num_train = 1000
num_val = 500
num_classes = [7, 7, 7, 7, 7]  # Number of classes each head
image_size = 224

# The image values should be range in [0, 1]
# We're using sparse form for targets
x_train = np.random.randint(0, 256, size=(num_train, image_size, image_size, 3), dtype='uint8')
y_train = [np.random.randint(0, 2, size=(num_train, n_classes)) for n_classes in num_classes]

x_val = np.random.randint(0, 256, size=(num_val, image_size, image_size, 3), dtype='uint8')
y_val = [np.random.randint(0, 2, size=(num_val, n_classes)) for n_classes in num_classes]

print(f'x_train: {x_train.shape}')
print(f'x_val: {x_val.shape}')
print(y_train[0].shape)

from albumentations.core.transforms_interface import ImageOnlyTransform

class SprinklesTransform(ImageOnlyTransform):
    """SprinklesTransform Class"""
    def __init__(self, num_holes=100, side_length=10, always_apply=False, p=1.0):
        super(SprinklesTransform, self).__init__(always_apply, p)
        self.sprinkles = Sprinkles(num_holes=num_holes, side_length=side_length)
    
    def apply(self, image, **params):
        if isinstance(image, Image.Image):
            image = tf.constant(np.array(image), dtype=tf.float32)
        elif isinstance(image, np.ndarray):
            image = tf.constant(image, dtype=tf.float32)

        return self.sprinkles(image).numpy()



# # Data Augmentation with built-in Keras functions
# train_gen = keras.preprocessing.image.ImageDataGenerator(
#     rotation_range=20,
#     width_shift_range=20,
#     height_shift_range=20,
#     brightness_range=[0.2, 1.0],
#     shear_range=20,
#     horizontal_flip=True,
#     rescale=1./255
# )

# test_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

# train_data = train_gen.flow(x_train, y_train)
# val_data = test_gen.flow(x_val, y_val)


from albumentations import (
    Compose, HorizontalFlip, CLAHE, HueSaturationValue,
    RandomBrightness, RandomContrast, RandomGamma,
    ToFloat, ShiftScaleRotate, 
)

train_augments = Compose([
    HorizontalFlip(p=0.5),
    RandomContrast(limit=0.2, p=0.5),
    RandomGamma(gamma_limit=(80, 120), p=0.5),
    RandomBrightness(limit=0.2, p=0.5),
    HueSaturationValue(hue_shift_limit=5, sat_shift_limit=20,
                       val_shift_limit=10, p=.9),
    ShiftScaleRotate(
        shift_limit=0.0625, scale_limit=0.1, 
        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), 
    ToFloat(max_value=255),
    SprinklesTransform(p=0.5),
])

test_augments = Compose([
    ToFloat(max_value=255)
])

train_data = CustomDataGenerator(x_train, y_train, augmentations=train_augments)
val_data   = CustomDataGenerator(x_train, y_train, augmentations=test_augments)




"""Train the model (Data augmentation)"""

@tf.function
def train_step(x, y, model, loss_fn, optimizer):
    """
    """
    with tf.GradientTape() as tape:
        outputs = model(x, training=True)

        all_losses = []
        for y_true_head, y_pred_head in zip(y, outputs):
            head_loss = loss_fn(y_true_head, y_pred_head)
            all_losses.append(head_loss)
        
        loss = tf.reduce_mean(all_losses)
    
    grad = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grad, model.trainable_variables))
    return loss, all_losses, outputs


@tf.function
def test_step(x, y, model, loss_fn):
    """
    """
    with tf.GradientTape() as tape:
        outputs = model(x, training=False)
        
        all_losses = []
        for y_true_head, y_pred_head in zip(y, outputs):
            head_loss = loss_fn(y_true_head, y_pred_head)
            all_losses.append(head_loss)
        
        loss = tf.reduce_mean(all_losses)
    return loss, all_losses, outputs


input_shape = (image_size, image_size, 3)
num_classes = [7, 7, 7, 7, 7]
batch_size = 32
epochs = 2
learning_rate = 0.001

model = build_model(input_shape=input_shape,
                    num_classes=num_classes)
model.summary()

# Optimizers
opt = 'madgrad'
if opt == 'adam':
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
elif opt == 'adamw':
    optimizer = AdamW(lr=1e-4, model=model, lr_multipliers={'lstm_1': 0.5},
                    use_cosine_annealing=True, total_iterations=24)
elif opt == 'ada-bound':
    optimizer = AdaBound(lr=1e-3, final_lr=0.1)
elif opt == 'radam':
    optimizer = RAdam(learning_rate=0.001)
elif opt == 'madgrad':
    optimizer = MadGrad(lr=0.01)

accuracies = [keras.metrics.CategoricalAccuracy(name='accuracy_head_%i') for i in range(len(num_classes))]
loss = keras.metrics.Mean(name='loss')
head_losses = [keras.metrics.Mean(name='loss_head_%i') for i in range(len(num_classes))]

val_accuracies = [keras.metrics.CategoricalAccuracy(name='val_accuracy_head_%i') for i in range(len(num_classes))]
val_loss = keras.metrics.Mean(name='loss')
val_head_losses = [keras.metrics.Mean(name='val_loss_head_%i') for i in range(len(num_classes))]


print('Start training...')
for epoch in range(epochs):
    # Training
    for batch_idx, (images, *labels) in enumerate(train_data):
        batch_loss, loss_list, batch_outputs = train_step(images, labels, model, custom_loss, optimizer)

        for i in range(len(batch_outputs)):
            accuracies[i].update_state(labels[i], batch_outputs[i])
            head_losses[i].update_state(loss_list[i])
        loss.update_state(batch_loss)

        if (batch_idx + 1) % 10 == 0:
            print('[Epoch {:03d} iter {:04d}] - Loss: {:.3f} - {} - {}'.format(
                epoch + 1, batch_idx + 1, loss.result(),
                ' - '.join(['Head_{}_Loss: {:.3f}'.format(i, l.result()) for i, l in enumerate(head_losses)]),
                ' - '.join(['Head_{}_Accuracy: {:.3f}'.format(i, accuracy.result()) for i, accuracy in enumerate(accuracies)])
            ))
    
    # Validation
    print('\nEvaluating...')
    for i, (images, *labels) in enumerate(val_data):
        batch_loss, loss_list, batch_outputs = test_step(images, labels, model, custom_loss)

        for i in range(len(batch_outputs)):
            val_accuracies[i].update_state(labels[i], batch_outputs[i])
            val_head_losses[i].update_state(loss_list[i])
        loss.update_state(batch_loss)
    
    print('[Epoch {:03d}] - Loss: {:.3f} - {} - {}\n'.format(
        epoch + 1, val_loss.result(),
        ' - '.join(['Head_{}_Loss: {:.3f}'.format(i, l.result()) for i, l in enumerate(val_head_losses)]),
        ' - '.join(['Head_{}_Accuracy: {:.3f}'.format(i, accuracy.result()) for i, accuracy in enumerate(val_accuracies)])
    ))
    
    for acc in accuracies:
        acc.reset_state()

    for val_acc in val_accuracies:
        val_acc.reset_state()

"""Train the model [Original]"""

# @tf.function
# def train_step(x, y, model, loss_fn, optimizer):
#     """
#     """
#     with tf.GradientTape() as tape:
#         outputs = model(x, training=True)

#         all_losses = []
#         for y_true_head, y_pred_head in zip(y, outputs):
#             head_loss = loss_fn(y_true_head, y_pred_head)
#             all_losses.append(head_loss)
        
#         loss = tf.reduce_mean(all_losses)
    
#     grad = tape.gradient(loss, model.trainable_variables)
#     optimizer.apply_gradients(zip(grad, model.trainable_variables))
#     return loss, all_losses, outputs


# @tf.function
# def test_step(x, y, model, loss_fn):
#     """
#     """
#     with tf.GradientTape() as tape:
#         outputs = model(x, training=False)
        
#         all_losses = []
#         for y_true_head, y_pred_head in zip(y, outputs):
#             head_loss = loss_fn(y_true_head, y_pred_head)
#             all_losses.append(head_loss)
        
#         loss = tf.reduce_mean(all_losses)
#     return loss, all_losses, outputs


# input_shape = (image_size, image_size, 3)
# num_classes = [7, 7, 7, 7, 7]
# batch_size = 32
# epochs = 2
# learning_rate = 0.001

# model = build_model(input_shape=input_shape,
#                     num_classes=num_classes)
# model.summary()
# optimizer = keras.optimizers.Adam(learning_rate=learning_rate)

# accuracies = [keras.metrics.CategoricalAccuracy(name='accuracy_head_%i') for i in range(len(num_classes))]
# loss = keras.metrics.Mean(name='loss')
# head_losses = [keras.metrics.Mean(name='loss_head_%i') for i in range(len(num_classes))]

# val_accuracies = [keras.metrics.CategoricalAccuracy(name='val_accuracy_head_%i') for i in range(len(num_classes))]
# val_loss = keras.metrics.Mean(name='loss')
# val_head_losses = [keras.metrics.Mean(name='val_loss_head_%i') for i in range(len(num_classes))]

# train_data = tf.data.TFRecordDataset.from_tensor_slices((x_train, *y_train))\
#     .shuffle(buffer_size=num_train)\
#     .batch(batch_size, num_parallel_calls=2)\
#     .prefetch(1)

# val_data = tf.data.TFRecordDataset.from_tensor_slices((x_val, *y_val)) \
#     .batch(batch_size, num_parallel_calls=2) \
#     .prefetch(1)

# print('Start training...')
# for epoch in range(epochs):
#     # Training
#     for batch_idx, (images, *labels) in enumerate(train_data):
#         batch_loss, loss_list, batch_outputs = train_step(images, labels, model, custom_loss, optimizer)

#         for i in range(len(batch_outputs)):
#             accuracies[i].update_state(labels[i], batch_outputs[i])
#             head_losses[i].update_state(loss_list[i])
#         loss.update_state(batch_loss)

#         if (batch_idx + 1) % 10 == 0:
#             print('[Epoch {:03d} iter {:04d}] - Loss: {:.3f} - {} - {}'.format(
#                 epoch + 1, batch_idx + 1, loss.result(),
#                 ' - '.join(['Head_{}_Loss: {:.3f}'.format(i, l.result()) for i, l in enumerate(head_losses)]),
#                 ' - '.join(['Head_{}_Accuracy: {:.3f}'.format(i, accuracy.result()) for i, accuracy in enumerate(accuracies)])
#             ))
    
#     # Validation
#     print('\nEvaluating...')
#     for i, (images, *labels) in enumerate(val_data):
#         batch_loss, loss_list, batch_outputs = test_step(images, labels, model, custom_loss)

#         for i in range(len(batch_outputs)):
#             val_accuracies[i].update_state(labels[i], batch_outputs[i])
#             val_head_losses[i].update_state(loss_list[i])
#         loss.update_state(batch_loss)
    
#     print('[Epoch {:03d}] - Loss: {:.3f} - {} - {}\n'.format(
#         epoch + 1, val_loss.result(),
#         ' - '.join(['Head_{}_Loss: {:.3f}'.format(i, l.result()) for i, l in enumerate(val_head_losses)]),
#         ' - '.join(['Head_{}_Accuracy: {:.3f}'.format(i, accuracy.result()) for i, accuracy in enumerate(val_accuracies)])
#     ))
    
#     for acc in accuracies:
#         acc.reset_state()
    
#     # for head_loss in head_losses:
#     #     head_loss.reset_state()
#     # loss.reset_state()

#     for val_acc in val_accuracies:
#         val_acc.reset_state()
#     # for val_head_loss in val_head_losses:
#     #     val_head_loss.reset_state()
#     # val_loss.reset_state()
