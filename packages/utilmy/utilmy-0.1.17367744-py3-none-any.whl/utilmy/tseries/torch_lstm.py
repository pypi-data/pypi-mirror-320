# -*- coding: utf-8 -*-
"""torch_lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/arampacha/2cfa109f0b9a43828e4a99b130abe917/torch_lstm.ipynb
"""

import torch
from torch import nn
import torch.nn.functional as F

from fastai.callback.hook import Hooks

bs, seq_len, d_in = 4, 10, 8
x = torch.randn(bs, seq_len, d_in)

d_h = 16
rnn = nn.LSTM(d_in, d_h, batch_first=True)

for n, p in rnn.named_parameters():
    print(f"{n:<12} {p.shape}")

out , (h, c) = rnn(x)
out.shape, h.shape, c.shape

assert (out[:, -1, :] == h.squeeze()).all()

out2, _ = rnn(x, (h, c))

assert not torch.allclose(out.data, out2.data)

rnn2 = nn.LSTM(d_in, d_h, num_layers=2, batch_first=True)

for n, p in rnn2.named_parameters():
    print(f"{n:<12} {p.shape}")

out , (h, c) = rnn2(x)
out.shape, h.shape, c.shape

assert (out[:, -1, :] == h[-1]).all()

rnn_b = nn.LSTM(d_in, d_h, num_layers=1, bidirectional=True, batch_first=True)

for n, p in rnn_b.named_parameters():
    print(f"{n:<20} {p.shape}")

out , (h, c) = rnn_b(x)
out.shape, h.shape, c.shape

assert (out[:, -1, :16] == h[0]).all()
assert (out[:, 0, 16:] == h[1]).all()

rnn_b2 = nn.LSTM(d_in, d_h, num_layers=2, bidirectional=True, batch_first=True)

for n, p in rnn_b.named_parameters():
    print(f"{n:<20} {p.shape}")

out, (h, c) = rnn_b2(x)
out.shape, h.shape, c.shape

assert (out[:, -1, :16] == h[2]).all()
assert (out[:, 0, 16:] == h[3]).all()

x.requires_grad_()
out, (h, c) = rnn(x)

loss = out.sum()
grad_x, = torch.autograd.grad(loss, x, create_graph=True)

import torchviz

torchviz.make_dot((grad_x, x, out), {"grad_x": grad_x, "x": x, "loss": loss})

hook_ih = rnn.weight_ih_l0.register_hook(lambda x: print(f"comuted ih grad shape {x.shape}"))
hook_hh = rnn.weight_hh_l0.register_hook(lambda x: print(f"comuted hh grad shape {x.shape}"))

out, (h, c)  = rnn(x)
out.mean().backward()

hook_ih.remove()
hook_hh.remove()

length = torch.randint(5, 10, (x.size(0), ))

from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence

x = [torch.randn(l, d_in) for l in length]
x = pad_sequence(x, batch_first=True)
x.shape

x_pack = pack_padded_sequence(x, length, enforce_sorted=False, batch_first=True)

out_pack, (h, c) = rnn(x_pack)

out, length_out = pad_packed_sequence(out_pack, batch_first=True)

assert (length_out == length).all()

(out[:, -1, :] == h[0]).all()

