{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(v4)DialoGPT_gen_topk.ipynb (turntaking_syntax + fulldialogue_refeed/tabdelimit)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnYifKtqSmmS",
        "colab_type": "code",
        "outputId": "d791c26e-2199-484d-d2dd-401a90f3790a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.3)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 4.8MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 62.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 72.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 58.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.13 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.13->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.13->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=b0b4ddc83489bfa02aa54a27c3cc5a4db296e60fdfdec9ae94798d169dab6573\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: regex, sacremoses, sentencepiece, transformers\n",
            "Successfully installed regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZd4iXX5SqKC",
        "colab_type": "code",
        "outputId": "92a782b1-c7b9-44f5-fb49-f813292240be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "import os\n",
        "# !wget https://convaisharables.blob.core.windows.net/lsp/multiref/large_ft.pkl\n",
        "!wget https://convaisharables.blob.core.windows.net/lsp/multiref/medium_ft.pkl\n",
        "  \n",
        "if not os.path.exists(\"DialoGPT\"):\n",
        "  !git clone https://github.com/microsoft/DialoGPT.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-11 20:39:45--  https://convaisharables.blob.core.windows.net/lsp/multiref/medium_ft.pkl\n",
            "Resolving convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)... 13.77.184.64\n",
            "Connecting to convaisharables.blob.core.windows.net (convaisharables.blob.core.windows.net)|13.77.184.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862954531 (823M) [application/octet-stream]\n",
            "Saving to: ‘medium_ft.pkl’\n",
            "\n",
            "medium_ft.pkl       100%[===================>] 822.98M  48.8MB/s    in 12s     \n",
            "\n",
            "2019-11-11 20:39:58 (66.8 MB/s) - ‘medium_ft.pkl’ saved [862954531/862954531]\n",
            "\n",
            "Cloning into 'DialoGPT'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 152 (delta 31), reused 56 (delta 15), pack-reused 74\u001b[K\n",
            "Receiving objects: 100% (152/152), 44.62 MiB | 31.10 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jwDKRIfk-Dzy",
        "outputId": "75f327c1-eaae-4c60-a267-1e119e7fd5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# weights = torch.load('large_ft.pkl')\n",
        "weights = torch.load('medium_ft.pkl')\n",
        "\n",
        "# cfg = GPT2Config(n_embd=1024,n_layer=24,n_head=16)\n",
        "cfg = GPT2Config.from_json_file('DialoGPT/configs/345M/config.json')\n",
        "model = GPT2LMHeadModel(cfg)\n",
        "\n",
        "# fix misused key value\n",
        "weights[\"lm_head.weight\"] = weights[\"lm_head.decoder.weight\"]\n",
        "weights.pop(\"lm_head.decoder.weight\",None)\n",
        "\n",
        "model.load_state_dict(weights)\n",
        "model.to('cuda')\n",
        "model.eval()\n",
        "\n",
        "conditioned_tokens = []\n",
        "generated_tokens = []\n",
        "past = None\n",
        "fst = True\n",
        "DELIMITER = \"/n\"\n",
        "\n",
        "def reinput(user_msg):\n",
        "\tglobal conditioned_tokens; global fst\n",
        "\tos.system('cls' if os.name == 'nt' else 'clear')\n",
        "\t  \n",
        "\tconditioned_tokens = tokenizer.decode(conditioned_tokens)\n",
        "\n",
        "\tprint(\"Me: \" + user_msg + \"\\n\" + \"Bot: \",end='')\n",
        "\t\n",
        "\tif fst:\n",
        "\t  fst = False\n",
        "\t  user_msg = \"<|endoftext|>\" + user_msg\n",
        "  \n",
        "\tuser_msg = \"\" + user_msg\n",
        "\tconditioned_tokens += user_msg \n",
        "\tconditioned_tokens = tokenizer.encode(conditioned_tokens) \n",
        "\tconditioned_tokens += [50256] # Append operator to prepend conversation history\n",
        "  \n",
        "\n",
        "\n",
        "def top_p_filtering(logits, top_p=0.9, filter_value=-float('Inf')):\n",
        "  \"\"\"\n",
        "  Credit: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "  \"\"\"\n",
        "  assert logits.dim() == 1  # batch size 1 for single word generation\n",
        "  sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "  cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "  # remove tokens with cumulative probability above the threshold\n",
        "  sorted_indices_to_remove = cumulative_probs > top_p\n",
        "  # shift the indices to the right to keep also the first token above the threshold\n",
        "  sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "  sorted_indices_to_remove[..., 0] = 0\n",
        "  indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "  logits[indices_to_remove] = filter_value\n",
        "  return logits\n",
        "\n",
        "\n",
        "def recalc(p=None):\n",
        "\tglobal conditioned_tokens\n",
        "\tglobal generated_tokens\n",
        "\tglobal past\n",
        "  \n",
        "  # for segment display purpose, keep 2 sets of tokens\n",
        "\tindexed_tokens = conditioned_tokens + generated_tokens\n",
        "  \n",
        "\ttokens_tensor = torch.tensor([indexed_tokens])\n",
        "  \n",
        "\ttokens_tensor = tokens_tensor.to('cuda')\n",
        "\twith torch.no_grad():\n",
        "\t    outputs,past = model(tokens_tensor, past=p)\n",
        "\t    predictions = outputs\n",
        "\tlogits = predictions[0, -1, :]\n",
        "\tfiltered_logits = top_p_filtering(logits)\n",
        "\tprobabilities = F.softmax(filtered_logits, dim=-1)\n",
        "\tnext_token = torch.multinomial(probabilities, 1)\n",
        "\tgenerated_tokens.append(next_token.item())\n",
        "\treturn next_token.item()\n",
        "\n",
        "def generate():\n",
        "\tglobal conditioned_tokens\n",
        "\tglobal generated_tokens\n",
        "\tglobal past\n",
        " \n",
        "\twhile True:\n",
        "\t\tif len(tokenizer.decode(conditioned_tokens)) > 320:\n",
        "\t\t  dc = tokenizer.decode(conditioned_tokens)\n",
        "\t\t  dc = dc[len(dc)-320:]\n",
        "\t\t  idx = dc.find(DELIMITER)\n",
        "\t\t  if idx != -1:\n",
        "\t\t    dc = dc[idx+len(DELIMITER):]\n",
        "\t\t    conditioned_tokens = tokenizer.encode(dc)\n",
        "    \n",
        "\t\tresult = recalc()\n",
        "\n",
        "\t\tif result == 50256:\n",
        "      \n",
        "      # end-of-text : 50256\n",
        "      # use this special token to split segments\n",
        "\n",
        "\t\t\tdecoded_reply = tokenizer.decode(generated_tokens)\n",
        "      \n",
        "\t\t\tto_print = decoded_reply\n",
        "\t\t\tif to_print.endswith(\"<|endoftext|>\"):\n",
        "\t\t\t  to_print = to_print[:-len(\"<|endoftext|>\")]\n",
        "\t\t\t  decoded_reply = to_print\n",
        "\t\t\tprint(to_print)\n",
        "\t\t\t\n",
        "\t\t\tdecoded_reply = \"\" + decoded_reply\n",
        "\t\t\tdecoded_reply = decoded_reply+DELIMITER\n",
        "\t\t\tconditioned_tokens += (tokenizer.encode(decoded_reply))\n",
        "    \n",
        "\t\t\t# Remove end of text tokens from feeding back into model\n",
        "      # -- see here for reason https://github.com/huggingface/transformers/issues/429#issuecomment-479380117\n",
        "\t\t\tcond_str = tokenizer.decode(conditioned_tokens)\n",
        "\t\t\tcond_str = cond_str.replace(\"<|endoftext|>\",DELIMITER) # Since \"<end-of-text> (50256)\" is mapped to the newline character (\\n) by default when calling decode():\n",
        "\t\t\tconditioned_tokens = tokenizer.encode(cond_str)\n",
        "    \n",
        "# Uncomment to debug (print) conversation history that is re-fed to the model\n",
        "# \t\t\tcond_str = tokenizer.decode(conditioned_tokens)\n",
        "# \t\t\tcond_str = cond_str.replace(\"<|endoftext_|>\",\"\\n\").replace(DELIMITER,\"\\n\") # Since \"<end-of-text> (50256)\" is mapped to the newline character (\\n) by default when calling decode():\n",
        "# \t\t\tprint(\"(condstr_dbg) \" + cond_str)\n",
        "# \t\t\tprint(r\"(/constr_dbg)\")\n",
        "    \n",
        "\t\t\tgenerated_tokens = []\n",
        "\t\t\tbreak"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 243712/1042301 [00:00<00:01, 551866.75B/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlgZWTWPTWiA",
        "colab_type": "text"
      },
      "source": [
        "How to use:\n",
        "\n",
        "This script will automatically generate a complete segment(or a post if you wish) and wait for user input.\n",
        "1. Press Enter (input nothing) to keep generating under current topic.\n",
        "2. Type in prompt text to start another topic.\n",
        "\n",
        "3. You can now type 'reset' to start the conversation over."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jj5M-uxNPfZZ",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import logging\n",
        "\n",
        "def new_dialogue():\n",
        "\n",
        "  global conditioned_tokens\n",
        "  global generated_tokens\n",
        "  global past; global fst\n",
        "  \n",
        "  conditioned_tokens = []; generated_tokens = []; past = None; fst = True\n",
        "  reinput(\"What is the meaning of life?\")\n",
        "  generate()\n",
        "\n",
        "  while True:\n",
        "    cmd = input()\n",
        "\n",
        "    if cmd == \"reset\":\n",
        "      clear_output()\n",
        "      new_dialogue()\n",
        "      break\n",
        "\n",
        "    if cmd != \"\":\t\n",
        "      reinput(cmd)\n",
        "    generate()\n",
        "    \n",
        "new_dialogue()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}