# -*- coding: utf-8 -*-
"""training lightgbm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ITrg8oxp7eVbpf-ZROcZgBSxfE1ReZ2B
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! pip install utilmy python-box dirty-cat category_encoders xxhash
# ! pip install git+https://github.com/MaxHalford/xam
#

import os,sys, pandas as pd, polars as pl, numpy as np, scipy as sp
from box import Box
from utilmy import (log, pd_read_file, pd_to_file, date_now, os_makedirs,
     glob_glob, log2)
import  lightgbm
os.environ["CUDA_VISIBLE_DEVICES"]="3"
from sklearn.metrics import f1_score
from functools import partial
import category_encoders as ce


def log_pd(df):
    log(list(df.columns))
    log(df.shape)

def np_unique(ll:list)->list:
   ll2 =[]
   for x in ll:
       if x not in ll2:
           ll2.append(x)
   return ll2

from google.colab import drive
drive.mount('/content/drive')

!ls





"""# 1) Change dir googleDrive"""

from google.colab import drive
drive.mount('/content/drive')



"""### **Create Shortcut of folder "data3-final" to YOUR GOOGLE DRIVE **

######################################################
Create a shortcut of the folder data3-final
in YOUR Google Drive
So you can access from Colab --> google drive path

https://www.youtube.com/watch?v=7baCEh0kaPU

############################################################
""

![image.png](data:image/png;base64,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)
"""

import pandas as pd

#  Check if the foldre is accessible file using Pandas
dftrain = pd.read_parquet("/content/drive/MyDrive/data3-final/data3-final/dftrain.parquet")
dfeval = pd.read_parquet("/content/drive/MyDrive/data3-final/data3-final/dfeval.parquet")

dftrain.shape, dfeval.shape

dftrain = dftrain[ dftrain.ymd <= 20230830 ]
dftrain.shape, dfeval.shape

import os
os.getcwd()

### Data Folder

myshortcut_folder = "data3-final/"

dir0="/content/drive/MyDrive/" + myshortcut_folder
os.chdir(dir0)
log(os.getcwd())
log(glob_glob(dir0 + "/*"))







"""# 2) Load data"""

dftrain = pd_read_file(dir0 + "/dftrain*")
dfval   = pd_read_file(dir0 + "/dfeval*")


dftrain = dftrain[ dftrain.ymd <= 20230830 ] ###no overlap train/test
dftrain = dftrain.sample(frac= 0.6)
print(dftrain.ymd.max())



dftrain

dfval







"""# 3) Train V1"""

""" TODO :
    1) Try to XAM encoder, bayesian stuff,  Cycle transformer, cosinus, sinus

    https://github.com/MaxHalford/xam/blob/master/docs/feature-extraction.md
    trans = xam.feature_extraction.CycleTransformer()


    https://contrib.scikit-learn.org/category_encoders/

        lname = ["category_encoders.BackwardDifferenceEncoder"
        ,"category_encoders.BaseNEncoder"
        ,"category_encoders.BinaryEncoder"
        ,"category_encoders.CatBoostEncoder"
        ,"category_encoders.CountEncoder"
        ,"category_encoders.GLMMEncoder"
        ,"category_encoders.HashingEncoder"
        ,"category_encoders.HelmertEncoder"
        ,"category_encoders.JamesSteinEncoder"
        ,"category_encoders.LeaveOneOutEncoder"
        ,"category_encoders.MEstimateEncoder"
        #,"category_encoders.OneHotEncoder"
        ,"category_encoders.OrdinalEncoder"
        ,"category_encoders.PolynomialEncoder"
        ,"category_encoders.QuantileEncoder"
        ,"category_encoders.SumEncoder"
        ,"category_encoders.TargetEncoder"
        ,"category_encoders.WOEEncoder"

    ## Results

"""


#################################################################################
cols_drop = [ "ymd", "y", "dt", "dt-month",  ]

cols_leak = [
    *[f"fe-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
    *[f"com-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
    *[f"qkey-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
    *[f"comcat1_hash-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
]
print('cols_leak\n', cols_leak)


cols_targetenc = [
    'dt-day',
    'dt-hour', 'dt-weekday', 'dt-islunch', 'dt-isevening', 'dt-isweekend',
    "hh",
    "qkey",
    'qkey-zoom',
    'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1'

    # "com",
    # 'comcat1_hash',
    "fe",
]

### Initial values are STRING
cols_ordinalenc = [
    "com",
    "comcat1_hash"
    # "qkey",
    # "fe",
]

dftrain[cols_leak].shape





from lightgbm import LGBMClassifier


X_train = dftrain.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
y_train = dftrain["y"]

X_val = dfval.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
y_val = dfval["y"]


log("target encoder") ### Considered as Float numerical
target_encoder = ce.target_encoder.TargetEncoder(cols=cols_ordinalenc)
X_train = target_encoder.fit_transform(X_train, y_train)
X_val   = target_encoder.transform(X_val)


log("ordinal encoder") ### Considered as Float numerical
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cols_targetenc)
X_train = ordinal_encoder.fit_transform(X_train, y_train)
X_val   = ordinal_encoder.transform(X_val)


log("train")
pars_lgm = {
    "objective": "binary",
    "iterations": 10000,
    "random_seed": 42,
    "early_stopping_rounds": 300,
    "learning_rate": 0.05,
    ###
    'bagging_fraction': 0.8,
    "colsample_bytree": 0.9,
    'max_depth': 7,
}
model = LGBMClassifier(**pars_lgm, )


pd_to_file( X_train.head(2).T, "ztmp/atest/xtrain.csv", sep="\t", show=1 )

model.fit( X_train, y_train, eval_set=[(X_val, y_val)])

yval_pred = model.predict(X_val)
score     = f1_score(y_val, yval_pred )
log('F1 Score Val:', score)

















"""# Best baseline"""

def get_best_baseline(dftrain=None, dfval=None, dirin="./", frac=0.5):
    ## Xtrain, Xval, Xval, yval = get_best_baseline(dftrain, dfval)
    import xam, category_encoders as ce
    from utilmy import (log, pd_read_file, pd_to_file)

    if dftrain is None :
        dftrain = pd_read_file(dirin + "/dftrain.parquet")
        dfval = pd_read_file(dirin   + "/dfval.parquet")

        dftrain = dftrain[ dftrain.ymd <= 20230830 ] ###no overlap train/test
        dftrain = dftrain.sample(frac= 0.6)
        print(dftrain.ymd.max())

    #################################################################################
    cols_drop = [ "ymd", "y", "dt", "dt-month",  ]
    cols_leak = [
        *[f"fe-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
        *[f"com-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
        *[f"qkey-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
        *[f"comcat1_hash-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
    ]
    X_train = dftrain.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
    y_train = dftrain["y"]
    X_val = dfval.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
    y_val = dfval["y"]


    ############ Encode Best #########################################################
    log("Count encoder") ### Considered as Float numerical
    cols_targetenc = ['dt-islunch', 'dt-isevening', 'dt-isweekend',
        "hh",
        "qkey",
        'qkey-zoom',
        'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1',
        "fe",
    ]
    enc = xam.feature_extraction.CountEncoder( columns=cols_targetenc, suffix='' )
    X_train = enc.fit_transform(X_train, y_train)
    X_val  = enc.transform(X_val)


    log("ordinal encoder") ### Considered as Float numerical
    cols_ordinalenc = [    "com",    "comcat1_hash" ]
    enc = ce.target_encoder.OrdinalEncoder(cols=cols_ordinalenc)
    X_train = enc.fit_transform(X_train, y_train)
    X_val   = enc.transform(X_val)


    log("###### Cyclic Features Encoding ")
    cols = ['dt-day','dt-hour', 'dt-weekday']
    trans = xam.feature_extraction.CycleTransformer()

    newcols = [f"{col}_xam1" for col in cols] + [f"{col}_xam2" for col in cols]
    X_train[newcols] = trans.fit_transform(X_train[cols])
    X_val[newcols] = trans.transform(X_val[cols])

    # Get rid of old dt columns
    X_train.drop(columns=cols, inplace=True)
    X_val.drop(columns=cols, inplace=True)



    return X_train,y_train, X_val, y_val


#### Check if it works
# X_train_best ,y_train, X_val_best, y_val = get_best_baseline(dftrain, dfval, dirin="./", frac=0.5)













"""# zahidcseku version"""



drive.mount('/content/drive')

# Attempt to read the file using Pandas
dfeval = pd.read_parquet("/content/drive/MyDrive/Datasets/dfeval.parquet")
dftrain = pd.read_parquet("/content/drive/MyDrive/Datasets/dftrain.parquet")

"""## v1

## Laod dataset
"""

! pip install utilmy python-box dirty-cat category_encoders xxhash
! pip install xam

! pip install utilmy python-box dirty-cat category_encoders xxhash

import pandas as pd
import lightgbm as lgb
import category_encoders as ce
from sklearn.metrics import f1_score

from dirty_cat import SimilarityEncoder, GapEncoder
from google.colab import drive

## Setting up the variables for processing
cols_drop = [
    "ymd", "y", "dt",
    "com-n_fe",  "dt-month",
    "comcat1_hash-n_imp",
    "comcat1_hash-n_fe",
  ]

cols_leak = [
      'com-n_clk', 'com-n_imp', 'com-n_cta',
      *[f"fe-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
      *[f"com-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
      *[f"qkey-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
  ]

cols_targetenc = [
      'dt-day',
      'dt-hour', 'dt-weekday', 'dt-islunch', 'dt-isevening', 'dt-isweekend',
      "hh",
      "qkey",
      "fe",
      'qkey-zoom',
      'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1'
  ]

### Initial values are STRING
cols_ordinalenc = [
      "com",
      "comcat1_hash"
  ]

pars_lgm = {
    "objective": "binary",
    "iterations": 10000,
    "random_seed": 42,
    "early_stopping_rounds": 300,
    "learning_rate": 0.05,
    'bagging_fraction': 0.8,
    "colsample_bytree": 0.9,
    'max_depth': 7,
  }

drive.mount('/content/drive')

# Attempt to read the file using Pandas
dfeval = pd.read_parquet("/content/drive/MyDrive/Datasets/dfeval.parquet")
dftrain = pd.read_parquet("/content/drive/MyDrive/Datasets/dftrain.parquet")


dftrain = dftrain[ dftrain.ymd <= 20230830 ] ###no overlap train/test
print(dftrain.shape, dfeval.shape)

## Dropping the cols to drop ones
X_train = dftrain.copy()
y_train = X_train['y']
X_train = X_train.drop(cols_drop, axis=1)
X_train = X_train.drop(cols_leak, axis=1)

X_eval = dfeval.copy()
y_eval = X_eval['y']
X_eval = X_eval.drop(cols_drop, axis=1)
X_eval = X_eval.drop(cols_leak, axis=1)

print(X_train.shape, X_eval.shape)

"""## Model with Target Encoding from category_encoders

## Model parameters
"""



print("Target encoder") ### Considered as Float numerical
target_enc = ce.target_encoder.TargetEncoder(cols=cols_targetenc)
X_train0 = target_enc.fit_transform(X_train, y_train)
X_eval  = target_enc.transform(X_eval)
print('Shapes after target encoding')
print(X_train.shape, X_eval.shape)


print("Ordinal encoder") ### Considered as Float numerical
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cols_ordinalenc)
X_train = ordinal_encoder.fit_transform(X_train, y_train)
X_eval   = ordinal_encoder.transform(X_eval)
print('Shapes after target encoding')
print(X_train.shape, X_eval.shape)

"""### Train model with target encoding"""

print('Starting training using target encoding ...')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(X_train, y_train, eval_set=[(X_eval, y_eval)])

print('Starting predicting...')
y_pred   = clf.predict(X_eval, num_iteration=clf.best_iteration_)
score_bl = f1_score(y_eval, y_pred)

print(f'F1 score val - baseline: {score_bl:.4f}')

dic_resutls = dict(target_encoding=score_bl)

dic_resutls

"""## Similarty encoding from dirty_cat

### Encoding the cat features
"""

ncats = 10
##Similarity encoding
print("Similarity encoder")
senc = SimilarityEncoder(categories="most_frequent", n_prototypes=ncats)
senc.set_output(transform="pandas")

enc_cols_train = senc.fit_transform(X_train[cols_targetenc])
enc_cols_eval  = senc.transform(X_eval[cols_targetenc])

# Transform the features, rename columns with _cb suffix, and join to dataframe
train_SE = X_train.join(enc_cols_train, rsuffix=f'_se')
valid_SE = X_eval.join(enc_cols_eval, rsuffix=f'_se')

## all feat shape after encoding
train_SE.shape, valid_SE.shape

# Remove the original cols
train_SE = train_SE.drop(cols_targetenc, axis=1)
valid_SE = valid_SE.drop(cols_targetenc, axis=1)

train_SE.shape, valid_SE.shape

"""### Apply ordinal encoder"""

print("Ordinal encoder") ### Considered as Float numerical
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cols_ordinalenc)
train_SE = ordinal_encoder.fit_transform(train_SE, y_train)
valid_SE = ordinal_encoder.transform(valid_SE)

print('Shapes after target encoding')
print(train_SE.shape, valid_SE.shape)

"""### Train model with similarty encoder"""

print('Starting training with similarty encoder...')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(train_SE, y_train, eval_set=[(valid_SE, y_eval)])

print('Starting predicting...')
y_pred = clf.predict(valid_SE, num_iteration=clf.best_iteration_)
score_se     = f1_score(y_eval, y_pred)

print(f'F1 score val - similarity encoding: {score_se:.4f}')
dic_resutls["similarty_enc_ptype10"] = score_se

dic_resutls

"""### Similarty encoding with different ncats=15

- crashes with ncats = 20 so limiting to 15
"""

dic_resutls = {'target_encoding': 0.6789906023164266,
 'similarty_enc_ptype10': 0.6991531047092883}

datadic = init_setup()

X_train = datadic["X_train"]
y_train = datadic["y_train"]
X_eval = datadic["X_eval"]
y_eval = datadic["y_eval"]

X_train.shape, X_eval.shape

cols_targetenc = datadic["cols_targetenc"]
#enc_cols_train = datadic["enc_cols_train"]

datadic.keys()

ncats = 15
##Similarity encoding
print("Similarity encoder")
senc = SimilarityEncoder(categories="most_frequent", n_prototypes=ncats)
senc.set_output(transform="pandas")

enc_cols_train = senc.fit_transform(X_train[cols_targetenc])
enc_cols_eval  = senc.transform(X_eval[cols_targetenc])

# Transform the features, rename columns with _cb suffix, and join to dataframe
train_SE = X_train.join(enc_cols_train, rsuffix=f'_se')
valid_SE = X_eval.join(enc_cols_eval, rsuffix=f'_se')

## all feat shape after encoding
train_SE.shape, valid_SE.shape

# Remove the original cols
train_SE = train_SE.drop(cols_targetenc, axis=1)
valid_SE = valid_SE.drop(cols_targetenc, axis=1)

train_SE.shape, valid_SE.shape

cols_ordinalenc = datadic["cols_ordinalenc"]

print("Ordinal encoder") ### Considered as Float numerical
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cols_ordinalenc)
train_SE = ordinal_encoder.fit_transform(train_SE, y_train)
valid_SE = ordinal_encoder.transform(valid_SE)

print('Shapes after target encoding')
print(train_SE.shape, valid_SE.shape)

print('Starting training with similarty encoder...')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(train_SE, y_train, eval_set=[(valid_SE, y_eval)])

print('Starting predicting...')
y_pred = clf.predict(valid_SE, num_iteration=clf.best_iteration_)
score_se     = f1_score(y_eval, y_pred)

print(f'F1 score val - similarity encoding: {score_se:.4f}')
dic_resutls["similarty_enc_ptype15"] = score_se

dic_resutls

"""## Using CatBoost Encoder from CE"""

# Create the encoder
cb_enc = ce.CatBoostEncoder(cols=cols_targetenc)
cb_enc.fit(X_train[cols_targetenc], y_train)

# Transform the features, rename columns with _cb suffix, and join to dataframe
train_CBE = X_train.join(cb_enc.transform(X_train[cols_targetenc]).add_suffix('_cb'))
valid_CBE = X_eval.join(cb_enc.transform(X_eval[cols_targetenc]).add_suffix('_cb'))

train_CBE.shape, valid_CBE.shape

# Remove the encoded cols
train_CBE = train_CBE.drop(cols_targetenc, axis=1)
valid_CBE = valid_CBE.drop(cols_targetenc, axis=1)

train_CBE.shape, valid_CBE.shape

print("Ordinal encoder") ### Considered as Float numerical
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cols_ordinalenc)
train_CBE = ordinal_encoder.fit_transform(train_CBE, y_train)
valid_CBE = ordinal_encoder.transform(valid_CBE)

print('Shapes after target encoding')
print(train_CBE.shape, valid_CBE.shape)

print('Starting training...')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(train_CBE, y_train, eval_set=[(valid_CBE, y_eval)])


print('Starting predicting...')
# predict
y_pred = clf.predict(valid_CBE, num_iteration=clf.best_iteration_)
score_cb = f1_score(y_eval, y_pred)
print(f'F1 score val - catboost encoding: {score_cb}')

dic_resutls["score_cb"] = score_cb

dic_resutls

"""## Count encoder from CE"""

# Create the encoder
count_enc = ce.CountEncoder(cols=cols_targetenc)
count_enc.fit(X_train[cols_targetenc], y_train)

# Transform the features, rename columns with _cb suffix, and join to dataframe
train_CE = X_train.join(count_enc.transform(X_train[cols_targetenc]).add_suffix('_count'))
valid_CE = X_eval.join(count_enc.transform(X_eval[cols_targetenc]).add_suffix('_count'))

train_CE.shape, valid_CE.shape

# Remove the unencoded cols
train_CE = train_CE.drop(cols_targetenc, axis=1)
valid_CE = valid_CE.drop(cols_targetenc, axis=1)

print("Ordinal encoder") ### Considered as Float numerical
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cols_ordinalenc)
train_CE = ordinal_encoder.fit_transform(train_CE, y_train)
valid_CE   = ordinal_encoder.transform(valid_CE)

print('Shapes after ordinal encoding')
print(train_CE.shape, valid_CE.shape)

print('Starting training...')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(train_CE, y_train, eval_set=[(valid_CE, y_eval)])


print('Starting predicting...')
# predict
y_pred = clf.predict(valid_CE, num_iteration=clf.best_iteration_)
score_ce     = f1_score(y_eval, y_pred)
print(f'F1 score val - count encoding: {score_ce: .4f}')

dic_resutls["score_count_enc"] = score_ce

dic_resutls

import seaborn as sns

keys = list(dic_resutls.keys())
vals = list(dic_resutls.values())

ax = sns.barplot(x=vals, y=keys)

ax.set_yticks(range(len(keys)),
              labels=['Target encoding', 'Similart-10 encoding', 'Similart-15 encoding',
                      'Catboost encoding', 'Count encoding']);

"""## HashingEncoder encoder CE"""



dic_results = {'target_encoding': 0.6789906023164266,
 'similarty_enc_ptype10': 0.6991531047092883,
 'similarty_enc_ptype20': 0.6845111655437874,
 'score_cb': 0.6482160637238633,
 'score_count_enc': 0.7303069954705587}



print("HashingEncoder encoder")
# Create the encoder
hash_enc = ce.HashingEncoder(cols=cols_targetenc)
hash_enc.fit(X_train[cols_targetenc], y_train)

# Transform the features, rename columns with _cb suffix, and join to dataframe
train_HE = X_train.join(hash_enc.transform(X_train[cols_targetenc]).add_suffix('_he'))
valid_HE = X_eval.join(hash_enc.transform(X_eval[cols_targetenc]).add_suffix('_he'))

train_HE.shape, valid_HE.shape

print("Ordinal encoder") ### Considered as Float numerical
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cols_ordinalenc)
train_HE = ordinal_encoder.fit_transform(train_HE, y_train)
valid_HE   = ordinal_encoder.transform(valid_HE)

print('Shapes after ordinal encoding')
print(train_HE.shape, valid_HE.shape)

# Remove the unencoded cols
train_HE = train_HE.drop(cols_targetenc, axis=1)
valid_HE = valid_HE.drop(cols_targetenc, axis=1)

print('Starting training...')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(train_HE, y_train, eval_set=[(valid_HE, y_eval)])


print('Starting predicting...')
# predict
y_pred = clf.predict(valid_HE, num_iteration=clf.best_iteration_)
score_he = f1_score(y_eval, y_pred)
print(f'F1 score val - hash encoding: {score_he: .4f}')

dic_results["hash_enc"] = score_he

dic_results

"""## MEstimateEncoder from CE"""

print("MEstimate encoder")
# Create the encoder
mest_enc = ce.MEstimateEncoder(cols=cols_targetenc)
mest_enc.fit(X_train[cols_targetenc], y_train)

# Transform the features, rename columns with _cb suffix, and join to dataframe
train_ME = X_train.join(mest_enc.transform(X_train[cols_targetenc]).add_suffix('_me'))
valid_ME = X_eval.join(mest_enc.transform(X_eval[cols_targetenc]).add_suffix('_me'))

train_ME.shape, valid_ME.shape

print("Ordinal encoder") ### Considered as Float numerical
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cols_ordinalenc)
train_ME = ordinal_encoder.fit_transform(train_ME, y_train)
valid_ME   = ordinal_encoder.transform(valid_ME)

print('Shapes after ordinal encoding')
print(train_ME.shape, valid_ME.shape)

# Remove the unencoded cols
train_ME = train_ME.drop(cols_targetenc, axis=1)
valid_ME = valid_ME.drop(cols_targetenc, axis=1)

print('Starting training...')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(train_ME, y_train, eval_set=[(valid_ME, y_eval)])

print('Starting predicting...')
# predict
y_pred = clf.predict(valid_ME, num_iteration=clf.best_iteration_)
score_me = f1_score(y_eval, y_pred)
print(f'F1 score val - mest encoding: {score_me: .4f}')

dic_results["mest_enc"] = score_me

dic_results

"""## XAM encodings"""

dic_results = {'target_encoding': 0.6789906023164266,
 'similarty_enc_ptype10': 0.6991531047092883,
 'similarty_enc_ptype20': 0.6845111655437874,
 'score_cb': 0.6482160637238633,
 'score_count_enc': 0.7303069954705587,
 'hash_enc': 0.6614350547499988,
 'mest_enc':  0.6478
               }

cols_cyclic =[ 'dt-day', 'dt-hour', 'dt-weekday']

cols_targetenc_xam = ['dt-islunch', 'dt-isevening', 'dt-isweekend', 'hh', 'qkey', 'fe', 'qkey-zoom',
                      'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1']

!pip install git+https://github.com/MaxHalford/xam --upgrade

"""### cyclic encoding of the temporal features"""

import xam

print("Cyclc encoder")
# Create the encoder
cyenc = xam.feature_extraction.CycleTransformer()
cyenc.fit(X_train[cols_cyclic])

# Transform the features, rename columns with _cb suffix, and join to dataframe
train_enc_arr = cyenc.transform(X_train[cols_cyclic])
valid_enc_arr = cyenc.transform(X_eval[cols_cyclic])

train_enc_arr.shape, valid_enc_arr.shape

colnames = []

for l in cols_cyclic:
  colnames.append(f'{l}_sin')
  colnames.append(f'{l}_cos')
colnames

"""### Convert np array to dataframe"""

train_dfenc = pd.DataFrame(train_enc_arr, columns = colnames)
valid_dfenc = pd.DataFrame(valid_enc_arr, columns = colnames)

X_train.shape, X_eval.shape

train_CYE = X_train.join(train_dfenc, rsuffix = '_cye')
valid_CYE = X_eval.join(valid_dfenc, rsuffix = '_cye')
train_CYE.shape, valid_CYE.shape

"""### Count encoding of the other features"""

# Create the encoder
count_enc = ce.CountEncoder(cols=cols_targetenc_xam)
count_enc.fit(X_train[cols_targetenc_xam], y_train)

# Transform the features, rename columns with _cb suffix, and join to dataframe
train_CYE = train_CYE.join(count_enc.transform(train_CYE[cols_targetenc_xam]).add_suffix('_count'))
valid_CYE = valid_CYE.join(count_enc.transform(valid_CYE[cols_targetenc_xam]).add_suffix('_count'))

train_CYE.shape, valid_CYE.shape

print("Ordinal encoder") ### Considered as Float numerical
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cols_ordinalenc)
train_CYE = ordinal_encoder.fit_transform(train_CYE, y_train)
valid_CYE   = ordinal_encoder.transform(valid_CYE)

print('Shapes after ordinal encoding')
print(train_CYE.shape, valid_CYE.shape)

"""### Remove the original features"""

# Remove the unencoded cols
train_CYE = train_CYE.drop(cols_targetenc_xam, axis=1)
valid_CYE = valid_CYE.drop(cols_targetenc_xam, axis=1)

train_CYE = train_CYE.drop(cols_cyclic, axis=1)
valid_CYE = valid_CYE.drop(cols_cyclic, axis=1)

print(train_CYE.shape, valid_CYE.shape)

print('Starting training...')
clf = lgb.LGBMClassifier(**pa)
clf.fit(train_CYE, y_train, eval_set=[(valid_CYE, y_eval)])

print('Starting predicting...')
# predict
y_pred = clf.predict(valid_CYE, num_iteration=clf.best_iteration_)
score_cyc = f1_score(y_eval, y_pred)
print(f'F1 score val - cyclic encoding: {score_cyc: .4f}')

dic_results["cyclic_enc"] = score_cyc

cols_drop

cols_leak

cols_targetenc

cols_ordinalenc

cols_targetenc_xam

"""## Experiment with UMAP


"""

umap_cat_cols = ['dt-day',
 'dt-hour',
 'dt-weekday',
 'dt-islunch',
 'dt-isevening',
 'dt-isweekend',
 'hh',
 'qkey-zoom',
 'qkey-qhash',
 'qkey-a',
 'qkey-b',
 'qkey-c',
 'qkey-1']

X_train[cols_targetenc].head(3)

cols_targetenc_umap = list(set(cols_targetenc)-set(umap_cat_cols))

"""### Normalize the UMAP features"""

from sklearn.preprocessing import RobustScaler

transformer = RobustScaler().fit(X_train[umap_cat_cols])
transX_train = transformer.transform(X_train[umap_cat_cols])
transX_eval = transformer.transform(X_eval[umap_cat_cols])

"""### Learn UMAP embedding"""

! pip install umap-learn

!git clone https://github.com/rapidsai/rapidsai-csp-utils.git
!python rapidsai-csp-utils/colab/env-check.py

!bash rapidsai-csp-utils/colab/update_gcc.sh
import os
os._exit(00)

import condacolab
condacolab.install()

# Installing RAPIDS is now 'python rapidsai-csp-utils/colab/install_rapids.py <release> <packages>'
# The <release> options are 'stable' and 'nightly'.  Leaving it blank or adding any other words will default to stable.
!python rapidsai-csp-utils/colab/install_rapids.py stable
import os
os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'
os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'
os.environ['CONDA_PREFIX'] = '/usr/local'

import cuml
from cupy import asnumpy
from joblib import dump, load

# Commented out IPython magic to ensure Python compatibility.
# %time

import umap
import numpy as np

#samplesize = (int(transX_train.shape[0]*.3), transX_train.shape[1])
idx = np.random.randint(transX_train.shape[0], size=int(transX_train.shape[0]*.1))
sample_umap_trainX = transX_train[idx, :]

trans_umap = umap.UMAP(n_neighbors=5, random_state=42).fit(sample_umap_trainX)

umap_trainX = trans_umap.transform(X_train[umap_cat_cols])
umap_evalX = trans_umap.transform(X_eval[umap_cat_cols])

np.savez("umap_features", trainX=umap_trainX, evalX=umap_evalX)

umap_trainX.shape, umap_evalX.shape

print('Starting training...')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(umap_trainX, y_train, eval_set=[(umap_evalX, y_eval)])

print('Starting predicting...')
# predict
y_pred = clf.predict(umap_evalX, num_iteration=clf.best_iteration_)
score_umap = f1_score(y_eval, y_pred)
print(f'F1 score val - UMAP encoding: {score_umap: .4f}')
score_umap

"""## UMAP Final"""

!pip install xxhash

import xxhash

def hash_int32(xstr:str):
  return xxhash.xxh32_intdigest(str(xstr),seed=0)

! pip install git+https://github.com/MaxHalford/xam

#from utilmy import log
import xam
import category_encoders as ce

#pd_read_file, pd_to_file, date_now, os_makedirs,
#     glob_glob, log2, log_pd)

def get_best_baseline(dftrain=None, dfval=None, dirin="./", frac=0.5):
   ## Xtrain, Xval, Xval, yval = get_best_baseline(dftrain, dfval)
   if dftrain is None:
      dftrain = pd_read_file(dirin + "/dftrain.parquet")
      dfval = pd_read_file(dirin   + "/dfval.parquet")

   #################################################################################
   cols_drop = [ "ymd", "y", "dt", "dt-month",  ]
   cols_leak = [
       *[f"fe-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
       *[f"com-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
       *[f"qkey-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
       *[f"comcat1_hash-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
   ]
   X_train = dftrain.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
   y_train = dftrain["y"]
   X_val = dfval.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
   y_val = dfval["y"]



   ############ Encode Best #########################################################
   print("Count encoder") ### Considered as Float numerical
   cols_targetenc = ['dt-islunch', 'dt-isevening', 'dt-isweekend',
       "hh",
       "qkey",
       'qkey-zoom',
       'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1',
       "fe",
   ]
   enc = xam.feature_extraction.CountEncoder( columns=cols_targetenc, suffix='' )
   X_train = enc.fit_transform(X_train, y_train)
   X_val  = enc.transform(X_val)




   print("ordinal encoder") ### Considered as Float numerical
   cols_ordinalenc = [    "com",    "comcat1_hash" ]
   enc = ce.target_encoder.OrdinalEncoder(cols=cols_ordinalenc)
   X_train = enc.fit_transform(X_train, y_train)
   X_val   = enc.transform(X_val)




   print("###### Cyclic Features Encoding ")
   cols = ['dt-day','dt-hour', 'dt-weekday']
   trans = xam.feature_extraction.CycleTransformer()


   newcols = [f"{col}_xam1" for col in cols] + [f"{col}_xam2" for col in cols]
   X_train[newcols] = trans.fit_transform(X_train[cols])
   X_val[newcols] = trans.transform(X_val[cols])


   # Get rid of old dt columns
   X_train.drop(columns=cols, inplace=True)
   X_val.drop(columns=cols, inplace=True)




   return X_train, y_train, X_val, y_val



X_train_best, y_train, X_val_best, y_val = get_best_baseline(dftrain, dfeval)

X_train_best.shape, X_val_best.shape

"""## Now need to add UMAP cols"""

import numpy as np

def pd_colcat_onehot_to_csr(df:pd.DataFrame,
                            cols:list=None,
                            cols_onehotdim:list=None,
                            dtype=np.float32
                            ):
   """ Creates One Hot Sparse Matrix for category data


        Int col --> [0 0 0 1] in Sparse form
        Str col --> [0 0 0 1] in Sparse form
        float --> float (no change)


   """
   import mmh3
   from scipy.sparse import coo_matrix, csr_matrix, lil_matrix
   #from utilmy import hash_int32



   ##### Get bucket column size for 1 hot #######################
   colsize =[]
   for ii, ci in enumerate(cols):
       di = str(df[ci].dtypes)
       if 'float' in di :
           colsize.append(1)
       else:
           if cols_onehotdim is None :
               ni = int(np.sqrt(df[ci].nunique()))
           else:
               ni = cols_onehotdim[ii]
           colsize.append(ni)


   Xcols = sum(colsize)
   Xrows = len(df)     # No. rows for sparse matrix X
   X     = lil_matrix((Xrows, Xcols), dtype=dtype)     # Create zeros sparse matrix
   log('bucket size', colsize )


   ###########################################################
   bucket = 0 ; ntot=0
   for j, colj in enumerate(cols):
       nj = colsize[j]
       vj = df[colj].values
       dtypej = str(df[colj].dtypes)
       for idx in range(len(df)):
           if 'float' in dtypej:
               X[ (idx, bucket )] = vj[idx]  ### Just Float values
           else:
               colid = hash_int32(str(vj[idx ])) % nj
               X[ (idx, bucket + colid)] = 1

       bucket += nj


   X = csr_matrix( X )
   print('Sparse matrix shape:', X.shape)
   print('Expected no. of Ones: ', ntot )
   print('No. of Ones in the Matrix: ', X.count_nonzero() )
   return X, colsize

import umap

def get_umap_encoder(Xtrain, ytrain, frac, ndim_out=2):
   #cols  = ['com',  'comcat1_hash', 'fe',  'qkey-1', 'qkey-a', 'qkey-b', 'qkey-c',  'fe-ctr_30D',   'y' ]
   colsX = ['com', 'comcat1_hash', 'fe',  'qkey-1', 'qkey-a', 'qkey-b', 'qkey-c',  'fe-ctr_30D']
   Xtrain['y'] = ytrain
   df = Xtrain.sample(frac=frac)

   csr_X, ohe_colsize = pd_colcat_onehot_to_csr(df, cols=colsX, dtype=np.float32, cols_onehotdim=None)

   print(f'One-hot-encoding size: {csr_X.shape}')

   #do the umap encoding
   # conver the df to np arrays
   Xtrain = csr_X.values if isinstance(csr_X, pd.DataFrame) else csr_X
   ytrain = df['y'].values if isinstance(df['y'], pd.DataFrame) else df['y']

   umap_encoder = umap.UMAP(n_components=ndim_out,
                       n_neighbors=5,
                       metric='cosine',
                       random_state=42,
                       low_memory=True
                       )
   print(Xtrain.shape, ytrain.shape)
   umap_encoder.fit(Xtrain, y=ytrain)

   return umap_encoder, ohe_colsize

umap_enc, ohe_colsize  = get_umap_encoder(X_train_best, y_train, ndim_out=2, frac=0.3)

colsX = ['com', 'comcat1_hash', 'fe',  'qkey-1', 'qkey-a', 'qkey-b', 'qkey-c',  'fe-ctr_30D']

csr_Xtrain_best, _ = pd_colcat_onehot_to_csr(X_train_best,
                                          cols=colsX,
                                          dtype=np.float32,
                                          cols_onehotdim=ohe_colsize
                                          )
print(csr_Xtrain_best.shape)
csr_Xtrain_best = csr_Xtrain_best.values if isinstance(csr_Xtrain_best, pd.DataFrame) else csr_Xtrain_best
Xtrain_umap = umap_enc.transform(csr_Xtrain_best)

csr_Xeval_best, _ = pd_colcat_onehot_to_csr(X_val_best,
                                         cols=colsX,
                                         dtype=np.float32,
                                         cols_onehotdim=ohe_colsize
                                         )
csr_Xeval_best = csr_Xeval_best.values if isinstance(csr_Xeval_best, pd.DataFrame) else csr_Xeval_best
Xeval_umap = umap_enc.transform(csr_Xeval_best)

Xtrain_umap.shape, Xeval_umap.shape

"""## Add UMAP with best features"""

best_umap_Xtrain = pd.concat([Xtrain_best, Xtrain_umap], axis=1)
best_umap_Xeval = pd.concat([X_val_best, Xeval_umap], axis=1)

best_umap_Xtrain.shape, best_umap_Xeval.shape

print('Training....')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(best_umap_Xtrain, y_train, eval_set=[(best_umap_Xeval, y_eval)])

print('Starting predicting...')
# predict
y_pred = clf.predict(best_umap_Xeval, num_iteration=clf.best_iteration_)
score_umap = f1_score(y_eval, y_pred)
print(f'F1 score val - mest encoding: {score_umap: .4f}')

score_umap

def feat_add_cat_embedding(df, colsX, colsy, colpref="c1", mode='umap', ndim_out=2, encoder=None,
                         cols_onehotdim=None):
   """


   """
   Xsp = pd_colcat_onehot_to_csr(df,
                                 cols=colsX,
                                 dtype=np.float32,
                                 cols_onehotdim=cols_onehotdim
                                 )


   if encoder is None:
       encoder, Xemb = model_dim_reduction(Xtrain=Xsp, ytrain=df[colsy], mode=mode, ndim_out=ndim_out,
                                           n_neighbors=5, verbose=True)
   else:
       Xemb = encoder.transform(Xsp)


   colsnew = [ f"{colpref}-{i}" for i in  range(Xemb.shape[1]) ]
   Xemb    = pd.DataFrame(Xemb, columns= colsnew)
   #Xemb.indices = Xsp.indices
   df = pd.concat([pd.DataFrame(Xsp.toarray()), Xemb], axis=1)


   return encoder, df






def model_dim_reduction(Xtrain=None, ytrain=None, mode='pacmap', ndim_out=2, n_neighbors=10, show=1,
                       returnval=1,dirout=None, model=None,   **kw):
   """
   model_dim_reduction(Xtrain=None, ytrain=None, mode='pacmap',
                       ndim_out=2, n_neighbors=10, show=1,
                       returnval=1,dirout=None, )
   Hellinger distance which measures the similarity between two probability distributions.
   Each document has a set of counts generated by a multinomial distribution
   where we can use Hellinger distance to measure the similarity of these distributions.


   umap
           https://umap-learn.readthedocs.io/en/latest/sparse.html
               n_neighbors=15,
               n_components=2,
               metric="euclidean",
               metric_kwds=None,
               output_metric="euclidean",
               output_metric_kwds=None,
               n_epochs=None,
               learning_rate=1.0,
               init="spectral",
               min_dist=0.1,
               spread=1.0,
               low_memory=True,
               n_jobs=-1,
               set_op_mix_ratio=1.0,
               local_connectivity=1.0,
               repulsion_strength=1.0,
               negative_sample_rate=5,
               transform_queue_size=4.0,
               random_state=None,
               angular_rp_forest=False,
               target_n_neighbors=-1,
               target_metric="categorical",
               target_metric_kwds=None,
               target_weight=0.5,
               transform_mode="embedding",
               force_approximation_algorithm=False,
               verbose=False,
               tqdm_kwds=None,
               unique=False,
               densmap=False,
               dens_lambda=2.0,
               dens_frac=0.3,
               dens_var_shift=0.1,
               output_dens=False,
               disconnection_distance=None,
               precomputed_knn=(None, None, None),


   PACMAP
       https://towardsdatascience.com/why-you-should-not-rely-on-t-sne-umap-or-trimap-f8f5dc333e59
       https://pypi.org/project/pacmap/
       n_components:  number of dimension of  output. Default to 2.
       n_neighbors:  number of neighbors considered in  k-Nearest Neighbor graph. Default to 10 for dataset whose sample size is smaller than 10000. For large dataset whose sample size (n) is larger than 10000,  default value is: 10 + 15 * (log10(n) - 4).
       MN_ratio:  ratio of  number of mid-near pairs to  number of neighbors, n_MN =  n_neighbors * MN_ratio  . Default to 0.5.
       FP_ratio:  ratio of  number of furr pairs to  number of neighbors, n_FP =  n_neighbors * FP_ratio  Default to 2.




       if show>0:
          from utilmy import pd_plot_multi( )
          # visualize  embedding
          import matplotlib.pyplot as plt
          fig, ax = plt.subplots(1, 1, figsize=(6, 6))
          ax.scatter(Xnew[:, 0], Xnew[:, 1], cmap="Spectral", c=y, s=0.6)




   """
   if Xtrain is None :
       from sklearn.datasets import load_breast_cancer
       data = load_breast_cancer(as_frame=False)
       Xtrain, ytrain =data['data'], data['target']


   Xtrain = Xtrain.values if isinstance(Xtrain, pd.DataFrame) else Xtrain
   ytrain = ytrain.values if isinstance(ytrain, pd.DataFrame) else ytrain

   reducer, Xnew = None, None
   if mode == 'pacmap':
       import pacmap
       MN_ratio = kw.get('MN_ratio', 0.5  )
       FP_ratio = kw.get('FP_ratio', 2.0  )
       reducer  = pacmap.PaCMAP(n_components=ndim_out, n_neighbors=n_neighbors,
                                MN_ratio=MN_ratio, FP_ratio=FP_ratio)
       Xnew   = reducer.fit_transform(Xtrain, init="pca")


   if mode == 'umap':
       import umap
       metric  = kw.pop('metric', 'cosine')
       reducer = umap.UMAP(n_components=ndim_out, n_neighbors= n_neighbors,
                           metric=metric, random_state=42, low_memory=True, **kw)
       Xnew = reducer.fit_transform(Xtrain, y=ytrain )


   if show>0:
       log(Xnew)


   if dirout is not None:
       from utilmy import save
       os_makedirs(dirout)
       save(reducer, dirout  + f"/reducer_{mode}.pkl", verbose=True)


   return reducer, Xnew

umap_enc, dfumap = test1_embed(X_train_best, y_train, ndim_out=2, frac=0.3)

dfumap.shape

XtrainBest_sparse = get_sparse(XtrainBest[cols_umap])

Xtrain_umap = umap.transform(XtrainBest_sparse)
Xval_umap = umap.transform(XtrainBest_sparse)


Xtrain2 = pd.concat(( XtrainBest, Xtrain_umap)









!pip install mmh3

!pip install umap-learn

"""## UMAP cuml"""

!nvidia-smi

!rm -r /content/rapidsai-csp-utils

!git clone https://github.com/rapidsai/rapidsai-csp-utils.git
!python rapidsai-csp-utils/colab/env-check.py

!bash rapidsai-csp-utils/colab/update_gcc.sh -qq

import os
os._exit(00)

import condacolab

condacolab.install()

!python rapidsai-csp-utils/colab/install_rapids.py stable

import os

os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'
os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'
os.environ['CONDA_PREFIX'] = '/usr/local'

import cudf, cuml

print('cuml version:', cuml.__version__, ', cudf version:', cudf.__version__)

dftrain.shape, dfeval.shape

umap_col = ['dt-day', 'dt-hour', 'dt-weekday', 'dt-islunch',
            'dt-isevening', 'dt-isweekend', 'hh', 'qkey-zoom', 'qkey-qhash',
            'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1']

## Encode the cols
import category_encoders as ce

count_enc = ce.CountEncoder(cols=umap_col)
count_enc.fit(dftrain[umap_col], dftrain['y'])

Xtrain_umap = count_enc.transform(dftrain[umap_col])
Xeval_umap = count_enc.transform(dfeval[umap_col])

print(len(umap_col))
Xtrain_umap.shape, Xeval_umap.shape

import cuml
from cupy import asnumpy
from cuml.manifold.umap import UMAP as cuUMAP

#trained_UMAP = cuUMAP(n_neighbors=5, n_components=2).fit(Xtrain_umap, dftrain['y'])

trained_UMAP_ce7 = cuUMAP(n_neighbors=5, n_components=7).fit(Xtrain_umap, dftrain['y'])

Xtrain_umap_ce_com7 = trained_UMAP_ce7.transform(Xtrain_umap)
Xeval_umap_ce_com7 = trained_UMAP_ce7.transform(Xeval_umap)

Xtrain_umap_ce_com7.shape, Xeval_umap_ce_com7.shape

# Save
import numpy as np

np.savez("umap_feat_ce_com7", trainX=Xtrain_umap_ce_com7, trainy=dftrain['y'],
         validX=Xeval_umap_ce_com7, validy=dfeval['y'])

"""## Merge best features and umap features"""

X_train_best.shape, X_val_best.shape

import numpy as np
umap_feat = np.load("/content/umap_feat_te_com7.npz")

umap_feat.files

umap_trainX = umap_feat_ce['trainX']
umap_validX = umap_feat_ce['validX']

umap_trainX.shape, umap_validX.shape

type(umap_cec2_trainX)

merged_trainX = np.hstack([X_train_best.values, umap_trainX])
merged_validX = np.hstack([X_val_best.values, umap_validX])

merged_trainX.shape, merged_validX.shape

y_train.shape, y_val.shape

#save
np.savez("merged_TE_comp7",
         trainX=merged_trainX,
         validX=merged_validX
         )

import lightgbm as lgb

print('Starting training...')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(merged_trainX, y_train, eval_set=[(merged_validX, y_val)])

print('Starting predicting...')
# predict
y_pred = clf.predict(merged_validX, num_iteration=clf.best_iteration_)
score_me = f1_score(y_val, y_pred)
print(f'F1 score val - Umap TE comp7: {score_me: .4f}')
score_me

!cp /content/umap_feat_ce_com2.npz /content/drive/MyDrive/data3-final/data3-final/

!cp /content/umap_feat_ce_com7.npz /content/drive/MyDrive/data3-final/data3-final/

!cp /content/umap_feat_te_com2.npz /content/drive/MyDrive/data3-final/data3-final/

!cp /content/umap_feat_te_com2.npz /content/drive/MyDrive/data3-final/data3-final/

!cp /content/merged_TE_comp2.npz /content/drive/MyDrive/data3-final/data3-final/

!cp /content/merged_TE_comp7.npz /content/drive/MyDrive/data3-final/data3-final/

"""## OpenFE"""

!pip install openfe

"""### Load the dataset"""

import pandas as pd

#  Check if the foldre is accessible file using Pandas
dftrain = pd.read_parquet("/content/drive/MyDrive/data3-final/data3-final/dftrain.parquet")
dfeval = pd.read_parquet("/content/drive/MyDrive/data3-final/data3-final/dfeval.parquet")

dftrain.shape, dfeval.shape

dftrain = dftrain[ dftrain.ymd <= 20230830 ]
dftrain.shape, dfeval.shape

"""### Load the best features"""

#from utilmy import log
import xam
import category_encoders as ce

#pd_read_file, pd_to_file, date_now, os_makedirs,
#     glob_glob, log2, log_pd)

def get_best_baseline(dftrain=None, dfval=None, dirin="./", frac=0.5):
   ## Xtrain, Xval, Xval, yval = get_best_baseline(dftrain, dfval)
   if dftrain is None:
      dftrain = pd_read_file(dirin + "/dftrain.parquet")
      dfval = pd_read_file(dirin   + "/dfval.parquet")

   #################################################################################
   cols_drop = [ "ymd", "y", "dt", "dt-month",  ]
   cols_leak = [
       *[f"fe-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
       *[f"com-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
       *[f"qkey-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
       *[f"comcat1_hash-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
   ]
   X_train = dftrain.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
   y_train = dftrain["y"]
   X_val = dfval.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
   y_val = dfval["y"]

   ############ Encode Best #########################################################
   print("Count encoder") ### Considered as Float numerical
   cols_targetenc = ['dt-islunch', 'dt-isevening', 'dt-isweekend',
       "hh",
       "qkey",
       'qkey-zoom',
       'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1',
       "fe",
   ]
   enc = xam.feature_extraction.CountEncoder( columns=cols_targetenc, suffix='' )
   X_train = enc.fit_transform(X_train, y_train)
   X_val  = enc.transform(X_val)

   print("ordinal encoder") ### Considered as Float numerical
   cols_ordinalenc = [    "com",    "comcat1_hash" ]
   enc = ce.target_encoder.OrdinalEncoder(cols=cols_ordinalenc)
   X_train = enc.fit_transform(X_train, y_train)
   X_val   = enc.transform(X_val)

   print("###### Cyclic Features Encoding ")
   cols = ['dt-day','dt-hour', 'dt-weekday']
   trans = xam.feature_extraction.CycleTransformer()

   newcols = [f"{col}_xam1" for col in cols] + [f"{col}_xam2" for col in cols]
   X_train[newcols] = trans.fit_transform(X_train[cols])
   X_val[newcols] = trans.transform(X_val[cols])

   # Get rid of old dt columns
   X_train.drop(columns=cols, inplace=True)
   X_val.drop(columns=cols, inplace=True)

   return X_train, y_train, X_val, y_val

X_train_best, y_train, X_val_best, y_val = get_best_baseline(dftrain, dfeval, dirin="./", frac=1.0)

X_train_best.shape, X_val_best.shape
#type(X_train_best)

"""### Get openfe transformations"""

from openfe import OpenFE, transform

ofe = OpenFE()
size = 100000

features = ofe.fit(data=X_train_best.loc[:size, :], label=y_train[:size], n_jobs=64)  # generate new features

train_x, eval_x = transform(X_train_best, X_val_best, features, n_jobs=64)

import lightgbm as lgb

print('Starting training...')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(train_x, y_train, eval_set=[(eval_x, y_val)])


print('Starting predicting...')
# predict
y_pred = clf.predict(eval_x, num_iteration=clf.best_iteration_)
score_ofe = f1_score(y_eval, y_pred)
print(f'F1 score val - cyclic encoding: {score_ofe: .4f}')
score_ofe

train_x, eval_x = transform(X_train_best, X_val_best, features, n_jobs=16)

train_x.shape, eval_x.shape

import lightgbm as lgb

print('Starting training...')
clf = lgb.LGBMClassifier(**pars_lgm)
clf.fit(train_x, y_train, eval_set=[(eval_x, y_val)])

print('Starting predicting...')
# predict
y_pred = clf.predict(eval_x, num_iteration=clf.best_iteration_)
score_ofe = f1_score(y_eval, y_pred)
print(f'F1 score val - cyclic encoding: {score_ofe: .4f}')
score_ofe

Xtrain_umap_te_com7 = trained_UMAP_te7.transform(Xtrain_umap)
Xeval_umap_te_com7 = trained_UMAP_te7.transform(Xeval_umap)

Xtrain_umap_te_com7.shape, Xeval_umap_te_com7.shape

# Save
np.savez("umap_feat_te_com7",
         trainX=Xtrain_umap_te_com7, trainy=dftrain['y'],
         validX=Xeval_umap_te_com7, validy=dfeval['y']
         )

## Get the best features



X_embedded = trained_UMAP.transform( dftrain[umap_cols] )

cu_score = cuml.metrics.trustworthiness( X_blobs, X_embedded )
sk_score = trustworthiness( asnumpy( X_blobs ),  asnumpy( X_embedded ) )

print(" cuml's trustworthiness score : ", cu_score )
print(" sklearn's trustworthiness score : ", sk_score )

# save
dump( trained_UMAP, 'UMAP.model')

# to reload the model uncomment the line below
# loaded_model = load('UMAP.model')







"""## HEBO"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install HEBO
# import hebo ; print(hebo)
# 
#

from hebo.design_space.design_space import DesignSpace
from hebo.optimizers.hebo import HEBO
from hebo.sklearn_tuner import sklearn_tuner
import lightgbm as lgb
from sklearn.metrics import roc_auc_score, f1_score



X_train_best, y_train, X_val_best, y_val = get_best_baseline(dftrain, dfval, dirin="./", frac=1.0)

X_train_best.shape



def lgm_hyper_params(Xtrain, ytrain, Xtest, ytest, niter=1, pars0=None,  nparallel=1,
                     nmax=10**7, dirout="ztmp/"):
  from hebo.design_space.design_space import DesignSpace
  from hebo.optimizers.hebo import HEBO
  from hebo.sklearn_tuner import sklearn_tuner
  import lightgbm as lgb
  from sklearn.metrics import roc_auc_score, f1_score
  from utilmy import pd_to_file, os_makedirs, date_now

  params = pars0
  if pars0 is None:
        params = {'boosting_type': 'gbdt',
            'objective': 'binary',
            'metric': 'auc',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            #'bagging_freq': 5,
            'verbose': 0,
            'max_depth': 7,
            'min_data_in_leaf': 50,
            'reg_alpha': 1.2,
            'reg_lambda': 1.1,
            'early_stopping_round': 30
        }

  pars_space = [
       {'name': 'num_leaves',       'type': 'int', 'lb': 10,   'ub': 200},
       {'name': 'max_depth',        'type': 'int', 'lb': -1,   'ub': 50},
       {'name': 'min_data_in_leaf', 'type': 'int', 'lb': 5,    'ub': 100},
       {'name': 'feature_fraction', 'type': 'num', 'lb': 0.05,  'ub': 0.9},
       {'name': 'bagging_fraction', 'type': 'num', 'lb': 0.05,  'ub': 0.9},
       #{'name': 'max_bin',          'type': 'int', 'lb': 255,  'ub': 500},
       {'name': 'learning_rate',    'type': 'num', 'lb': 0.01, 'ub': 0.3 },

       {'name': 'reg_alpha',    'type': 'num', 'lb': 0.01, 'ub': 2.0 },
       {'name': 'reg_lambda',    'type': 'num', 'lb': 0.01, 'ub': 2.0 },

       ]

  def lgb_f1_score(y_hat, data):
        y_true = data.get_label()
        y_hat = np.where(y_hat < 0.5, 0, 1)   # scikits f1 doesn't like probabilities
        return 'f1', f1_score(y_true, y_hat), True

  def train_model(pars):
       ret = np.zeros((len(pars), 1))
       print(len(pars), ret.shape)
       kk = -1
       for _, r in pars.iterrows():
          kk = kk + 1
          try:
                for x in pars_space:
                    name = x['name']
                    if x['type'] == 'int':
                        params[ name] = int(r[name]
                    elif x['type'] == 'float':
                        params[ name] = float(r[name]
                    else:
                        params[ name] = r[name]


                # d_train,d_eval = lgb.Dataset(Xtrain, label=ytrain), lgb.Dataset(Xtest, label=ytest)
                clf = lgb.LGBMClassifier(**params)
                clf.fit(Xtrain.iloc[:nmax, :], ytrain[:nmax],
                        eval_set=[(Xtest, ytest)],   )

                y_pred = clf.predict(Xtest)
                score  = f1_score(ytest, y_pred)
                print(f"score = {score}")
                ret[kk, 0] = -score
          except Exception as e:
            log(e)

       return ret  # we use negative score as HEBO maximizes the function


  #print(pars_space)
  dt = int(date_now(returnval='unix'))
  os_makedirs(dirout)
  dfall = pd.DataFrame()

  space = DesignSpace().parse(pars_space)
  optim = HEBO(space)
  for i in range(niter):
       log("\n\n######### Iter:", i )
       mpars = optim.suggest(n_suggestions = nparallel )
       score = train_model(mpars)
       print(score)
       optim.observe(mpars, score)
       # print( f'Iter:{i},  {optim.y.min() } ')

       dfi   = pd.concat((  pd.DataFrame(-optim.y, columns=['metric_target']), optim.X ), axis=1 )
       dfall = pd.concat((dfall, dfi))
       dfall = dfall.sort_values('metric_target', 0 )
       pd_to_file(dfall, dirout + f"/df_hyper_{dt}.csv", show=1)

  return optim, dfall

#lgm_hyper_params(Xtrain_hebo, ytrain_hebo, Xeval_hebo, yeval_hebo)
optim, dfall = lgm_hyper_params(X_train_best, y_train, X_val_best, y_val,
                 niter= 1, nparallel=5 )

!conda install pytorch -c pytorch
!conda install scipy
!pip install --upgrade git+https://github.com/cornellius-gp/gpytorch.git
!pip install --upgrade git+https://github.com/pytorch/botorch.git







"""# sebastiantarebustos

Initial Steps:
  1) Run the installs
  2) Mount the google drive
  3) Ready to execute runExperiment01()
  4) Import lgb, plt. Plot the features importances.

## V1
"""

from IPython.display import clear_output

! python --version

!nvidia-smi

#! pip install cudf-cu11 dask-cudf-cu11 --extra-index-url=https://pypi.nvidia.com
! pip install cuml-cu11 --extra-index-url=https://pypi.nvidia.com
#! pip install cugraph-cu11 --extra-index-url=https://pypi.nvidia.com

get_ipython().kernel.do_shutdown(restart=True)

! pip install tabgan==1.3.3
#! pip install sktools
clear_output()

# If using AutoML
#! pip install autogluon
#clear_output() #Accept the reload

! pip install utilmy python-box dirty-cat category_encoders xxhash
! pip install git+https://github.com/MaxHalford/xam
! pip install holidays

from google.colab import drive
drive.mount('/content/drive')

! ls /content/drive/

result_list = []
import lightgbm as lgb
import matplotlib.pyplot as plt
clear_output()

#%load_ext cudf.pandas
def runExperiment01(label, setup = 0, gpu = False):
  import pandas as pd
  from sklearn.utils import shuffle
  import seaborn as sns
  import plotly.express as px
  import matplotlib.pyplot as plt
  import gc
  import numpy as np
  from datetime import datetime
  from sklearn.model_selection import StratifiedKFold, KFold
  from sklearn.metrics import f1_score
  from lightgbm import LGBMClassifier
  import category_encoders as ce
  from sklearn.model_selection import train_test_split
  from sklearn.pipeline import make_pipeline
  from sklearn.preprocessing import StandardScaler, FunctionTransformer
  from sklearn.compose import ColumnTransformer
  from sklearn.feature_selection import VarianceThreshold, SelectFromModel, SelectKBest, mutual_info_classif
  from sklearn.ensemble import HistGradientBoostingClassifier
  #from sktools import GradientBoostingFeatureGenerator
  from sklearn.impute import SimpleImputer
  #import optuna
  import xam
  import lightgbm as lgb
  import holidays
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.compose import make_column_transformer
  #from autogluon.features.generators import AutoMLPipelineFeatureGenerator
  from IPython.display import clear_output
  from sklearn.preprocessing import (
    MaxAbsScaler,
    MinMaxScaler,
    Normalizer,
    PowerTransformer,
    QuantileTransformer,
    RobustScaler,
    StandardScaler,
    minmax_scale,
  )
  from dirty_cat import (
    SimilarityEncoder,
    TargetEncoder,
    MinHashEncoder,
    GapEncoder,
  )

  def reduce_mem_usage(df):
      start_mem = df.memory_usage().sum() / 1024**2
      print(f'Memory usage of dataframe is {start_mem:.2f} MB')

      for col in df.columns:
          col_type = df[col].dtype

          if pd.api.types.is_numeric_dtype(col_type):
              df[col] = pd.to_numeric(df[col], downcast='integer', errors='coerce')
              if col_type != 'int' and col_type != 'float':
                  df[col] = pd.to_numeric(df[col], downcast='float', errors='coerce')
          elif pd.api.types.is_datetime64_any_dtype(col_type):
              df[col] = pd.to_datetime(df[col], errors='coerce')
          elif pd.api.types.is_categorical_dtype(col_type):
              df[col] = df[col].astype('category')

      end_mem = df.memory_usage().sum() / 1024**2
      print(f'Memory usage after optimization is: {end_mem:.2f} MB')
      print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')

      return df


  RANDOM_STATE = 42
  #result_list = []

  parquet_train = "/content/drive/MyDrive/data3-final/dftrain.parquet"
  parquet_eval = "/content/drive/MyDrive/data3-final/dfeval.parquet"

  dftrain = pd.read_parquet(parquet_train)
  dfeval = pd.read_parquet(parquet_eval)
  dftrain = dftrain[ dftrain.ymd < 20230819 ]

  #### From original source
  cols_drop = [ "ymd", "y", "dt", "dt-month",  ] # df-month out because data needs more observations

  cols_leak = [
      *[f"fe-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
      *[f"com-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
      *[f"qkey-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
      *[f"comcat1_hash-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
  ]

  cols_targetenc = [
      'dt-day',
      'dt-hour',
      'dt-weekday',
      'dt-islunch',
      'dt-isevening',
      'dt-isweekend',
      "hh",
      "qkey",
      'qkey-zoom',
      'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1',
      "fe",
  ]

  cols_ordinalenc = [
      "com",
      "comcat1_hash"
  ]

  # Change columns based on type of encoder & complexity of the encoder & number of unique values

  # Cyclic
  if setup == 2:
    cols_targetenc = [
      #'dt-day', #Cyclic
      #'dt-hour', #Cyclic
      #'dt-weekday', #Cyclic
      #'dt-month', # Cyclic
      'dt-islunch',
      'dt-isevening',
      'dt-isweekend',
      "hh",
      "qkey",
      'qkey-zoom',
      'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1',
      # "com",
      # 'comcat1_hash',
      "fe",
    ]

  if setup == 6:
    # Gap
    cols_targetenc = [
        'dt-day',
        'dt-hour',
        'dt-weekday',
        'dt-islunch',
        'dt-isevening',
        'dt-isweekend',
        #"hh",
        "qkey",
        'qkey-zoom',
        'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1',
        # "com",
        'comcat1_hash',
        "fe",
    ]

    cols_ordinalenc = [
        "com",
        "hh",

        #### too many values
        #"comcat1_hash",
        #"qkey",
        #"fe",
    ]

  if setup == 7:

    cols_targetenc = [
        'dt-day',
        'dt-hour',
        'dt-weekday',
        'dt-islunch',
        'dt-isevening',
        'dt-isweekend',
        "hh",
        "qkey",
        'qkey-zoom',
        'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1',
        # "com",
        # 'comcat1_hash',
        "fe",
    ]

    cols_ordinalenc = [
        "com",
        "comcat1_hash"
        # "qkey",
        # "fe",
    ]


  def plotExperiments():
    result_df = pd.DataFrame(result_list, columns=['f1_score', 'label', 'time'])
    result_df['time'] = result_df['time'].dt.seconds
    result_df['model'] = result_df['label'].str.split().str[0]
    result_df = result_df.loc[result_df.groupby('label')['f1_score'].idxmax()]

    result_df = result_df.sort_values('f1_score', ascending=False)
    result_df = result_df.drop_duplicates()

    plt.figure(figsize=(6, len(result_df) * 0.4))

    def color_map(row):
      if row['label'].startswith('*'):
        return 'green'
      if row['f1_score'] > 0.8:
        return 'lightgreen'
      return 'yellow'

    colors = result_df.apply(color_map, axis=1)
    bars = plt.barh(range(len(result_df)), result_df['f1_score'], color=colors)

    for bar, f1 in zip(bars, result_df['f1_score']):
      plt.text(min(bar.get_width() + 0.01, 0.895), bar.get_y() + bar.get_height()/2, f'{f1:.4f}', ha='center', va='center')

    plt.yticks(range(len(result_df)), result_df['label'])
    plt.gca().invert_yaxis()
    plt.xlim(0.6, 1)
    plt.xticks([0.6, 0.65, 0.7, 0.75, 0.80, 0.85, 0.9, 0.95, 1.0])
    plt.xlabel('f1 score (higher is better)')
    plt.show()

  # Feature engineering
  def getFeatures(df_train, df_eval):
    cols_to_lag = cols_leak
    # Extra time data
    def get_time_features(df, col, country = 'US'):
      prefix = col + "-"
      df[col] = pd.to_datetime(df[col])
      df[prefix + 'quarter'] = df[col].dt.quarter # Trends
      df[prefix + 'is_month_start'] = df[col].dt.is_month_start.astype(int) # Paydays
      df[prefix + 'is_month_end'] = df[col].dt.is_month_end.astype(int) # No money

      # Add a column to indicate if the date is a holiday
      holiday_list = holidays.CountryHoliday(country, observed=True)
      df[prefix + 'is_holiday'] = df[col].dt.date.apply(lambda x: int(x in holiday_list))
    get_time_features(df_train, 'dt', 'US')
    get_time_features(df_eval, 'dt', 'US')

    def get_transform_features(X_train, X_test):
      #X_train['dt-day+com'] = X_train['dt-day'] + X_train['com']
      #X_test['dt-day+com'] = X_test['dt-day'] + X_test['com']

      ### Hourly

      # 3 Hours
      cols_D = [col for col in X_train.columns if '10D' in col or '30D' in col]
      new_cols_H = [col+'_4H' for col in cols_D]

      X_train[new_cols_H] = X_train.groupby(['dt-day', 'dt-hour', 'dt-weekday'])[cols_D].transform(lambda x: x.rolling(window=4, min_periods=1).mean())
      X_test[new_cols_H] = X_test.groupby(['dt-day', 'dt-hour', 'dt-weekday'])[cols_D].transform(lambda x: x.rolling(window=4, min_periods=1).mean())

      # Log 3 Hours
      X_train[new_cols_H] = np.log1p(X_train[new_cols_H])
      X_test[new_cols_H] = np.log1p(X_test[new_cols_H])

      # 7 Hours
      new_cols_H = [col+'_8H' for col in cols_D]
      X_train[new_cols_H] = X_train.groupby(['dt-day', 'dt-hour', 'dt-weekday'])[cols_D].transform(lambda x: x.rolling(window=8, min_periods=1).mean())
      X_test[new_cols_H] = X_test.groupby(['dt-day', 'dt-hour', 'dt-weekday'])[cols_D].transform(lambda x: x.rolling(window=8, min_periods=1).mean())

      # Log 7 Hours
      X_train[new_cols_H] = np.log1p(X_train[new_cols_H])
      X_test[new_cols_H] = np.log1p(X_test[new_cols_H])

      ### Daily

      new_cols_D = [col+'_3D' for col in cols_D]

      # 3 Days
      X_train[new_cols_D] = X_train.groupby(['dt-day', 'dt-weekday'])[cols_D].transform(lambda x: x.rolling(window=3, min_periods=1).mean())
      X_test[new_cols_D] = X_test.groupby(['dt-day', 'dt-weekday'])[cols_D].transform(lambda x: x.rolling(window=3, min_periods=1).mean())

      # Log 3 Days
      X_train[new_cols_D] = np.log1p(X_train[new_cols_D])
      X_test[new_cols_D] = np.log1p(X_test[new_cols_D])

      # 7 Days
      new_cols_D = [col+'_7D' for col in cols_D]
      X_train[new_cols_D] = X_train.groupby(['dt-day', 'dt-weekday'])[cols_D].transform(lambda x: x.rolling(window=7, min_periods=1).mean())
      X_test[new_cols_D] = X_test.groupby(['dt-day', 'dt-weekday'])[cols_D].transform(lambda x: x.rolling(window=7, min_periods=1).mean())

      # Log 7 Days
      X_train[new_cols_D] = np.log1p(X_train[new_cols_D])
      X_test[new_cols_D] = np.log1p(X_test[new_cols_D])

      ########## Category Grouping
      ### Fe

      new_cols_D = [col+'_fe_3D' for col in cols_D]

      # 3 Days
      X_train[new_cols_D] = X_train.groupby(['fe'])[cols_D].transform(lambda x: x.rolling(window=3, min_periods=1).mean())
      X_test[new_cols_D] = X_test.groupby(['fe'])[cols_D].transform(lambda x: x.rolling(window=3, min_periods=1).mean())

      # Log 3 Days
      X_train[new_cols_D] = np.log1p(X_train[new_cols_D])
      X_test[new_cols_D] = np.log1p(X_test[new_cols_D])

      # 7 Days
      new_cols_D = [col+'_fe_7D' for col in cols_D]

      X_train[new_cols_D] = X_train.groupby(['fe'])[cols_D].transform(lambda x: x.rolling(window=7, min_periods=1).mean())
      X_test[new_cols_D] = X_test.groupby(['fe'])[cols_D].transform(lambda x: x.rolling(window=7, min_periods=1).mean())

      # Log 7 Days
      X_train[new_cols_D] = np.log1p(X_train[new_cols_D])
      X_test[new_cols_D] = np.log1p(X_test[new_cols_D])

      ### Com
      new_cols_D = [col+'_com_3D' for col in cols_D]

      # 3 Days
      X_train[new_cols_D] = X_train.groupby(['com'])[cols_D].transform(lambda x: x.rolling(window=3, min_periods=1).mean())
      X_test[new_cols_D] = X_test.groupby(['com'])[cols_D].transform(lambda x: x.rolling(window=3, min_periods=1).mean())

      # Log 3 Days
      X_train[new_cols_D] = np.log1p(X_train[new_cols_D])
      X_test[new_cols_D] = np.log1p(X_test[new_cols_D])

      # 7 Days
      new_cols_D = [col+'_com_7D' for col in cols_D]

      X_train[new_cols_D] = X_train.groupby(['com'])[cols_D].transform(lambda x: x.rolling(window=7, min_periods=1).mean())
      X_test[new_cols_D] = X_test.groupby(['com'])[cols_D].transform(lambda x: x.rolling(window=7, min_periods=1).mean())

      # Log 7 Days
      X_train[new_cols_D] = np.log1p(X_train[new_cols_D])
      X_test[new_cols_D] = np.log1p(X_test[new_cols_D])

      ### qkey
      new_cols_D = [col+'_qkey_3D' for col in cols_D]

      # 3 Days
      X_train[new_cols_D] = X_train.groupby(['qkey'])[cols_D].transform(lambda x: x.rolling(window=3, min_periods=1).mean())
      X_test[new_cols_D] = X_test.groupby(['qkey'])[cols_D].transform(lambda x: x.rolling(window=3, min_periods=1).mean())

      # Log 3 Days
      X_train[new_cols_D] = np.log1p(X_train[new_cols_D])
      X_test[new_cols_D] = np.log1p(X_test[new_cols_D])

      # 7 Days
      new_cols_D = [col+'_qkey_7D' for col in cols_D]

      X_train[new_cols_D] = X_train.groupby(['qkey'])[cols_D].transform(lambda x: x.rolling(window=7, min_periods=1).mean())
      X_test[new_cols_D] = X_test.groupby(['qkey'])[cols_D].transform(lambda x: x.rolling(window=7, min_periods=1).mean())

      # Log 7 Days
      X_train[new_cols_D] = np.log1p(X_train[new_cols_D])
      X_test[new_cols_D] = np.log1p(X_test[new_cols_D])

    get_transform_features(dftrain, dfeval)

    # Lagged features
    #def add_lag_features(df, columns, lag=1):
    #  for column in columns:
    #      df[column + '_lags'] = df[column].shift(lag)

    #columns_to_lag = cols_to_lag
    #add_lag_features(df_train, columns_to_lag)
    #add_lag_features(df_eval, columns_to_lag)

  # Feature encoding
  def getEncoded(df_train, df_test, setup = 0):
    missing_train_columns = [col for col in cols_targetenc + cols_ordinalenc + cols_leak + cols_drop if col not in df_train.columns]
    missing_test_columns = [col for col in cols_targetenc + cols_ordinalenc + cols_leak + cols_drop if col not in df_test.columns]

    if missing_train_columns:
        raise ValueError(f"Missing the following columns in Train: {missing_train_columns}")
    if missing_test_columns:
        raise ValueError(f"Missing the following columns in Test: {missing_test_columns}")

    X_train = df_train.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
    y_train = df_train["y"]

    X_test = df_test.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
    y_test = df_test["y"]

    if setup == 19:
      # Count Encoding
      encoder = xam.feature_extraction.CountEncoder(
          columns=cols_targetenc,
          suffix=''
      )
      X_train = encoder.fit_transform(X_train, y_train)
      X_test = encoder.transform(X_test)

      # Ordinal Encoding
      ordinal_encoder = ce.OrdinalEncoder(cols=cols_ordinalenc)
      X_train = ordinal_encoder.fit_transform(X_train, y_train)
      X_test = ordinal_encoder.transform(X_test)

      features = X_train.columns

      # Removes outliers
      scaler = QuantileTransformer(n_quantiles=1000, output_distribution="normal", random_state=42)
      #scaler = StandardScaler()
      X_train = pd.DataFrame(scaler.fit_transform(X=X_train, y=y_train), columns=features)
      X_test = pd.DataFrame(scaler.transform(X=X_test), columns=features)

      print("After Transforming", X_train.shape)
      print(X_train.info())
    return X_train, y_train, X_test, y_test

  def featureSelection(X_train, y_train, X_test):
    feature_selection_transformer = ColumnTransformer(
        transformers=[
            ('variance_threshold', VarianceThreshold(), X_train.columns),
            ('mutual_info_selection', SelectKBest(score_func=mutual_info_classif), X_train.columns)
        ])

    X_train_selected = feature_selection_transformer.fit_transform(X_train, y_train)
    X_test_selected = feature_selection_transformer.transform(X_test)

    selected_feature_names = data.columns[feature_selection_transformer.named_transformers_['mutual_info_selection'].get_support()]
    print('Selected Feature Names:', selected_feature_names)

    X_train = X_train[selected_feature_names]
    X_test = X_test[selected_feature_names]


  best_params = {
    'objective': "binary",
    "random_state": RANDOM_STATE,
    'n_estimators': 1000,
    'learning_rate': 0.05,
    'num_leaves': 100,
    'max_depth': -1,
    'min_data_in_leaf': 100,
    'max_bin': 255,
    'lambda_l1': 0.1,
    'early_stopping_rounds': 300
  }

  if gpu:
    best_params = {
    'objective': "binary",
    "random_state": RANDOM_STATE,
    'n_estimators': 1000,
    'learning_rate': 0.05,
    'num_leaves': 100,
    'max_depth': -1,
    'min_data_in_leaf': 100,
    'max_bin': 255,
    'lambda_l1': 0.1,
    'early_stopping_rounds': 300,

    'boosting_type': 'gbdt',
    'device': 'gpu',
    'gpu_platform_id': 0,
    'gpu_device_id': 0
  }

  start_time = datetime.now()

  # Memory reduction
  dftrain = reduce_mem_usage(dftrain)
  dfeval = reduce_mem_usage(dfeval)

  # Feature engineering
  getFeatures(dftrain, dfeval)

  # Shuffle Data with seed
  dftrain = shuffle(dftrain, random_state=RANDOM_STATE)

  # Feature encoding
  X_train, y_train, X_test, y_test = getEncoded(dftrain, dfeval, setup)

  # Feature selection
  featureSelection(X_train, y_train, X_test)

  # Training
  model = LGBMClassifier(**best_params, verbose=-1)
  model.fit(X_train, y_train, eval_set=[(X_test, y_test)])

  yval_pred = model.predict(X_test)
  score     = f1_score(y_test, yval_pred )

  execution_time = datetime.now() - start_time

  clear_output()

  print('F1 Score:', score)
  result_list.append((score, label, execution_time))
  plotExperiments()

  return model, X_train

#! cat automl-stare_v0.py;

#! nohup  2>&1 bash -c ' python3 automl-stare_v0.py  ' &

# This can be achieved with lagging the features based on clicks.
model1 = runExperiment01('Baseline', 0) # With the old leakage params

lgb.plot_importance(model1, importance_type='split', figsize=(10, 12))
plt.show()

"""### Baseline Target & Ordinal (CE)"""

model1 = runExperiment01('Baseline', 0)

"""### Bayesian (XAM)"""

model1 = runExperiment01('Bayesian', 1)

lgb.plot_importance(model1, importance_type='split', figsize=(10, 12))
plt.show()

"""### Cyclic (XAM)"""

model2 = runExperiment01('Cyclic', 2)

lgb.plot_importance(model2, importance_type='split', figsize=(10, 12))
plt.show()

"""### Count (XAM)"""

# Best encoder performance
model3 = runExperiment01('Count', 3)

lgb.plot_importance(model3, importance_type='split', figsize=(10, 12))
plt.show()

"""### Combine (XAM)"""

model4 = runExperiment01('Combine', 4)

lgb.plot_importance(model4, importance_type='split', figsize=(10, 12))
plt.show()

"""### Count (with suffix) (XAM)

- Count Encoding All ordinal+target cols, it didn't improve the score further.
- This adds a suffix for the new columns instead of replacing the originals.
"""

model5 = runExperiment01('Count 2', 5)

lgb.plot_importance(model5, importance_type='split', figsize=(10, 12))
plt.show()

"""### Similarity (DirtyCat)"""

# Similarity encoding sums all the unique values of both columns and creates new features with the combinations, not good for too many unique values.
model7 = runExperiment01('Similarity', 7)

lgb.plot_importance(model7, importance_type='split', figsize=(10, 17))
plt.show()

lgb.plot_importance(model7, importance_type='split', figsize=(10, 12))
plt.show()

"""### RankHot (CE)"""

# Very costly, will generate many features
model9 = runExperiment01('RankHotEncoding', 9)

lgb.plot_importance(model9, importance_type='split', figsize=(10, 12))
plt.show()

"""### AutoML (Auto)"""

model10 = runExperiment01('AutoML', 10) # 0.6905

lgb.plot_importance(model10, importance_type='split', figsize=(10, 12))
plt.show()

"""### Count + AutoML"""

model11 = runExperiment01('Count + AutoML', 11)

"""### AutoML (Num, Cat, Dt)"""

model13 = runExperiment01('AutoML (Numeric, Categoric, Date)', 13)

"""### Count Cyclic +"""

model14 = runExperiment01('Count Cyclyc +', 14)

"""### LR 0.001 Count Cyclic +"""

model15 = runExperiment01('LR 0.001 Count Cyclic +', 15)

lgb.plot_importance(model15, importance_type='split', figsize=(10, 12))
plt.show()

"""### Count + Engineering + AutoML"""

model16 = runExperiment01('Count + Roll + AutoML', 16)

lgb.plot_importance(model16, importance_type='split', figsize=(10, 16))
plt.show()

"""### Count + Roll + Log + Quantile"""

# _3D and _7D are actually 3H and 7H (Hours not days)
model17 = runExperiment01('Count + Roll + Log + Quantile', 17)

lgb.plot_importance(model17, importance_type='split', figsize=(10, 20))
plt.show()

"""### Count + Roll (4H, 8H, 3D, 7D) + Log + Quantile"""

model18 = runExperiment01('Count + Roll + Log + Quantile (Hourly & Daily Rolling mean)', 18)

lgb.plot_importance(model18, importance_type='split', figsize=(10, 50))
plt.show()

"""### Count + Roll + Log + Quantile (Hourly & Daily & FE & COM & QKEY Rolling mean)"""

model19 = runExperiment01('Count + Roll + Log + Quantile (Hourly & Daily & FE & COM & QKEY Rolling mean)', 19)

import pickle
with open('90_f1_lgbm_classifier.pkl', 'wb') as file:
    pickle.dump(model19, file)

lgb.plot_importance(model19, importance_type='split', figsize=(10, 70))
plt.show()

model19 = runExperiment01('Count + Roll + Log + Quantile (Hourly & Daily & FE & COM & QKEY Rolling mean)', 19)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model19 = runExperiment01('Count + Roll + Log + Quantile (Hourly & Daily & FE & COM & QKEY Rolling mean)', 19)

"""### Rapids Experiment"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# %load_ext cudf.pandas
# model19 = runExperiment01('Count + Roll + Log + Quantile (Hourly & Daily & FE & COM & QKEY Rolling mean)', 19)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# %load_ext cudf.pandas
# model19 = runExperiment01('Count + Roll + Log + Quantile (Hourly & Daily & FE & COM & QKEY Rolling mean)', 19, True)

"""### Feature Selection"""

# Commented out IPython magic to ensure Python compatibility.
# # Test with min window = 1
# %%time
# model, data = runExperiment01('Best + Feature Selection', 19)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model, data = runExperiment01('Best + Feature Selection', 19)

data.columns

"""# Gaurav version"""

!pip install catboost
!pip install scikit-learn
from catboost import CatBoostClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
import numpy as np

!pip install git+https://github.com/MaxHalford/xam --upgrade
import xam

import polars as pl
import os
import lightgbm
os.environ["CUDA_VISIBLE_DEVICES"]="3"
import pandas as pd
from sklearn.metrics import f1_score
import numpy as np
from functools import partial
import scipy as sp
from box import Box

cols_drop = [
"ymd", "y", "dt",
"com-n_fe", "dt-month",
"comcat1_hash-n_imp",
"comcat1_hash-n_fe",



]



cols_leak = [
"n_clk", "n_imp", "n_cta",
]



cols_targetenc = [
'dt-day',
'dt-hour', 'dt-weekday', 'dt-islunch', 'dt-isevening', 'dt-isweekend',
"hh",
"qkey",
"fe",
'qkey-zoom',
'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1'



# "com",
# "comcat1_hash"



]



### Initial values are STRING
cols_ordinalenc = [
"com",
"comcat1_hash"
]



dirtrain = "ztmp/ctr/exp/exp1/train/20230619_*/*.parquet"
dftr = pd_read_file(dirtrain )
log(dftr)
dftr = dftr[ dftr.ymd < 20230830 ] ###no overlap train/test



dirval = "ztmp/ctr/exp/exp1/val/202308*/*.parquet"
dfval = pd_read_file(dirval )
log(dfval)



X_train = dftr.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
y_train = dftr["y"]



X_val = dfval.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
y_val = dfval["y"]



log("target encoder") ### Considered as Float numerical
target_encoder = ce.target_encoder.TargetEncoder(cols=cols_ordinalenc)
X_train = target_encoder.fit_transform(X_train, y_train)
X_val = target_encoder.transform(X_val)



log("ordinal encoder") ### Considered as Float numerical
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cols_targetenc)
X_train = ordinal_encoder.fit_transform(X_train, y_train)
X_val = ordinal_encoder.transform(X_val)

"""## V1"""

no = sum(y_1== 0)/sum(y_1 == 1)


# Define hyperparameters for the CatBoostClassifier
cat_params= {'n_estimators':5000,
             'objective': 'Logloss','allow_writing_files': False,
             'eval_metric':'F1',
          'scale_pos_weight':no # Set the scale_pos_weight based on the ratio of '0' to '1' in the target variable
            }
# Initialize the CatBoostClassifier model with the defined hyperparameters
cat_model=CatBoostClassifier(**cat_params)

# Create a StratifiedKFold cross-validation object with 5 folds
kf=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)

# Initialize arrays to store predictions, scores, and feature importance
oof_cat = np.zeros(dftrain.shape[0])
target_column = np.zeros(dftrain.shape[0])
cat_test_clf = []
cat_test_clf_new_feat=[]
train_scores =[]
val_scores =[]
out_of_fold = []

feature_names = list(dftrain.columns)


# Initialize an array to store feature importance values
feature_importance_values = np.zeros(len(feature_names))

# Loop through the folds for cross-validation
for fold_idx, (train_index, test_index) in enumerate(kf.split(dftrain, y_1)):

    # Split the data into training and validation sets
    X_train,X_test,y_train,y_test= dftrain.loc[train_index,:] ,dftrain.loc[test_index,:],y_1[train_index], y_1[test_index]

    # Fit the CatBoost model to the training data

    cat_model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=500,verbose=False)

    feature_importance_values += cat_model.feature_importances_ / 5

    # Set the number of estimators based on the best iteration
    cat_model.n_estimators = cat_model.best_iteration_

    # Generate predictions on the training and validation sets
    pred_1=cat_model.predict(X_train)
    pred_2=cat_model.predict(X_test)

    # Store predictions for out-of-fold analysis
    oof_cat[test_index] = pred_2
    target_column[test_index] = y_test

    # Generate predictions on the test data and store them
    cat_test_clf_new_feat.append(cat_model.predict(dfval))

    # Calculate ROC AUC scores for training and validation sets
    training_score=f1_score(y_train,pred_1)
    val_score=f1_score(y_test,pred_2)

    # Print fold information and scores
    print(f'fold no ----->{fold_idx+1}')
    print(f'training_f1_score-->>>{training_score}')
    print(f'val_f1_score-->>>{val_score}')
    print('*******************************************')
    print('*******************************************')


    # Store the scores for each fold
    train_scores.append(training_score)
    val_scores.append(val_score)

# Calculate and print the mean ROC AUC scores for training and validation sets
print(f'train mean score  is ---->{np.mean(train_scores)}')
print(f'val mean score  is ---->{np.mean(val_scores)}')

# Calculate the final test predictions by averaging predictions from all folds
cat_test_clf_predictions_new_feat = np.squeeze(np.mean(cat_test_clf_new_feat, axis=0))





encoder = xam.feature_extraction.BayesianTargetEncoder(columns=['com','hh'],
prior_weight=3,suffix='')

dftrain=encoder.fit_transform(dftrain, dftrain['y'])
dfval=encoder.fit_transform(dfval, dfval['y'])













"""# 3) AutoML Script"""



"""## A) template"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile automl_v0.py
# #### All imports
# import os,sys, pandas as pd, polars as pl, numpy as np, scipy as sp
# from box import Box
# from utilmy import (log, pd_read_file, pd_to_file, date_now, os_makedirs,  glob_glob, log2, log_pd)
# import  lightgbm
# os.environ["CUDA_VISIBLE_DEVICES"]="3"
# from sklearn.metrics import f1_score
# from functools import partial
# import category_encoders as ce
# 
# 
# #### Data Load
# myshortcut_folder = "data3-final/"
# dir0="/content/drive/MyDrive/" + myshortcut_folder
# os.chdir(dir0)
# log(os.getcwd(), glob_glob(dir0 + "/*"))
# 
# dftrain = pd_read_file(dir0 + "/dftrain*")
# dfval   = pd_read_file(dir0 + "/dfeval*")
# dftrain = dftrain[ dftrain.ymd <= 20230830 ] ###no overlap train/test
# dftrain = dftrain.sample(frac= 0.6)
# log(dftrain.ymd.max())
# log(dfval.shape)
# 
# #### Clean #############################################
# cols_drop = [ "ymd", "y", "dt", "dt-month",  ] # df-month out because data needs more observations
# cols_leak = [ *[f"fe-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
#     *[f"com-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
#     *[f"qkey-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
#     *[f"comcat1_hash-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
# ]
# 
# X_train = dftrain.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
# y_train = dftrain["y"]
# 
# X_val = dfval.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
# y_val = dfval["y"]
# del dftrain, dfval
# 
# 
# ### Prepro  ###########################################
# cols_targetenc = [
#     'dt-day', 'dt-hour', 'dt-weekday', 'dt-islunch', 'dt-isevening', 'dt-isweekend',
#     "hh",
#     "qkey",
#     'qkey-zoom',
#     'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1',
#     "fe",
# ]
# 
# cols_ordinalenc = [ "com",  "comcat1_hash"   ]
# 
# # Count Encoding
# encoder = xam.feature_extraction.CountEncoder(
#     columns=cols_targetenc,
#     suffix=''
# )
# X_train = encoder.fit_transform(X_train, y_train)
# X_test = encoder.transform(X_test)
# 
# # Ordinal Encoding
# ordinal_encoder = ce.OrdinalEncoder(cols=cols_ordinalenc)
# X_train = ordinal_encoder.fit_transform(X_train, y_train)
# X_test = ordinal_encoder.transform(X_test)
# 
# 
# 
# 
# ### Your AUTOML Code
# 
# 
# 
# 
# 
# 
# 
# 
#

### Check if the code is correct
! cat automl_v0.py

#### Run in background mode   Send All output to nohup.out
! nohup  2>&1 bash -c ' python3 automl_v0.py  ' &







"""## B) V2"""











"""# 4) Train V2"""

def run3():
    """ TODO :

    1) Try to XAM encoder, bayesian stuff,  Cycle transformer, cosinus, sinus

    https://github.com/MaxHalford/xam/blob/master/docs/feature-extraction.md
    trans = xam.feature_extraction.CycleTransformer()

   2) try this one
      https://dirty-cat.github.io/stable/auto_examples/01_dirty_categories.html#example-table-vectorizer

    ## Results


    lname = ["category_encoders.BackwardDifferenceEncoder"
    ,"category_encoders.BaseNEncoder"
    ,"category_encoders.BinaryEncoder"
    ,"category_encoders.CatBoostEncoder"
    ,"category_encoders.CountEncoder"
    ,"category_encoders.GLMMEncoder"
    ,"category_encoders.HashingEncoder"
    ,"category_encoders.HelmertEncoder"
    ,"category_encoders.JamesSteinEncoder"
    ,"category_encoders.LeaveOneOutEncoder"
    ,"category_encoders.MEstimateEncoder"
    #,"category_encoders.OneHotEncoder"
    ,"category_encoders.OrdinalEncoder"
    ,"category_encoders.PolynomialEncoder"
    ,"category_encoders.QuantileEncoder"
    ,"category_encoders.SumEncoder"
    ,"category_encoders.TargetEncoder"
    ,"category_encoders.WOEEncoder"
    ]


    """
    dftr, dfval = lload( 1*10**6)




    ############# V1 ###################################################
    # Update model with shifted feature removed
    cols_drop = ["ymd", "y", "dt",
        "com-n_fe",  "dt-month",
        "comcat1_hash-n_imp",
       "comcat1_hash-n_fe",
    ]

    cols_leak = [
        "n_clk", "n_imp", "n_cta",
        # *[f"fe-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
        # *[f"com-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
        # *[f"qkey-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
        # "dt-day"
    ]

    Xtrain = dftr.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
    ytrain = dftr["y"]
    Xval = dfval.drop(cols_drop, axis=1).drop(cols_leak, axis=1)
    yval = dfval["y"]



    #####################################################################
    cols_1 = [
        #'dt-day',
        #'dt-hour',
        'dt-weekday', 'dt-islunch', 'dt-isevening', 'dt-isweekend',
        "hh",
        # "com",
        # 'comcat1_hash',
        "qkey",
        "fe",
        'qkey-zoom',
        'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1'

        # "com",
        # "comcat1_hash"

    ]


    ### Initial values are STRING
    cols_2 = [
        "com",
        "comcat1_hash"
        # "qkey",
        # "fe",
    ]


    cols_3 = [ 'dt-day',  'dt-hour' ]


    exp_list = [
        [
            (cols_1,  "category_encoders.TargetEncoder",  {}  ),
            (cols_2,  "category_encoders.OrdinalEncoder", {}   ),

            (cols_3,  "xam.feature_extraction.CycleTransformer", {}   ),
        ]




    ]

    log("##### Run all experiments ")
    for ii, enc_list in enumerate(exp_list):
        log("\n\n####", ii )
        ##### Encode All #################################################
        for x in enc_list:
            encClass = load_function_uri( x[1]  ) ### Will load from the String name
            enc      = encClass(cols=x[0])
            log(enc)
            Xtrain = enc.fit_transform(Xtrain, ytrain)
            Xval   = enc.transform(Xval)

        pd_to_file( Xtrain.head(2).T, f"ztmp/atest/exp_{ii}/xtrain.csv", sep="\t", show=1 )
        f1 = lgbm_eval(Xtrain,ytrain, Xval, yval)



####################################################################################
def lload(nmax=1000000000):
    dirtrain = "dftrain.parquet"
    dftr = pd_read_file(dirtrain )
    log(dftr)
    dftr = dftr[ dftr.ymd < 20230830 ] ###no overlap train/test

    dirval   = "dfval.parquet"
    dfval   = pd_read_file(dirval )
    log(dfval)
    return dftr.iloc[:nmax, :], dfval.iloc[:nmax, :]



def lgbm_eval(Xtrain,ytrain, Xval, yval):
    log("train")
    pars_lgm = {
        "objective": "binary",
        # "iterations": 10000,
        "random_seed": 42,
        "early_stopping_rounds": 300,
        "learning_rate": 0.05,
        ###
        'bagging_fraction': 0.8,
        "colsample_bytree": 0.9,
        'max_depth': 7,
    }
    model = LGBMClassifier(**pars_lgm, )
    model.fit( Xtrain, ytrain, eval_set=[(Xval, yval)],
               verbose=100)

    yval_pred = model.predict(Xval)
    score     = f1_score(yval, yval_pred )
    log('F1 Score Val:', score)
    return score































"""# 4) Train V3

https://github.com/zelros/cinnamon
        https://riverml.xyz/0.19.0/examples/the-art-of-using-pipelines/
        https://adapt-python.github.io/adapt/generated/adapt.instance_based.TrAdaBoostR2.html







"""
"""

def model_train_get_sampleweight(cfg, Xtrain, Xval, ytrain, yval, method='ada', cc= None):
    """Evaluates train data
    Pars:

        cfg (object):        configuration object.
        Xtrain (array-like): training data.
        Xval (array-like):   validation data.

        Xweight = model_train_get_sampleweight(cfg, Xtrain, Xval, ytrain, yval, method='ada', cc= cc)

    Docs:
        https://github.com/awesome-mlops/awesome-ml-monitoring
        https://github.com/zelros/cinnamon
        https://riverml.xyz/0.19.0/examples/the-art-of-using-pipelines/
        https://adapt-python.github.io/adapt/generated/adapt.instance_based.TrAdaBoostR2.html

    """
    from sklearn.ensemble import (RandomForestClassifier)
    from adapt.instance_based import (TrAdaBoost, IWN)

    if method == 'data':
        model = TrAdaBoost(RandomForestClassifier(), n_estimators=10, Xt=Xval, yt=yval, random_state=0)
        model.fit(Xtrain, ytrain)
        return model.predict_weights(Xval)

    if method == 'irwn':
        model = IWN(RandomForestClassifier(), Xt=Xval, sigma_init=0.1, random_state=0,
                    pretrain=True, pretrain__epochs=100, pretrain__verbose=0)
        model.fit(Xtrain, ytrain, epochs=100, batch_size=256, verbose=1)
        return model.predict_weights(Xval)

"""# -----------------------------------"""

















































"""# Zold"""

drop_cols = [
    "ymd", "y", "dt",
    # "fe", "com", "qkey"
]
# train_drop_cols = drop_cols + ["fold"]

leakage_cols = [
    'com-n_clk', 'com-n_imp', 'com-n_cta',
    # *[f"fe-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
    # *[f"com-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
    # *[f"qkey-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
    # "dt-day"
]
cat_cols = [
    'dt-day', 'dt-hour', 'dt-weekday', 'dt-islunch', 'dt-isevening', 'dt-isweekend',
    "hh",
    # "com",
    # 'comcat1_hash',
    "qkey",
    "fe",
    'qkey-zoom',
    'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1'
]
cat_to_encode_cols = [
    "com",
    "comcat1_hash"
    # "qkey",
    # "fe",
]

dftrain

X_train = dftrain.drop(drop_cols, axis=1).drop(leakage_cols, axis=1)
y_train = dftrain["y"]

X_val = dfval.drop(drop_cols, axis=1).drop(leakage_cols, axis=1)
y_val = dfval["y"]

print("target encoder")
# Target encoder
target_encoder = ce.target_encoder.TargetEncoder(cols=cat_to_encode_cols)
X_train = target_encoder.fit_transform(X_train, y_train)
X_val = target_encoder.transform(X_val)
print("ordinal encoder")
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cat_cols)
X_train = ordinal_encoder.fit_transform(X_train, y_train)
X_val = ordinal_encoder.transform(X_val)



print("train")
params = {
    "objective": "binary",
    # "devices": "0",
    # "grow_policy": "Lossguide",
    "iterations": 10000,
    "random_seed": 42,
    "early_stopping_rounds": 300,
    "learning_rate": 0.05,
    # "bootstrap_type": "Poisson",
    ###
    # "subsample": 0.75,
    # "depth": 9,
    ###
    'bagging_fraction': 0.8,
    "colsample_bytree": 0.9,
    'max_depth': 7,
    # 'l2_leaf_reg': 2.7781618792450087,
    # 'min_data_in_leaf': 18,
    # 'max_leaves': 18
}
classifier = LGBMClassifier(
    **params,
    # eval_metric="F1"
)
classifier.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)])

score = f1_score(y_val, classifier.predict(X_val))
print(score)

from google.colab import drive
drive.mount('/content/drive')



import dask.dataframe as dd
import category_encoders as ce
from lightgbm import LGBMClassifier
from sklearn.metrics import f1_score

# Define a list of category encoders to test
encoder_names = [
    "BackwardDifferenceEncoder",
    "BaseNEncoder",
    "BinaryEncoder",
    "CatBoostEncoder",
    "CountEncoder",
    "GLMMEncoder",
    "HashingEncoder",
    "HelmertEncoder",
    "JamesSteinEncoder",
    "LeaveOneOutEncoder",
    "MEstimateEncoder",
    "OrdinalEncoder",
    "PolynomialEncoder",
    "QuantileEncoder",
    "SumEncoder",
    "TargetEncoder",
    "WOEEncoder"
]

# Load your data using Dask
dftrain = dd.read_parquet("dftrain.parquet")
dfval = dd.read_parquet("dfval.parquet")

# Create a dictionary to store encoder names and their corresponding F1 scores
encoder_scores = {}

# Loop through each encoder and evaluate its performance
for encoder_name in encoder_names:
    encoder = getattr(ce, encoder_name)()

    # Apply encoder on training and validation data in chunks
    X_train_encoded = encoder.fit_transform(dftrain.drop(columns=["y"]), dftrain["y"]).compute()
    X_val_encoded = encoder.transform(dfval.drop(columns=["y"])).compute()

    # Initialize and train the model
    model = LGBMClassifier()
    model.fit(X_train_encoded, dftrain["y"].compute())

    # Make predictions and calculate F1 score
    y_pred = model.predict(X_val_encoded)
    f1 = f1_score(dfval["y"].compute(), y_pred)

    # Store the encoder name and F1 score in the dictionary
    encoder_scores[encoder_name] = f1

# Find the best encoder based on the highest F1 score
best_encoder = max(encoder_scores, key=encoder_scores.get)
best_f1_score = encoder_scores[best_encoder]

print(f"Best Encoder: {best_encoder}")
print(f"Best F1 Score: {best_f1_score}")

class OptimizedRounder(object):
    def __init__(self, beta=0.5):
        self.coef_ = 0
        self.beta = beta

    def _kappa_loss(self, coef, y_true, y_pred):
        ll = f1_score(y_true, self.predict(y_pred, coef))
        print(*coef, f" => {ll}")
        return -ll

    def fit(self, y_true, y_pred, initial_coef=[0.7]):
        loss_partial = partial(self._kappa_loss, y_true=y_true, y_pred=y_pred)
        self.coef_ = sp.optimize.minimize(
            loss_partial, initial_coef, method="nelder-mead"
        )

    def predict(self, X, coef):
        X_p = np.copy(X)
        for i, pred in enumerate(X_p):
            if pred < coef[0]:
                X_p[i] = 0
            else:
                X_p[i] = 1
        return X_p

    def coefficients(self):
        return self.coef_["x"]

optR = OptimizedRounder(beta=0.1)
optR.fit(y_val, classifier.predict_proba(X_val)[:,1])
coefficients = optR.coefficients()

!pip install adversarial_test

import adversarial_test

adv_model = adversarial_test.adversarial_model.AdversarialModel()
adv_model.fit(
    X_train,
    X_val,
    cat_features=cat_cols
)
adv_model.evaluate()



"""Remove shifted features"""

# Update model with shifted feature removed
drop_cols = [
    "ymd", "y", "dt",
    "dt-day", "dt-month",
    "com-n_fe",
    "comcat1_hash-n_clk_10D", "comcat1_hash-n_cta_10D", "comcat1_hash-n_fe", "comcat1_hash-n_imp"
    # "fe", "com", "qkey"
]
# train_drop_cols = drop_cols + ["fold"]

leakage_cols = [
    "n_clk", "n_imp", "n_cta",
    *[f"fe-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
    # *[f"com-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
    # *[f"qkey-{field}" for field in ["n_clk", "n_imp", "n_cta"]],
    # "dt-day"
]
cat_cols = [
    # 'dt-day',
    'dt-hour', 'dt-weekday', 'dt-islunch', 'dt-isevening', 'dt-isweekend',
    "hh",
    # "com",
    # 'comcat1_hash',
    "qkey",
    "fe",
    'qkey-zoom',
    'qkey-qhash', 'qkey-a', 'qkey-b', 'qkey-c', 'qkey-1'
]
cat_to_encode_cols = [
    "com",
    "comcat1_hash"
    # "qkey",
    # "fe",
]

X_train = train_df.drop(drop_cols, axis=1).drop(leakage_cols, axis=1)
y_train = train_df["y"]

X_val = val_df.drop(drop_cols, axis=1).drop(leakage_cols, axis=1)
y_val = val_df["y"]

print("target encoder")
# Target encoder
target_encoder = ce.target_encoder.TargetEncoder(cols=cat_to_encode_cols)
X_train = target_encoder.fit_transform(X_train, y_train)
X_val = target_encoder.transform(X_val)
print("ordinal encoder")
ordinal_encoder = ce.ordinal.OrdinalEncoder(cols=cat_cols)
X_train = ordinal_encoder.fit_transform(X_train, y_train)
X_val = ordinal_encoder.transform(X_val)



print("train")
params = {
    "objective": "binary",
    # "devices": "0",
    # "grow_policy": "Lossguide",
    "iterations": 10000,
    "random_seed": 42,
    "early_stopping_rounds": 300,
    "learning_rate": 0.05,
    # "bootstrap_type": "Poisson",
    ###
    # "subsample": 0.75,
    # "depth": 9,
    ###
    'bagging_fraction': 0.8,
    "colsample_bytree": 0.9,
    'max_depth': 7,
    # 'l2_leaf_reg': 2.7781618792450087,
    # 'min_data_in_leaf': 18,
    # 'max_leaves': 18
}
classifier = LGBMClassifier(
    **params,
    # eval_metric="F1"
)
classifier.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    verbose=100
)

score = f1_score(y_val, classifier.predict(X_val))
print(score)





"""# New Section"""

def feat_prepro_train1(cfg:dict=None, nmax:int=None, dt_start=None, dt_end=None, dirin=None,
                       dirout:str=None, mode='train', returnval=0, exportdisk=1,
                       use_impnoclk=0
                       )->pd.DataFrame:
    """  Get Updated list of featureid into Dict format
    Docs::

         py2 feat_prepro_train1 --mode train  --dt_start 20230101  --dt_end  20230401  --cfg $cfg

    """
    cfg  = config_load(cfg, to_dataclass=True)
    cfgp = cfg[mode]

    cfg[mode].dirin   = cfgp['dirin' ] if dirin  is None else dirin
    cfg[mode].dirout  = cfgp['dirout'] if dirout is None else dirout
    cfg[mode].ymd_min = int(cfgp.get('ymd_min', 20220901))  if dt_start is None else int(dt_start)
    cfg[mode].ymd_max = int(cfgp.get('ymd_max', 20230201))  if dt_end   is None else int(dt_end)
    cfg[mode].nmax    = int(cfgp.get('nmax', 10**5))        if nmax     is None else int(nmax)

    nmax   = cfg[mode].nmax
    dirout = cfg[mode].dirout

    cols0 = ['qkey', 'com', 'fe', 'ymd', 'hh' ]
    dfq   = data_get_qkey_feid_log(cfg, nmax=nmax, mode='hourly', task=mode, colkeys = cols0,
                                   use_impnoclk=use_impnoclk)

    ### 1 if nb of clicks > 0
    dfq['y'] = dfq.apply( lambda x : 1 if x["n_clk"] +  x["n_cta"] > 0  else 0 , axis=1 )
    dfq      = dfq[cols0 + [ 'n_imp', 'n_clk', 'n_cta',  'y'    ] ]


    #### Feature Creation  ####################################
    dfq = featgroup_add_date(dfq,    cfg=cfg, mode=mode, use_existing_date=True)
    dfq = featgroup_add_quadkey(dfq, cfg=cfg, mode=mode)
    dfq = featgroup_add_company(dfq, cfg=cfg, mode=mode)
    dfq = featgroup_add_poi(dfq,     cfg=cfg, mode=mode)

    ## TODO: Need to fix error with empty data
    # dfq = featgroup_add_weather(dfq, cfg=cfg, mode=mode)


    ### Take long time : need direct fetch
    dfq = featgroup_add_hourly(dfq,  cfg=cfg, mode=mode, dt=cfg[mode].ymd_max, add_days=-1, fetch=1)

    dfq = feat_dtype_norm(dfq, cfg=cfg, mode=mode)


    log_pd(dfq)

    t0, t1 = int(dfq['ymd'].min()),  int(dfq['ymd'].max())
    log('Dates', t0, t1 )

    if dirout is not None and exportdisk>0 :
       dirouti = dirout + f"/{t0}_{t1}/"
       pd_to_file_stats(dfq,  dirouti , mode=mode)

    if returnval>0:
        return dfq













"""# New Section"""
