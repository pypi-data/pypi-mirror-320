{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9852976,"sourceType":"datasetVersion","datasetId":6046103}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n# SET ENV VARIABLE\nNote: This script only works when run on Kaggle notebook directly (not run with !python secret.py)\n\"\"\"\nexec(\"\"\"\\nimport os\\nfrom kaggle_secrets import UserSecretsClient\\n\\nSECRET_REPO_DIR='/secret'\\n\\nuser_secrets = UserSecretsClient()\\nprint(\"Get kaggle secret ...\")\\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\\n\\nos.system(f'rm -rf {SECRET_REPO_DIR};')\\nos.system(f'git clone https://hahunavth:{GITHUB_TOKEN}@github.com/hahunavth/kaggle-secret.git {SECRET_REPO_DIR}')\\n\\nprint(\"Load .env ...\")\\nwith open(f\"{SECRET_REPO_DIR}/kaggle.env\", \"r\") as f:\\n    for line in f.readlines():\\n        line = line.rstrip()\\n        if len(line) == 0:\\n            continue\\n        line = line.split(\"=\")\\n        if len(line) == 2:\\n            name, value = line\\n            print(f\"Set: {name}\")\\n            os.environ[name.rstrip()] = value.rstrip()\\n        elif len(line) > 0:\\n            print(f\"Ignore: {line[0]}, invalid syntax\")\\n\\nprint(\"Remove repo ...\")\\nos.system(f'rm -rf {SECRET_REPO_DIR};')\\nprint(\"Done\")\\n\\nassert os.environ[name]\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:28:19.855552Z","iopub.execute_input":"2024-11-09T13:28:19.856698Z","iopub.status.idle":"2024-11-09T13:28:20.957935Z","shell.execute_reply.started":"2024-11-09T13:28:19.856643Z","shell.execute_reply":"2024-11-09T13:28:20.956568Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Get kaggle secret ...\n","output_type":"stream"},{"name":"stderr","text":"Cloning into '/secret'...\n","output_type":"stream"},{"name":"stdout","text":"Load .env ...\nSet: HF_TOKEN\nSet: GITHUB_TOKEN\nSet: KAGGLE_USERNAME\nSet: KAGGLE_KEY\nSet: WANDB_SECRET\nIgnore: # Speech Enhancement Project, invalid syntax\nIgnore: SE_SERVICE_ACCOUNT_JSON, invalid syntax\nRemove repo ...\nDone\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install peft -q","metadata":{"execution":{"iopub.status.busy":"2024-11-09T13:28:20.961049Z","iopub.execute_input":"2024-11-09T13:28:20.961652Z","iopub.status.idle":"2024-11-09T13:28:37.811818Z","shell.execute_reply.started":"2024-11-09T13:28:20.961592Z","shell.execute_reply":"2024-11-09T13:28:37.810210Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Benchmark\n\nTry inference gemma-2-2b on CPU","metadata":{}},{"cell_type":"markdown","source":"## Without quantization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport numpy as np\nimport os\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\n\n\nclass GemmaSentenceEmbedding:\n    def __init__(self, model_path='google/gemma-2b-it', adapter_path=None):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model_path, \n            token=os.environ['HF_TOKEN']\n        )\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_path, \n            torch_dtype=torch.bfloat16,\n            device_map=device,\n            trust_remote_code=True,\n            token=os.environ['HF_TOKEN']\n        )\n        if adapter_path != None:\n            # Load fine-tuned LoRA\n            self.model.load_adapter(adapter_path)\n\n    def get_last_hidden_state(self, text):\n        inputs = self.tokenizer(text, return_tensors=\"pt\").to(device)\n        print(\"N token:\", inputs[0])\n        with torch.no_grad():\n            out = self.model(**inputs, output_hidden_states=True).hidden_states[-1][0, -1, :]\n        return out.squeeze().float().cpu().numpy()\n\n    def encode(self, sentences: list[str], **kwargs) -> list[np.ndarray]:\n        \"\"\"\n        Returns a list of embeddings for the given sentences.\n        \n        Args:\n            sentences: List of sentences to encode\n\n        Returns:\n            List of embeddings for the given sentences\n        \"\"\"\n\n        out = []\n\n        for s in sentences:\n            out.append(self.get_last_hidden_state(s))\n\n        return out\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-09T00:24:00.939134Z","iopub.execute_input":"2024-11-09T00:24:00.939532Z","iopub.status.idle":"2024-11-09T00:24:05.905238Z","shell.execute_reply.started":"2024-11-09T00:24:00.939489Z","shell.execute_reply":"2024-11-09T00:24:05.904281Z"},"_kg_hide-input":true,"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"gemma_2_2b_it = GemmaSentenceEmbedding(\n    'google/gemma-2-2b-jpn-it', \n    None\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-09T00:24:05.906342Z","iopub.execute_input":"2024-11-09T00:24:05.906791Z","iopub.status.idle":"2024-11-09T00:26:18.027989Z","shell.execute_reply.started":"2024-11-09T00:24:05.906756Z","shell.execute_reply":"2024-11-09T00:26:18.027085Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/46.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d57b29f271184c06bf3c93c7fc22aefd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960346b7af7e45ddbd4c48370250afb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96be2f44abff4db89abb6d41f9fd8069"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42fb9062636643f38d5ebab17caf0e00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4c75aba06464535b6ff1f6d529b4a0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7b6393f39fa47cb866876c1ce410f84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e96c773ca64722be300fcc73ef78e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ddb391669be41a69bc50e30be7eae11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91286c96f73545fb927f88c0c22f92d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1d926a8febd4d7395b4658f01d2cda8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9081c83b3347400598a8ebee0c98dab6"}},"metadata":{}}]},{"cell_type":"code","source":"%%time\nout = gemma_2_2b_it.encode([\"I like apple and i don't like banana.\"])\nnp.array(out).shape","metadata":{"execution":{"iopub.status.busy":"2024-11-09T00:26:18.029989Z","iopub.execute_input":"2024-11-09T00:26:18.030442Z","iopub.status.idle":"2024-11-09T00:26:19.423323Z","shell.execute_reply.started":"2024-11-09T00:26:18.030394Z","shell.execute_reply":"2024-11-09T00:26:19.422327Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"N token: Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n","output_type":"stream"},{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 637 ms, sys: 249 ms, total: 886 ms\nWall time: 1.39 s\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(1, 2304)"},"metadata":{}}]},{"cell_type":"code","source":"%%time\ngemma_2_2b_it.encode([\"Hello\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-09T00:26:19.424378Z","iopub.execute_input":"2024-11-09T00:26:19.424852Z","iopub.status.idle":"2024-11-09T00:26:19.582793Z","shell.execute_reply.started":"2024-11-09T00:26:19.424817Z","shell.execute_reply":"2024-11-09T00:26:19.581908Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"N token: Encoding(num_tokens=2, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\nCPU times: user 142 ms, sys: 10.8 ms, total: 153 ms\nWall time: 152 ms\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[array([ 0.4921875 ,  0.20117188, -0.28515625, ...,  3.921875  ,\n         0.40429688, -0.62890625], dtype=float32)]"},"metadata":{}}]},{"cell_type":"markdown","source":"## With quantization\n\n- Model: gemma-2-2b-it-Q4_K_M.gguf","metadata":{}},{"cell_type":"code","source":"!huggingface-cli download bartowski/gemma-2-2b-it-GGUF --include \"## Without quantization\" --local-dir ./","metadata":{"execution":{"iopub.status.busy":"2024-11-08T23:41:48.784027Z","iopub.execute_input":"2024-11-08T23:41:48.784670Z","iopub.status.idle":"2024-11-08T23:42:31.914021Z","shell.execute_reply.started":"2024-11-08T23:41:48.784625Z","shell.execute_reply":"2024-11-08T23:42:31.912255Z"},"_kg_hide-output":true,"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Fetching 1 files:   0%|                                   | 0/1 [00:00<?, ?it/s]Downloading 'gemma-2-2b-it-Q4_K_M.gguf' to '.cache/huggingface/download/gemma-2-2b-it-Q4_K_M.gguf.e0aee85060f168f0f2d8473d7ea41ce2f3230c1bc1374847505ea599288a7787.incomplete'\n\ngemma-2-2b-it-Q4_K_M.gguf:   0%|                    | 0.00/1.71G [00:00<?, ?B/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   1%|           | 10.5M/1.71G [00:00<00:43, 39.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   1%|▏          | 21.0M/1.71G [00:00<00:41, 41.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   2%|▏          | 31.5M/1.71G [00:00<00:39, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   2%|▎          | 41.9M/1.71G [00:00<00:38, 42.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   3%|▎          | 52.4M/1.71G [00:01<00:38, 42.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   4%|▍          | 62.9M/1.71G [00:01<00:38, 43.0MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   4%|▍          | 73.4M/1.71G [00:01<00:38, 42.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   5%|▌          | 83.9M/1.71G [00:01<00:37, 43.0MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   6%|▌          | 94.4M/1.71G [00:02<00:37, 42.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   6%|▋           | 105M/1.71G [00:02<00:37, 42.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   7%|▊           | 115M/1.71G [00:02<00:37, 42.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   7%|▉           | 126M/1.71G [00:02<00:36, 43.0MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   8%|▉           | 136M/1.71G [00:03<00:36, 43.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   9%|█           | 147M/1.71G [00:03<00:36, 43.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:   9%|█           | 157M/1.71G [00:03<00:36, 43.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  10%|█▏          | 168M/1.71G [00:03<00:36, 41.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  10%|█▎          | 178M/1.71G [00:04<00:36, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  11%|█▎          | 189M/1.71G [00:04<00:36, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  12%|█▍          | 199M/1.71G [00:04<00:35, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  12%|█▍          | 210M/1.71G [00:04<00:35, 41.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  13%|█▌          | 220M/1.71G [00:05<00:35, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  14%|█▌          | 231M/1.71G [00:05<00:34, 42.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  14%|█▋          | 241M/1.71G [00:05<00:34, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  15%|█▊          | 252M/1.71G [00:05<00:35, 41.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  15%|█▊          | 262M/1.71G [00:06<00:34, 41.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  16%|█▉          | 273M/1.71G [00:06<00:34, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  17%|█▉          | 283M/1.71G [00:06<00:33, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  17%|██          | 294M/1.71G [00:06<00:34, 41.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  18%|██▏         | 304M/1.71G [00:07<00:33, 42.0MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  18%|██▏         | 315M/1.71G [00:07<00:33, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  19%|██▎         | 325M/1.71G [00:07<00:32, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  20%|██▎         | 336M/1.71G [00:07<00:33, 41.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  20%|██▍         | 346M/1.71G [00:08<00:32, 41.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  21%|██▌         | 357M/1.71G [00:08<00:32, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  21%|██▌         | 367M/1.71G [00:08<00:31, 42.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  22%|██▋         | 377M/1.71G [00:08<00:32, 41.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  23%|██▋         | 388M/1.71G [00:09<00:31, 41.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  23%|██▊         | 398M/1.71G [00:09<00:31, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  24%|██▊         | 409M/1.71G [00:09<00:30, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  25%|██▉         | 419M/1.71G [00:09<00:31, 41.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  25%|███         | 430M/1.71G [00:10<00:30, 41.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  26%|███         | 440M/1.71G [00:10<00:30, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  26%|███▏        | 451M/1.71G [00:10<00:29, 42.0MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  27%|███▏        | 461M/1.71G [00:10<00:29, 41.6MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  28%|███▎        | 472M/1.71G [00:11<00:29, 42.0MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  28%|███▍        | 482M/1.71G [00:11<00:29, 41.6MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  29%|███▍        | 493M/1.71G [00:11<00:29, 41.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  29%|███▌        | 503M/1.71G [00:11<00:28, 41.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  30%|███▌        | 514M/1.71G [00:12<00:28, 42.0MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  31%|███▋        | 524M/1.71G [00:12<00:28, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  31%|███▊        | 535M/1.71G [00:12<00:27, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  32%|███▊        | 545M/1.71G [00:12<00:27, 41.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  33%|███▉        | 556M/1.71G [00:13<00:27, 42.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  33%|███▉        | 566M/1.71G [00:13<00:26, 42.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  34%|████        | 577M/1.71G [00:13<00:26, 42.6MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  34%|████        | 587M/1.71G [00:13<00:27, 41.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  35%|████▏       | 598M/1.71G [00:14<00:26, 41.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  36%|████▎       | 608M/1.71G [00:14<00:26, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  36%|████▎       | 619M/1.71G [00:14<00:25, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  37%|████▍       | 629M/1.71G [00:14<00:26, 41.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  37%|████▍       | 640M/1.71G [00:15<00:25, 41.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  38%|████▌       | 650M/1.71G [00:15<00:25, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  39%|████▋       | 661M/1.71G [00:15<00:24, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  39%|████▋       | 671M/1.71G [00:15<00:25, 41.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  40%|████▊       | 682M/1.71G [00:16<00:24, 41.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  41%|████▊       | 692M/1.71G [00:16<00:24, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  41%|████▉       | 703M/1.71G [00:16<00:23, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  42%|█████       | 713M/1.71G [00:16<00:24, 41.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  42%|█████       | 724M/1.71G [00:17<00:23, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  43%|█████▏      | 734M/1.71G [00:17<00:23, 42.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  44%|█████▏      | 744M/1.71G [00:17<00:22, 42.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  44%|█████▎      | 755M/1.71G [00:17<00:23, 41.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  45%|█████▍      | 765M/1.71G [00:18<00:22, 41.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  45%|█████▍      | 776M/1.71G [00:18<00:22, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  46%|█████▌      | 786M/1.71G [00:18<00:21, 42.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  47%|█████▌      | 797M/1.71G [00:18<00:22, 41.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  47%|█████▋      | 807M/1.71G [00:19<00:21, 41.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  48%|█████▋      | 818M/1.71G [00:19<00:21, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  48%|█████▊      | 828M/1.71G [00:19<00:20, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  49%|█████▉      | 839M/1.71G [00:19<00:21, 41.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  50%|█████▉      | 849M/1.71G [00:20<00:20, 41.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  50%|██████      | 860M/1.71G [00:20<00:20, 42.0MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  51%|██████      | 870M/1.71G [00:20<00:19, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  52%|██████▏     | 881M/1.71G [00:20<00:19, 41.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  52%|██████▎     | 891M/1.71G [00:21<00:19, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  53%|██████▎     | 902M/1.71G [00:21<00:19, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  53%|██████▍     | 912M/1.71G [00:21<00:18, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  54%|██████▍     | 923M/1.71G [00:21<00:19, 41.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  55%|██████▌     | 933M/1.71G [00:22<00:18, 41.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  55%|██████▋     | 944M/1.71G [00:22<00:18, 41.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  56%|██████▋     | 954M/1.71G [00:22<00:17, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  56%|██████▊     | 965M/1.71G [00:22<00:17, 41.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  57%|██████▊     | 975M/1.71G [00:23<00:17, 41.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  58%|██████▉     | 986M/1.71G [00:23<00:17, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  58%|██████▉     | 996M/1.71G [00:23<00:16, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  59%|██████▍    | 1.01G/1.71G [00:23<00:16, 41.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  60%|██████▌    | 1.02G/1.71G [00:24<00:16, 41.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  60%|██████▌    | 1.03G/1.71G [00:24<00:16, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  61%|██████▋    | 1.04G/1.71G [00:24<00:15, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  61%|██████▊    | 1.05G/1.71G [00:24<00:15, 41.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  62%|██████▊    | 1.06G/1.71G [00:25<00:15, 42.0MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  63%|██████▉    | 1.07G/1.71G [00:25<00:15, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  63%|██████▉    | 1.08G/1.71G [00:25<00:14, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  64%|███████    | 1.09G/1.71G [00:25<00:14, 41.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  64%|███████    | 1.10G/1.71G [00:26<00:14, 41.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  65%|███████▏   | 1.11G/1.71G [00:26<00:14, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  66%|███████▏   | 1.12G/1.71G [00:26<00:13, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  66%|███████▎   | 1.13G/1.71G [00:26<00:13, 41.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  67%|███████▎   | 1.14G/1.71G [00:27<00:13, 41.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  68%|███████▍   | 1.15G/1.71G [00:27<00:13, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  68%|███████▍   | 1.16G/1.71G [00:27<00:12, 42.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  69%|███████▌   | 1.17G/1.71G [00:27<00:12, 41.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  69%|███████▋   | 1.18G/1.71G [00:28<00:12, 41.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  70%|███████▋   | 1.20G/1.71G [00:28<00:12, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  71%|███████▊   | 1.21G/1.71G [00:28<00:11, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  71%|███████▊   | 1.22G/1.71G [00:28<00:11, 41.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  72%|███████▉   | 1.23G/1.71G [00:29<00:11, 41.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  72%|███████▉   | 1.24G/1.71G [00:29<00:13, 35.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  73%|████████   | 1.25G/1.71G [00:29<00:12, 37.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  74%|████████   | 1.26G/1.71G [00:30<00:11, 38.6MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  74%|████████▏  | 1.27G/1.71G [00:30<00:11, 39.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  75%|████████▏  | 1.28G/1.71G [00:30<00:10, 40.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  75%|████████▎  | 1.29G/1.71G [00:30<00:10, 41.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  76%|████████▎  | 1.30G/1.71G [00:31<00:09, 41.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  77%|████████▍  | 1.31G/1.71G [00:31<00:09, 42.0MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  77%|████████▌  | 1.32G/1.71G [00:31<00:09, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  78%|████████▌  | 1.33G/1.71G [00:31<00:08, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  79%|████████▋  | 1.34G/1.71G [00:32<00:08, 42.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  79%|████████▋  | 1.35G/1.71G [00:32<00:08, 42.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  80%|████████▊  | 1.36G/1.71G [00:32<00:08, 42.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  80%|████████▊  | 1.37G/1.71G [00:32<00:07, 42.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  81%|████████▉  | 1.38G/1.71G [00:33<00:07, 42.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  82%|████████▉  | 1.39G/1.71G [00:33<00:07, 42.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  82%|█████████  | 1.41G/1.71G [00:33<00:07, 42.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  83%|█████████  | 1.42G/1.71G [00:33<00:06, 42.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  83%|█████████▏ | 1.43G/1.71G [00:34<00:06, 42.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  84%|█████████▏ | 1.44G/1.71G [00:34<00:06, 42.6MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  85%|█████████▎ | 1.45G/1.71G [00:34<00:06, 42.6MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  85%|█████████▍ | 1.46G/1.71G [00:34<00:05, 42.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  86%|█████████▍ | 1.47G/1.71G [00:34<00:05, 42.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  87%|█████████▌ | 1.48G/1.71G [00:35<00:05, 42.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  87%|█████████▌ | 1.49G/1.71G [00:35<00:05, 42.6MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  88%|█████████▋ | 1.50G/1.71G [00:35<00:04, 42.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  88%|█████████▋ | 1.51G/1.71G [00:35<00:04, 42.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  89%|█████████▊ | 1.52G/1.71G [00:36<00:04, 42.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  90%|█████████▊ | 1.53G/1.71G [00:36<00:04, 42.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  90%|█████████▉ | 1.54G/1.71G [00:36<00:03, 42.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  91%|█████████▉ | 1.55G/1.71G [00:36<00:03, 42.6MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  91%|██████████ | 1.56G/1.71G [00:37<00:03, 42.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  92%|██████████▏| 1.57G/1.71G [00:37<00:03, 42.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  93%|██████████▏| 1.58G/1.71G [00:37<00:02, 42.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  93%|██████████▎| 1.59G/1.71G [00:37<00:02, 42.5MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  94%|██████████▎| 1.60G/1.71G [00:38<00:02, 42.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  95%|██████████▍| 1.61G/1.71G [00:38<00:02, 42.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  95%|██████████▍| 1.63G/1.71G [00:38<00:01, 42.7MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  96%|██████████▌| 1.64G/1.71G [00:38<00:01, 41.8MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  96%|██████████▌| 1.65G/1.71G [00:39<00:01, 42.2MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  97%|██████████▋| 1.66G/1.71G [00:39<00:01, 42.4MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  98%|██████████▋| 1.67G/1.71G [00:39<00:00, 42.3MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  98%|██████████▊| 1.68G/1.71G [00:39<00:00, 41.6MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  99%|██████████▊| 1.69G/1.71G [00:40<00:00, 41.9MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf:  99%|██████████▉| 1.70G/1.71G [00:40<00:00, 42.1MB/s]\u001b[A\ngemma-2-2b-it-Q4_K_M.gguf: 100%|███████████| 1.71G/1.71G [00:40<00:00, 42.0MB/s]\u001b[A\nDownload complete. Moving file to gemma-2-2b-it-Q4_K_M.gguf\nFetching 1 files: 100%|███████████████████████████| 1/1 [00:40<00:00, 40.96s/it]\n/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install llama-cpp-python -q","metadata":{"execution":{"iopub.status.busy":"2024-11-08T23:43:59.247486Z","iopub.execute_input":"2024-11-08T23:43:59.248032Z","iopub.status.idle":"2024-11-08T23:46:44.847915Z","shell.execute_reply.started":"2024-11-08T23:43:59.247968Z","shell.execute_reply":"2024-11-08T23:46:44.846021Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"from llama_cpp import Llama\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-11-08T23:50:33.804790Z","iopub.execute_input":"2024-11-08T23:50:33.806133Z","iopub.status.idle":"2024-11-08T23:50:33.816705Z","shell.execute_reply.started":"2024-11-08T23:50:33.806078Z","shell.execute_reply":"2024-11-08T23:50:33.815498Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = Llama(model_path=\"./gemma-2-2b-it-Q4_K_M.gguf\", embedding=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T23:57:50.766191Z","iopub.execute_input":"2024-11-08T23:57:50.767157Z","iopub.status.idle":"2024-11-08T23:57:51.826951Z","shell.execute_reply.started":"2024-11-08T23:57:50.767099Z","shell.execute_reply":"2024-11-08T23:57:51.825792Z"},"_kg_hide-output":true,"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 39 key-value pairs and 288 tensors from ./gemma-2-2b-it-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-2\nllama_model_loader: - kv   5:                         general.size_label str              = 2B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\nllama_model_loader: - kv   8:                      gemma2.context_length u32              = 8192\nllama_model_loader: - kv   9:                    gemma2.embedding_length u32              = 2304\nllama_model_loader: - kv  10:                         gemma2.block_count u32              = 26\nllama_model_loader: - kv  11:                 gemma2.feed_forward_length u32              = 9216\nllama_model_loader: - kv  12:                gemma2.attention.head_count u32              = 8\nllama_model_loader: - kv  13:             gemma2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  14:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                gemma2.attention.key_length u32              = 256\nllama_model_loader: - kv  16:              gemma2.attention.value_length u32              = 256\nllama_model_loader: - kv  17:                          general.file_type u32              = 15\nllama_model_loader: - kv  18:              gemma2.attn_logit_softcapping f32              = 50.000000\nllama_model_loader: - kv  19:             gemma2.final_logit_softcapping f32              = 30.000000\nllama_model_loader: - kv  20:            gemma2.attention.sliding_window u32              = 4096\nllama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\nllama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  34:               general.quantization_version u32              = 2\nllama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/gemma-2-2b-it-GGUF/gemma-...\nllama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 182\nllama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 128\nllama_model_loader: - type  f32:  105 tensors\nllama_model_loader: - type q4_K:  156 tensors\nllama_model_loader: - type q6_K:   27 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 249\nllm_load_vocab: token to piece cache size = 1.6014 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = gemma2\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 256000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 2304\nllm_load_print_meta: n_layer          = 26\nllm_load_print_meta: n_head           = 8\nllm_load_print_meta: n_head_kv        = 4\nllm_load_print_meta: n_rot            = 256\nllm_load_print_meta: n_swa            = 4096\nllm_load_print_meta: n_embd_head_k    = 256\nllm_load_print_meta: n_embd_head_v    = 256\nllm_load_print_meta: n_gqa            = 2\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 9216\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 2B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 2.61 B\nllm_load_print_meta: model size       = 1.59 GiB (5.21 BPW) \nllm_load_print_meta: general.name     = Gemma 2 2b It\nllm_load_print_meta: BOS token        = 2 '<bos>'\nllm_load_print_meta: EOS token        = 1 '<eos>'\nllm_load_print_meta: UNK token        = 3 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<pad>'\nllm_load_print_meta: LF token         = 227 '<0x0A>'\nllm_load_print_meta: EOT token        = 107 '<end_of_turn>'\nllm_load_print_meta: EOG token        = 1 '<eos>'\nllm_load_print_meta: EOG token        = 107 '<end_of_turn>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.13 MiB\nllm_load_tensors:        CPU buffer size =  1623.67 MiB\n..........................................................\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =    52.00 MiB\nllama_new_context_with_model: KV self size  =   52.00 MiB, K (f16):   26.00 MiB, V (f16):   26.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.01 MiB\nllama_new_context_with_model:        CPU compute buffer size =   504.50 MiB\nllama_new_context_with_model: graph nodes  = 1050\nllama_new_context_with_model: graph splits = 1\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.entries_count': '182', 'general.quantization_version': '2', 'quantize.imatrix.file': '/models_out/gemma-2-2b-it-GGUF/gemma-2-2b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'gemma2.attention.head_count': '8', 'gemma2.feed_forward_length': '9216', 'gemma2.block_count': '26', 'tokenizer.ggml.pre': 'default', 'general.license': 'gemma', 'general.type': 'model', 'gemma2.embedding_length': '2304', 'general.basename': 'gemma-2', 'tokenizer.ggml.padding_token_id': '0', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'general.finetune': 'it', 'general.file_type': '15', 'gemma2.attention.sliding_window': '4096', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '4', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.model': 'llama', 'general.name': 'Gemma 2 2b It', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'general.size_label': '2B', 'tokenizer.ggml.add_bos_token': 'true'}\nAvailable chat formats from metadata: chat_template.default\nUsing gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}\nUsing chat eos_token: <eos>\nUsing chat bos_token: <bos>\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nout = model.create_embedding([\"I like apple and i don't like banana.\"])\nnp.array(out['data'][0]['embedding']).shape","metadata":{"execution":{"iopub.status.busy":"2024-11-09T00:16:27.789881Z","iopub.execute_input":"2024-11-09T00:16:27.790845Z","iopub.status.idle":"2024-11-09T00:16:28.978316Z","shell.execute_reply.started":"2024-11-09T00:16:27.790790Z","shell.execute_reply":"2024-11-09T00:16:28.977095Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stderr","text":"llama_perf_context_print:        load time =    1167.91 ms\nllama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =    1154.01 ms /    13 tokens\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 4.28 s, sys: 17.6 ms, total: 4.3 s\nWall time: 1.18 s\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"(12, 2304)"},"metadata":{}}]},{"cell_type":"code","source":"%%time\nout = model.create_embedding(['Hello'])\nnp.array(out['data'][0]['embedding']).shape","metadata":{"execution":{"iopub.status.busy":"2024-11-09T00:09:14.040246Z","iopub.execute_input":"2024-11-09T00:09:14.040770Z","iopub.status.idle":"2024-11-09T00:09:14.317072Z","shell.execute_reply.started":"2024-11-09T00:09:14.040722Z","shell.execute_reply":"2024-11-09T00:09:14.315821Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"llama_perf_context_print:        load time =    1167.91 ms\nllama_perf_context_print: prompt eval time =       0.00 ms /     2 tokens (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =     255.10 ms /     3 tokens\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 990 ms, sys: 3.91 ms, total: 994 ms\nWall time: 268 ms\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"(2, 2304)"},"metadata":{}}]}]}