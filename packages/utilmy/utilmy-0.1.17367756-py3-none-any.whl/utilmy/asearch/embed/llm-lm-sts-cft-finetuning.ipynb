{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2880ede",
   "metadata": {
    "papermill": {
     "duration": 0.006677,
     "end_time": "2024-11-05T00:18:44.732298",
     "exception": false,
     "start_time": "2024-11-05T00:18:44.725621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554379a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:18:46.520280Z",
     "iopub.status.busy": "2024-11-05T00:18:46.519958Z",
     "iopub.status.idle": "2024-11-05T00:18:47.484661Z",
     "shell.execute_reply": "2024-11-05T00:18:47.483432Z"
    },
    "papermill": {
     "duration": 0.974287,
     "end_time": "2024-11-05T00:18:47.486874",
     "exception": false,
     "start_time": "2024-11-05T00:18:46.512587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get kaggle secret ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/secret'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load .env ...\n",
      "Set: HF_TOKEN\n",
      "Set: GITHUB_TOKEN\n",
      "Set: KAGGLE_USERNAME\n",
      "Set: KAGGLE_KEY\n",
      "Set: WANDB_SECRET\n",
      "Ignore: # Speech Enhancement Project, invalid syntax\n",
      "Ignore: SE_SERVICE_ACCOUNT_JSON, invalid syntax\n",
      "Remove repo ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_TOKEN'] = \"...\"\n",
    "os.environ['WANDB_API_KEY'] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69053a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:18:44.745407Z",
     "iopub.status.busy": "2024-11-05T00:18:44.745068Z",
     "iopub.status.idle": "2024-11-05T00:18:46.503816Z",
     "shell.execute_reply": "2024-11-05T00:18:46.502781Z"
    },
    "papermill": {
     "duration": 1.767423,
     "end_time": "2024-11-05T00:18:46.505754",
     "exception": false,
     "start_time": "2024-11-05T00:18:44.738331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=os.environ['WANDB_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a744af2",
   "metadata": {
    "papermill": {
     "duration": 0.007893,
     "end_time": "2024-11-05T00:18:47.501767",
     "exception": false,
     "start_time": "2024-11-05T00:18:47.493874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Clone repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b225235b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-05T00:18:47.516116Z",
     "iopub.status.busy": "2024-11-05T00:18:47.515807Z",
     "iopub.status.idle": "2024-11-05T00:18:51.949932Z",
     "shell.execute_reply": "2024-11-05T00:18:51.948845Z"
    },
    "papermill": {
     "duration": 4.44408,
     "end_time": "2024-11-05T00:18:51.952401",
     "exception": false,
     "start_time": "2024-11-05T00:18:47.508321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle\n",
      "Cloning into 'Language-Model-STS-CFT'...\r\n",
      "remote: Enumerating objects: 494, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (175/175), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (109/109), done.\u001b[K\r\n",
      "remote: Total 494 (delta 102), reused 66 (delta 66), pack-reused 319 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (494/494), 80.05 KiB | 5.34 MiB/s, done.\r\n",
      "Resolving deltas: 100% (248/248), done.\r\n"
     ]
    }
   ],
   "source": [
    "PROJ_DIR = \"/kaggle/Language-Model-STS-CFT\"\n",
    "\n",
    "%cd /kaggle\n",
    "!git clone https://github.com/trapoom555/Language-Model-STS-CFT\n",
    "\n",
    "!mkdir $PROJ_DIR/pretrained\n",
    "!mkdir /kaggle/working/out -p\n",
    "!ln -s /kaggle/working/out $PROJ_DIR/train/output\n",
    "# !mkdir $PROJ_DIR/train/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f9036",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working\n",
    "!curl -L -o archive.zip https://www.kaggle.com/api/v1/datasets/download/hahunavth/jsnli-for-simcse\n",
    "!unzip archive.zip -d jsnli-for-simcse\n",
    "!rm archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04bf5c7",
   "metadata": {
    "papermill": {
     "duration": 0.007249,
     "end_time": "2024-11-05T00:18:51.967608",
     "exception": false,
     "start_time": "2024-11-05T00:18:51.960359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Download pretrained and change a tokenizer setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1538092c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:18:51.983833Z",
     "iopub.status.busy": "2024-11-05T00:18:51.983492Z",
     "iopub.status.idle": "2024-11-05T00:18:51.988044Z",
     "shell.execute_reply": "2024-11-05T00:18:51.987202Z"
    },
    "papermill": {
     "duration": 0.01502,
     "end_time": "2024-11-05T00:18:51.989916",
     "exception": false,
     "start_time": "2024-11-05T00:18:51.974896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# openbmb/MiniCPM-2B-dpo-bf16\n",
    "\n",
    "# %cd $PROJ_DIR/pretrained\n",
    "# !git clone https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16\n",
    "\n",
    "# pt_tokenizer_config_file = f\"{PROJ_DIR}/pretrained/MiniCPM-2B-dpo-bf16/tokenizer_config.json\"\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(pt_tokenizer_config_file, \"r\") as f:\n",
    "#     config = json.loads(f.read())\n",
    "#     config['add_eos_token'] = True\n",
    "\n",
    "# with open(pt_tokenizer_config_file, \"w\") as f:\n",
    "#     json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "973e7ba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:18:52.005717Z",
     "iopub.status.busy": "2024-11-05T00:18:52.005417Z",
     "iopub.status.idle": "2024-11-05T00:18:52.009412Z",
     "shell.execute_reply": "2024-11-05T00:18:52.008614Z"
    },
    "papermill": {
     "duration": 0.013932,
     "end_time": "2024-11-05T00:18:52.011228",
     "exception": false,
     "start_time": "2024-11-05T00:18:51.997296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # google/gemma-2b-it\n",
    "\n",
    "# %cd $PROJ_DIR/pretrained\n",
    "# HF_TOKEN = os.environ['HF_TOKEN']\n",
    "# !git clone https://hahaunavth:$HF_TOKEN@huggingface.co/google/gemma-2b-it\n",
    "\n",
    "# pt_tokenizer_config_file = f\"{PROJ_DIR}/pretrained/gemma-2b-it/tokenizer_config.json\"\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(pt_tokenizer_config_file, \"r\") as f:\n",
    "#     config = json.loads(f.read())\n",
    "#     config['add_eos_token'] = True\n",
    "\n",
    "# with open(pt_tokenizer_config_file, \"w\") as f:\n",
    "#     json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b83a5cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:18:52.027566Z",
     "iopub.status.busy": "2024-11-05T00:18:52.026797Z",
     "iopub.status.idle": "2024-11-05T00:18:52.031005Z",
     "shell.execute_reply": "2024-11-05T00:18:52.030343Z"
    },
    "papermill": {
     "duration": 0.014231,
     "end_time": "2024-11-05T00:18:52.032893",
     "exception": false,
     "start_time": "2024-11-05T00:18:52.018662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # google/gemma-2-2b-it\n",
    "\n",
    "# %cd $PROJ_DIR/pretrained\n",
    "# HF_TOKEN = os.environ['HF_TOKEN']\n",
    "# !git clone https://hahaunavth:$HF_TOKEN@huggingface.co/google/gemma-2-2b-it\n",
    "\n",
    "# pt_tokenizer_config_file = f\"{PROJ_DIR}/pretrained/gemma-2-2b-it/tokenizer_config.json\"\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(pt_tokenizer_config_file, \"r\") as f:\n",
    "#     config = json.loads(f.read())\n",
    "#     config['add_eos_token'] = True\n",
    "\n",
    "# with open(pt_tokenizer_config_file, \"w\") as f:\n",
    "#     json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6300723d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:18:52.049007Z",
     "iopub.status.busy": "2024-11-05T00:18:52.048749Z",
     "iopub.status.idle": "2024-11-05T00:21:08.717895Z",
     "shell.execute_reply": "2024-11-05T00:21:08.716496Z"
    },
    "papermill": {
     "duration": 136.680104,
     "end_time": "2024-11-05T00:21:08.720416",
     "exception": false,
     "start_time": "2024-11-05T00:18:52.040312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/Language-Model-STS-CFT/pretrained\n",
      "Cloning into 'gemma-2-2b-jpn-it'...\r\n",
      "remote: Enumerating objects: 38, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (34/34), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (34/34), done.\u001b[K\r\n",
      "remote: Total 38 (delta 12), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\r\n",
      "Unpacking objects: 100% (38/38), 24.85 KiB | 1.55 MiB/s, done.\r\n",
      "Filtering content: 100% (4/4), 911.25 MiB | 6.91 MiB/s, done.\r\n",
      "Encountered 1 file(s) that may not have been copied correctly on Windows:\r\n",
      "\tmodel-00001-of-00002.safetensors\r\n",
      "\r\n",
      "See: `git lfs help smudge` for more details.\r\n"
     ]
    }
   ],
   "source": [
    "# google/gemma-2-2b-jpn-it\n",
    "\n",
    "%cd $PROJ_DIR/pretrained\n",
    "HF_TOKEN = os.environ['HF_TOKEN']\n",
    "!git clone https://hahaunavth:$HF_TOKEN@huggingface.co/google/gemma-2-2b-jpn-it\n",
    "\n",
    "pt_tokenizer_config_file = f\"{PROJ_DIR}/pretrained/gemma-2-2b-jpn-it/tokenizer_config.json\"\n",
    "\n",
    "import json\n",
    "\n",
    "with open(pt_tokenizer_config_file, \"r\") as f:\n",
    "    config = json.loads(f.read())\n",
    "    config['add_eos_token'] = True\n",
    "\n",
    "with open(pt_tokenizer_config_file, \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05308ce",
   "metadata": {
    "papermill": {
     "duration": 0.008679,
     "end_time": "2024-11-05T00:21:08.738660",
     "exception": false,
     "start_time": "2024-11-05T00:21:08.729981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b720fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:21:08.762568Z",
     "iopub.status.busy": "2024-11-05T00:21:08.762031Z",
     "iopub.status.idle": "2024-11-05T00:21:08.771226Z",
     "shell.execute_reply": "2024-11-05T00:21:08.770149Z"
    },
    "papermill": {
     "duration": 0.025424,
     "end_time": "2024-11-05T00:21:08.773340",
     "exception": false,
     "start_time": "2024-11-05T00:21:08.747916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/Language-Model-STS-CFT/data\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/Language-Model-STS-CFT/data\n",
    "\n",
    "# ENGLISH DATASET\n",
    "# !./download_nli.sh\n",
    "# !python nli_preprocess.py\n",
    "\n",
    "# # MU-Kindai/datasets-for-JCSE\n",
    "# !git clone https://huggingface.co/datasets/MU-Kindai/datasets-for-JCSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220cff6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:21:08.852108Z",
     "iopub.status.busy": "2024-11-05T00:21:08.851786Z",
     "iopub.status.idle": "2024-11-05T00:21:15.824637Z",
     "shell.execute_reply": "2024-11-05T00:21:15.823764Z"
    },
    "papermill": {
     "duration": 6.98475,
     "end_time": "2024-11-05T00:21:15.826926",
     "exception": false,
     "start_time": "2024-11-05T00:21:08.842176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/working/jsnli-for-simcse/jsnli_for_simcse.csv\")\n",
    "df['sent0'] = df['sent0'].apply(lambda x: x.replace(' ', \"\"))\n",
    "df['sent1'] = df['sent1'].apply(lambda x: x.replace(' ', \"\"))\n",
    "df['hard_neg'] = df['hard_neg'].apply(lambda x: x.replace(' ', \"\"))\n",
    "\n",
    "df.to_csv(\"jsnli_for_simcse_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86b2b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:21:15.846912Z",
     "iopub.status.busy": "2024-11-05T00:21:15.846623Z",
     "iopub.status.idle": "2024-11-05T00:21:15.854037Z",
     "shell.execute_reply": "2024-11-05T00:21:15.853009Z"
    },
    "papermill": {
     "duration": 0.019836,
     "end_time": "2024-11-05T00:21:15.856062",
     "exception": false,
     "start_time": "2024-11-05T00:21:15.836226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing nli_preprocess_jcse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nli_preprocess_jcse.py\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class NLIPreprocess:\n",
    "    def __init__(self, path, tokenizer_path):\n",
    "        self.ds = load_dataset(\"csv\", data_files=path)['train']\n",
    "\n",
    "#         tokenizer_path = '../pretrained/MiniCPM-2B-dpo-bf16/'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self._preprocess()\n",
    "\n",
    "    def _tokenize(self, text, id):\n",
    "\n",
    "        out = self.tokenizer(text,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=150)\n",
    "\n",
    "        out[id + '_input_ids'] = out.pop('input_ids')\n",
    "        out[id + '_attention_mask'] = out.pop('attention_mask')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _preprocess(self):\n",
    "        self.ds = self.ds.map(\n",
    "            lambda x: self._tokenize(x['sent0'], 'sent0'), batched=True)\n",
    "        self.ds = self.ds.map(\n",
    "            lambda x: self._tokenize(x['sent1'], 'sent1'), batched=True)\n",
    "        self.ds = self.ds.map(\n",
    "            lambda x: self._tokenize(x['hard_neg'], 'hard_neg'), batched=True)\n",
    "        self.ds.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"sent0_input_ids\", \"sent0_attention_mask\",\n",
    "                     \"sent1_input_ids\", \"sent1_attention_mask\",\n",
    "                     \"hard_neg_input_ids\", \"hard_neg_attention_mask\",]\n",
    "        )\n",
    "\n",
    "# nlip = NLIPreprocess('./datasets-for-JCSE/clinic_shuffle_for_simcse_top5_filtered.csv', tokenizer_path='../pretrained/gemma-2-2b-jpn-it/')\n",
    "# nlip = NLIPreprocess('./datasets-for-JCSE/nli_for_simcse.csv', tokenizer_path='../pretrained/gemma-2-2b-jpn-it/')\n",
    "# nlip = NLIPreprocess('./nli_for_simcse.csv', tokenizer_path='../pretrained/gemma-2b-it/')\n",
    "# nlip = NLIPreprocess('./nli_for_simcse.csv', tokenizer_path='../pretrained/gemma-2-2b-it/')\n",
    "# nlip = NLIPreprocess('./nli_for_simcse.csv', tokenizer_path='../pretrained/gemma-2-2b-jpn-it/')\n",
    "# nlip = NLIPreprocess('/kaggle/working/jsnli-for-simcse/jsnli_for_simcse.csv', tokenizer_path='../pretrained/gemma-2-2b-jpn-it/')\n",
    "\n",
    "nlip = NLIPreprocess('./jsnli_for_simcse_cleaned.csv', tokenizer_path='../pretrained/gemma-2-2b-jpn-it/')\n",
    "\n",
    "nlip.ds.save_to_disk(\"./processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbb93968",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:21:15.874519Z",
     "iopub.status.busy": "2024-11-05T00:21:15.874224Z",
     "iopub.status.idle": "2024-11-05T00:22:17.324597Z",
     "shell.execute_reply": "2024-11-05T00:22:17.323617Z"
    },
    "papermill": {
     "duration": 61.462295,
     "end_time": "2024-11-05T00:22:17.327022",
     "exception": false,
     "start_time": "2024-11-05T00:21:15.864727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train split: 100000 examples [00:00, 175654.70 examples/s]\r\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:15<00:00, 6473.87 examples/s]\r\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:16<00:00, 6025.42 examples/s]\r\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:16<00:00, 6137.95 examples/s]\r\n",
      "Saving the dataset (2/2 shards): 100%|‚ñà| 100000/100000 [00:00<00:00, 121895.62 e\r\n"
     ]
    }
   ],
   "source": [
    "!python nli_preprocess_jcse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e876d3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:22:17.399231Z",
     "iopub.status.busy": "2024-11-05T00:22:17.398845Z",
     "iopub.status.idle": "2024-11-05T00:22:19.945624Z",
     "shell.execute_reply": "2024-11-05T00:22:19.944676Z"
    },
    "papermill": {
     "duration": 2.58644,
     "end_time": "2024-11-05T00:22:19.947868",
     "exception": false,
     "start_time": "2024-11-05T00:22:17.361428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent0_input_ids': tensor([     1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      2,  24533, 235418,\n",
       "         197773, 235432,  71722,   5409, 235362,      1]),\n",
       " 'sent0_attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]),\n",
       " 'sent1_input_ids': tensor([     1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      2, 193067, 235398, 197773, 235432,  71722,\n",
       "          46034,  56985, 235372,  24533, 235362,      1]),\n",
       " 'sent1_attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]),\n",
       " 'hard_neg_input_ids': tensor([     1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      2,  74820, 235395, 222283,\n",
       "          96425,  24533, 235425,   5409, 235362,      1]),\n",
       " 'hard_neg_attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(f\"{PROJ_DIR}/data/processed\")\n",
    "ds[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "915512a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:22:20.016426Z",
     "iopub.status.busy": "2024-11-05T00:22:20.015928Z",
     "iopub.status.idle": "2024-11-05T00:22:21.668226Z",
     "shell.execute_reply": "2024-11-05T00:22:21.667081Z"
    },
    "papermill": {
     "duration": 1.68894,
     "end_time": "2024-11-05T00:22:21.670508",
     "exception": false,
     "start_time": "2024-11-05T00:22:19.981568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><bos>ÊÇ≤„Åó„ÅÑÈ°î„Çí„Åó„ÅüÔºï‰∫∫„Åå„É¨„Çπ„Éà„É©„É≥„Å´ÁùÄ„Åè„ÅÆ„ÇíÂæÖ„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ<eos>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../pretrained/gemma-2-2b-jpn-it/', local_files_only=True)\n",
    "tokenizer.decode(ds[245]['hard_neg_input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19071214",
   "metadata": {
    "papermill": {
     "duration": 0.033829,
     "end_time": "2024-11-05T00:22:21.738283",
     "exception": false,
     "start_time": "2024-11-05T00:22:21.704454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00871010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:22:21.808181Z",
     "iopub.status.busy": "2024-11-05T00:22:21.807609Z",
     "iopub.status.idle": "2024-11-05T00:22:21.811967Z",
     "shell.execute_reply": "2024-11-05T00:22:21.811106Z"
    },
    "papermill": {
     "duration": 0.041521,
     "end_time": "2024-11-05T00:22:21.813799",
     "exception": false,
     "start_time": "2024-11-05T00:22:21.772278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cat $PROJ_DIR/train/configs/ddp_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc35f715",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:22:21.881988Z",
     "iopub.status.busy": "2024-11-05T00:22:21.881661Z",
     "iopub.status.idle": "2024-11-05T00:22:21.887262Z",
     "shell.execute_reply": "2024-11-05T00:22:21.886452Z"
    },
    "papermill": {
     "duration": 0.042222,
     "end_time": "2024-11-05T00:22:21.889346",
     "exception": false,
     "start_time": "2024-11-05T00:22:21.847124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/Language-Model-STS-CFT/train/configs/ddp_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/Language-Model-STS-CFT/train/configs/ddp_config.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "distributed_type: MULTI_GPU\n",
    "downcast_bf16: 'no'\n",
    "enable_cpu_affinity: false\n",
    "gpu_ids: all\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: bf16\n",
    "num_machines: 1\n",
    "num_processes: 2 # 4\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b17dfd6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:22:21.959591Z",
     "iopub.status.busy": "2024-11-05T00:22:21.958982Z",
     "iopub.status.idle": "2024-11-05T00:22:35.213002Z",
     "shell.execute_reply": "2024-11-05T00:22:35.211924Z"
    },
    "papermill": {
     "duration": 13.291703,
     "end_time": "2024-11-05T00:22:35.215499",
     "exception": false,
     "start_time": "2024-11-05T00:22:21.923796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install peft -q # ==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8965153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:22:35.286376Z",
     "iopub.status.busy": "2024-11-05T00:22:35.285993Z",
     "iopub.status.idle": "2024-11-05T00:22:35.294105Z",
     "shell.execute_reply": "2024-11-05T00:22:35.293015Z"
    },
    "papermill": {
     "duration": 0.045641,
     "end_time": "2024-11-05T00:22:35.295990",
     "exception": false,
     "start_time": "2024-11-05T00:22:35.250349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/Language-Model-STS-CFT/train\n"
     ]
    }
   ],
   "source": [
    "%cd $PROJ_DIR/train\n",
    "# !chmod +x train.sh\n",
    "# !./train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa97e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:22:35.366078Z",
     "iopub.status.busy": "2024-11-05T00:22:35.365726Z",
     "iopub.status.idle": "2024-11-05T00:22:35.373546Z",
     "shell.execute_reply": "2024-11-05T00:22:35.372653Z"
    },
    "papermill": {
     "duration": 0.045005,
     "end_time": "2024-11-05T00:22:35.375358",
     "exception": false,
     "start_time": "2024-11-05T00:22:35.330353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from typing import Optional\n",
    "from datasets import load_from_disk\n",
    "from dataclasses import dataclass, field\n",
    "from contrastive_trainer import ContrastiveTrainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, HfArgumentParser, set_seed\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    lora_alpha: Optional[int] = field(default=32)\n",
    "    lora_dropout: Optional[float] = field(default=0.1)\n",
    "    lora_r: Optional[int] = field(default=8)\n",
    "    lora_target_modules: Optional[str] = field(\n",
    "        default=\"q_proj,v_proj\",\n",
    "        metadata={\"help\": \"comma separated list of target modules to apply LoRA layers to\"},\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    train_data_path: str = field(\n",
    "        metadata={\"help\": \"Path to training data\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    temperature: Optional[float] = field(default=0.05)\n",
    "\n",
    "def main(model_args, data_args, training_args):\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path,\n",
    "                                                torch_dtype=torch.bfloat16,\n",
    "                                                trust_remote_code=True,\n",
    "                                                local_files_only=True)\n",
    "\n",
    "    # PEFT\n",
    "    lora_config = LoraConfig(init_lora_weights=\"gaussian\",\n",
    "                            task_type=TaskType.CAUSAL_LM,\n",
    "                            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "                            r=model_args.lora_r,\n",
    "                            lora_alpha=model_args.lora_alpha,\n",
    "                            lora_dropout=model_args.lora_dropout,\n",
    "                            inference_mode=False)\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Data\n",
    "    train_dataset = load_from_disk(data_args.train_data_path)\n",
    "\n",
    "    trainer = ContrastiveTrainer(model=model,\n",
    "                                args=training_args,\n",
    "                                train_dataset=train_dataset)\n",
    "\n",
    "    trainer.accelerator.print(f\"{trainer.model}\")\n",
    "    trainer.model.print_trainable_parameters()\n",
    "\n",
    "    # Train\n",
    "    checkpoint = \"/kaggle/input/llm-gemma-2-2b-jpn-it-finetune/out/20241104132512/checkpoint-500\" # None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "    # Saving final model\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"WANDB_PROJECT\"] = \"minicpm-dense-retrieval\"\n",
    "    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    main(model_args, data_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2925f186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T00:22:35.445067Z",
     "iopub.status.busy": "2024-11-05T00:22:35.444779Z",
     "iopub.status.idle": "2024-11-05T08:29:36.598014Z",
     "shell.execute_reply": "2024-11-05T08:29:36.597125Z"
    },
    "papermill": {
     "duration": 29221.190505,
     "end_time": "2024-11-05T08:29:36.600515",
     "exception": false,
     "start_time": "2024-11-05T00:22:35.410010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20241105002235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.18it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.02it/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Gemma2ForCausalLM(\n",
      "      (model): Gemma2Model(\n",
      "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-25): 26 x Gemma2DecoderLayer(\n",
      "            (self_attn): Gemma2Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "              (rotary_emb): Gemma2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Gemma2MLP(\n",
      "              (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "              (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "              (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "              (act_fn): PytorchGELUTanh()\n",
      "            )\n",
      "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "trainable params: 1,597,440 || all params: 2,615,939,328 || trainable%: 0.0611\n",
      "trainable params: 1,597,440 || all params: 2,615,939,328 || trainable%: 0.0611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from /kaggle/input/llm-gemma-2-2b-jpn-it-finetune/out/20241104132512/checkpoint-500.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3262: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3262: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "***** Running training *****\n",
      "  Num examples = 100,000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 1,000\n",
      "  Number of trainable parameters = 1,597,440\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 500\n",
      "  Will skip the first 0 epochs then the first 4000 batches in the first epoch.\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2944: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "wandb: Currently logged in as: hahunavth. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /kaggle/Language-Model-STS-CFT/train/wandb/run-20241105_002306-e3bf5zz0\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run output/20241105002235/\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/hahunavth/minicpm-dense-retrieval\n",
      "wandb: üöÄ View run at https://wandb.ai/hahunavth/minicpm-dense-retrieval/runs/e3bf5zz0\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2944: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "[rank0]:[W1105 00:23:57.065450632 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W1105 00:23:57.116515001 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3816, 'grad_norm': 11.453920364379883, 'learning_rate': 2.8479327524001636e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3689, 'grad_norm': 12.42440128326416, 'learning_rate': 2.761321158169134e-05, 'epoch': 0.33}\n",
      "{'loss': 0.4072, 'grad_norm': 10.065420150756836, 'learning_rate': 2.674391184360313e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3675, 'grad_norm': 10.48132610321045, 'learning_rate': 2.587248741756253e-05, 'epoch': 0.35}\n",
      "{'loss': 0.409, 'grad_norm': 13.856084823608398, 'learning_rate': 2.5e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3836, 'grad_norm': 7.561926364898682, 'learning_rate': 2.4127512582437485e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3346, 'grad_norm': 7.291716575622559, 'learning_rate': 2.3256088156396868e-05, 'epoch': 0.36}\n",
      "{'loss': 0.391, 'grad_norm': 9.186484336853027, 'learning_rate': 2.238678841830867e-05, 'epoch': 0.37}\n",
      "{'loss': 0.4405, 'grad_norm': 11.761835098266602, 'learning_rate': 2.1520672475998373e-05, 'epoch': 0.38}\n",
      "{'loss': 0.4327, 'grad_norm': 11.6403226852417, 'learning_rate': 2.0658795558326743e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3403, 'grad_norm': 10.99782657623291, 'learning_rate': 1.980220772955602e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3811, 'grad_norm': 12.592992782592773, 'learning_rate': 1.895195261000831e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3717, 'grad_norm': 12.324170112609863, 'learning_rate': 1.8109066104575023e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3795, 'grad_norm': 11.706480979919434, 'learning_rate': 1.7274575140626318e-05, 'epoch': 0.41}\n",
      "{'loss': 0.431, 'grad_norm': 13.863530158996582, 'learning_rate': 1.6449496416858284e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3723, 'grad_norm': 11.334906578063965, 'learning_rate': 1.56348351646022e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3479, 'grad_norm': 8.668643951416016, 'learning_rate': 1.4831583923104999e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3447, 'grad_norm': 14.234842300415039, 'learning_rate': 1.4040721330273062e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3668, 'grad_norm': 11.432268142700195, 'learning_rate': 1.3263210930352737e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3482, 'grad_norm': 14.1084566116333, 'learning_rate': 1.2500000000000006e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3887, 'grad_norm': 16.57339096069336, 'learning_rate': 1.175201839416988e-05, 'epoch': 0.45}\n",
      "{'loss': 0.371, 'grad_norm': 13.582597732543945, 'learning_rate': 1.1020177413231334e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3549, 'grad_norm': 12.677901268005371, 'learning_rate': 1.0305368692688174e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3664, 'grad_norm': 9.971076965332031, 'learning_rate': 9.608463116858542e-06, 'epoch': 0.47}\n",
      "{'loss': 0.3871, 'grad_norm': 12.568639755249023, 'learning_rate': 8.930309757836517e-06, 'epoch': 0.48}\n",
      "{'loss': 0.3577, 'grad_norm': 9.911385536193848, 'learning_rate': 8.271734841028553e-06, 'epoch': 0.49}\n",
      "{'loss': 0.3629, 'grad_norm': 14.096944808959961, 'learning_rate': 7.633540738525066e-06, 'epoch': 0.49}\n",
      "{'loss': 0.3528, 'grad_norm': 13.515847206115723, 'learning_rate': 7.016504991533726e-06, 'epoch': 0.5}\n",
      "{'loss': 0.3968, 'grad_norm': 8.422048568725586, 'learning_rate': 6.421379363065142e-06, 'epoch': 0.51}\n",
      "{'loss': 0.3463, 'grad_norm': 13.37044906616211, 'learning_rate': 5.848888922025553e-06, 'epoch': 0.51}\n",
      "{'loss': 0.3309, 'grad_norm': 12.662362098693848, 'learning_rate': 5.299731159831953e-06, 'epoch': 0.52}\n",
      "{'loss': 0.3784, 'grad_norm': 12.614615440368652, 'learning_rate': 4.7745751406263165e-06, 'epoch': 0.52}\n",
      "{'loss': 0.3524, 'grad_norm': 15.85264778137207, 'learning_rate': 4.274060686123959e-06, 'epoch': 0.53}\n",
      "{'loss': 0.3191, 'grad_norm': 14.236181259155273, 'learning_rate': 3.798797596089351e-06, 'epoch': 0.54}\n",
      "{'loss': 0.3827, 'grad_norm': 11.909097671508789, 'learning_rate': 3.3493649053890326e-06, 'epoch': 0.54}\n",
      "{'loss': 0.3288, 'grad_norm': 8.787050247192383, 'learning_rate': 2.9263101785268254e-06, 'epoch': 0.55}\n",
      "{'loss': 0.3742, 'grad_norm': 10.517125129699707, 'learning_rate': 2.5301488425208296e-06, 'epoch': 0.56}\n",
      "{'loss': 0.3391, 'grad_norm': 12.453104019165039, 'learning_rate': 2.1613635589349756e-06, 'epoch': 0.56}\n",
      "{'loss': 0.3237, 'grad_norm': 10.38401985168457, 'learning_rate': 1.8204036358303173e-06, 'epoch': 0.57}\n",
      "{'loss': 0.3732, 'grad_norm': 11.708006858825684, 'learning_rate': 1.5076844803522922e-06, 'epoch': 0.58}\n",
      "{'loss': 0.2914, 'grad_norm': 12.030230522155762, 'learning_rate': 1.2235870926211619e-06, 'epoch': 0.58}\n",
      "{'loss': 0.3776, 'grad_norm': 14.503013610839844, 'learning_rate': 9.684576015420278e-07, 'epoch': 0.59}\n",
      "{'loss': 0.345, 'grad_norm': 9.394436836242676, 'learning_rate': 7.426068431000882e-07, 'epoch': 0.6}\n",
      "{'loss': 0.3178, 'grad_norm': 8.103813171386719, 'learning_rate': 5.463099816548579e-07, 'epoch': 0.6}\n",
      "{'loss': 0.3832, 'grad_norm': 14.648049354553223, 'learning_rate': 3.7980617469479953e-07, 'epoch': 0.61}\n",
      "{'loss': 0.3656, 'grad_norm': 10.924678802490234, 'learning_rate': 2.4329828146074095e-07, 'epoch': 0.61}\n",
      "{'loss': 0.3425, 'grad_norm': 14.054830551147461, 'learning_rate': 1.3695261579316777e-07, 'epoch': 0.62}\n",
      "{'loss': 0.4256, 'grad_norm': 12.503904342651367, 'learning_rate': 6.089874350439506e-08, 'epoch': 0.63}\n",
      "{'loss': 0.2939, 'grad_norm': 10.528154373168945, 'learning_rate': 1.522932452260595e-08, 'epoch': 0.63}\n",
      "{'loss': 0.4393, 'grad_norm': 13.329564094543457, 'learning_rate': 0.0, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [8:06:23<00:00, 58.42s/it]Saving model checkpoint to output/20241105002235/checkpoint-1000\n",
      "loading configuration file ../pretrained/gemma-2-2b-jpn-it/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 224,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 29186.6356, 'train_samples_per_second': 2.193, 'train_steps_per_second': 0.034, 'train_loss': 0.1836941692829132, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [8:06:23<00:00, 29.18s/it]\n",
      "Saving model checkpoint to output/20241105002235/\n",
      "loading configuration file ../pretrained/gemma-2-2b-jpn-it/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 224,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "[rank0]:[W1105 08:29:34.266164192 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "formatted_time=$(date +\"%Y%m%d%H%M%S\")\n",
    "echo $formatted_time\n",
    "\n",
    "# --model_name_or_path ../pretrained/MiniCPM-2B-dpo-bf16/ \\\n",
    "# --model_name_or_path ../pretrained/gemma-2-2b-jpn-it/ \\\n",
    "# --model_name_or_path ../pretrained/gemma-2b-it/ \\\n",
    "# --model_name_or_path ../pretrained/gemma-2-2b-it/ \\\n",
    "\n",
    "# --per_device_train_batch_size 4 \\\n",
    "# --gradient_accumulation_steps 1 \\\n",
    "# --max_steps 1000\n",
    "# --learning_rate 5e-5 \\\n",
    "# --weight_decay 1e-4 \\\n",
    "\n",
    "\n",
    "# export CUDA_LAUNCH_BLOCKING=1;\n",
    "# export TORCH_USE_CUDA_DSA=1;\n",
    "accelerate launch --config_file ./configs/ddp_config.yaml train.py \\\n",
    "--output_dir output/$formatted_time/ \\\n",
    "--model_name_or_path ../pretrained/gemma-2-2b-jpn-it/ \\\n",
    "--temperature 0.05 \\\n",
    "--train_data_path ../data/processed \\\n",
    "--learning_rate 5e-5 \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--bf16 \\\n",
    "--gradient_accumulation_steps 8 \\\n",
    "--warmup_steps 100 \\\n",
    "--max_steps 1000 \\\n",
    "--weight_decay 8e-4 \\\n",
    "--lr_scheduler_type \"cosine\" \\\n",
    "--lora_r 8 --lora_alpha 32 --lora_dropout 0.1 \\\n",
    "--save_strategy steps --save_steps 500 --seed 7 \\\n",
    "--remove_unused_columns False \\\n",
    "--log_level info --logging_strategy steps --logging_steps 10 --report_to wandb \\\n",
    "--max_grad_norm 1.0"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5997538,
     "sourceId": 9794355,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5999802,
     "sourceId": 9807760,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29456.575828,
   "end_time": "2024-11-05T08:29:38.579355",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-05T00:18:42.003527",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
