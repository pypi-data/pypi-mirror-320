{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"\"\"\"\n# SET ENV VARIABLE\nNote: This script only works when run on Kaggle notebook directly (not run with !python secret.py)\n\"\"\"\nexec(\"\"\"\\nimport os\\nfrom kaggle_secrets import UserSecretsClient\\n\\nSECRET_REPO_DIR='/secret'\\n\\nuser_secrets = UserSecretsClient()\\nprint(\"Get kaggle secret ...\")\\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\\n\\nos.system(f'rm -rf {SECRET_REPO_DIR};')\\nos.system(f'git clone https://hahunavth:{GITHUB_TOKEN}@github.com/hahunavth/kaggle-secret.git {SECRET_REPO_DIR}')\\n\\nprint(\"Load .env ...\")\\nwith open(f\"{SECRET_REPO_DIR}/kaggle.env\", \"r\") as f:\\n    for line in f.readlines():\\n        line = line.rstrip()\\n        if len(line) == 0:\\n            continue\\n        line = line.split(\"=\")\\n        if len(line) == 2:\\n            name, value = line\\n            print(f\"Set: {name}\")\\n            os.environ[name.rstrip()] = value.rstrip()\\n        elif len(line) > 0:\\n            print(f\"Ignore: {line[0]}, invalid syntax\")\\n\\nprint(\"Remove repo ...\")\\nos.system(f'rm -rf {SECRET_REPO_DIR};')\\nprint(\"Done\")\\n\\nassert os.environ[name]\\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:16:58.559370Z","iopub.execute_input":"2024-11-17T06:16:58.559806Z","iopub.status.idle":"2024-11-17T06:16:59.430595Z","shell.execute_reply.started":"2024-11-17T06:16:58.559764Z","shell.execute_reply":"2024-11-17T06:16:59.429151Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Get kaggle secret ...\n","output_type":"stream"},{"name":"stderr","text":"Cloning into '/secret'...\n","output_type":"stream"},{"name":"stdout","text":"Load .env ...\nSet: HF_TOKEN\nSet: GITHUB_TOKEN\nSet: KAGGLE_USERNAME\nSet: KAGGLE_KEY\nSet: WANDB_SECRET\nIgnore: # Speech Enhancement Project, invalid syntax\nIgnore: SE_SERVICE_ACCOUNT_JSON, invalid syntax\nRemove repo ...\nDone\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install peft -q","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:18:10.566119Z","iopub.execute_input":"2024-11-17T06:18:10.566574Z","iopub.status.idle":"2024-11-17T06:18:27.208668Z","shell.execute_reply.started":"2024-11-17T06:18:10.566531Z","shell.execute_reply":"2024-11-17T06:18:27.207235Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Load model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:21:17.950131Z","iopub.execute_input":"2024-11-17T06:21:17.950691Z","iopub.status.idle":"2024-11-17T06:21:24.007950Z","shell.execute_reply.started":"2024-11-17T06:21:17.950581Z","shell.execute_reply":"2024-11-17T06:21:24.006674Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model_name = \"google/gemma-2-2b-jpn-it\"\nadapter_path = \"hahunavth/gemma-2-2b-jpn-it-text-embedding-cft\"\nbase_model = AutoModelForCausalLM.from_pretrained(model_name)\npeft_model = PeftModel.from_pretrained(base_model, adapter_path)\n\n# # Adapter only\n# adp_dir = \"gemma_adapter\"\n# peft_model.save_pretrained(adp_dir)\n\n# # Merge LoRA weights into the base model\n# merged_dir = \"gemma_merged\"\n# merged_model = peft_model.merge_and_unload()\n# merged_model.save_pretrained(merged_dir)\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# tokenizer.save_pretrained(out_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:39:08.803287Z","iopub.execute_input":"2024-11-17T06:39:08.803794Z","iopub.status.idle":"2024-11-17T06:39:08.810399Z","shell.execute_reply.started":"2024-11-17T06:39:08.803747Z","shell.execute_reply":"2024-11-17T06:39:08.808872Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Install llama cpp","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp.git\n!pip install -r llama.cpp/requirements.txt -q","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:25:07.200961Z","iopub.execute_input":"2024-11-17T06:25:07.201582Z","iopub.status.idle":"2024-11-17T06:26:34.155341Z","shell.execute_reply.started":"2024-11-17T06:25:07.201520Z","shell.execute_reply":"2024-11-17T06:26:34.153619Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Cloning into 'llama.cpp'...\nremote: Enumerating objects: 37522, done.\u001b[K\nremote: Counting objects: 100% (7901/7901), done.\u001b[K\nremote: Compressing objects: 100% (360/360), done.\u001b[K\nremote: Total 37522 (delta 7740), reused 7541 (delta 7541), pack-reused 29621 (from 1)\u001b[K\nReceiving objects: 100% (37522/37522), 59.50 MiB | 22.72 MiB/s, done.\nResolving deltas: 100% (27335/27335), done.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.5 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.5 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntorchaudio 2.4.0+cpu requires torch==2.4.0, but you have torch 2.2.2+cpu which is incompatible.\ntorchvision 0.19.0+cpu requires torch==2.4.0, but you have torch 2.2.2+cpu which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Quantization","metadata":{}},{"cell_type":"code","source":"!python llama.cpp/convert_lora_to_gguf.py -h","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:35:59.121242Z","iopub.execute_input":"2024-11-17T06:35:59.121918Z","iopub.status.idle":"2024-11-17T06:36:04.984591Z","shell.execute_reply.started":"2024-11-17T06:35:59.121860Z","shell.execute_reply":"2024-11-17T06:36:04.983065Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"usage: convert_lora_to_gguf.py [-h] [--outfile OUTFILE]\n                               [--outtype {f32,f16,bf16,q8_0,auto}]\n                               [--bigendian] [--no-lazy] [--verbose]\n                               [--dry-run] [--base BASE]\n                               lora_path\n\nConvert a Hugging Face PEFT LoRA adapter to a GGUF file\n\npositional arguments:\n  lora_path             directory containing Hugging Face PEFT LoRA config\n                        (adapter_model.json) and weights\n                        (adapter_model.safetensors or adapter_model.bin)\n\noptions:\n  -h, --help            show this help message and exit\n  --outfile OUTFILE     path to write to; default: based on input. {ftype}\n                        will be replaced by the outtype.\n  --outtype {f32,f16,bf16,q8_0,auto}\n                        output format - use f32 for float32, f16 for float16,\n                        bf16 for bfloat16, q8_0 for Q8_0, auto for the\n                        highest-fidelity 16-bit float type depending on the\n                        first loaded tensor type\n  --bigendian           model is executed on big endian machine\n  --no-lazy             use more RAM by computing all outputs before writing\n                        (use in case lazy evaluation is broken)\n  --verbose             increase output verbosity\n  --dry-run             only print out what will be done, without writing any\n                        new files\n  --base BASE           directory containing Hugging Face model config files\n                        (config.json, tokenizer.json) for the base model that\n                        the adapter is based on - only config is needed,\n                        actual model weights are not required. If base model\n                        is unspecified, it will be loaded from Hugging Face\n                        hub based on the adapter config\n","output_type":"stream"}]},{"cell_type":"code","source":"!python llama.cpp/convert_hf_to_gguf.py -h","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:36:17.408493Z","iopub.execute_input":"2024-11-17T06:36:17.409087Z","iopub.status.idle":"2024-11-17T06:36:21.461560Z","shell.execute_reply.started":"2024-11-17T06:36:17.409024Z","shell.execute_reply":"2024-11-17T06:36:21.460019Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"usage: convert_hf_to_gguf.py [-h] [--vocab-only] [--outfile OUTFILE]\n                             [--outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}]\n                             [--bigendian] [--use-temp-file] [--no-lazy]\n                             [--model-name MODEL_NAME] [--verbose]\n                             [--split-max-tensors SPLIT_MAX_TENSORS]\n                             [--split-max-size SPLIT_MAX_SIZE] [--dry-run]\n                             [--no-tensor-first-split] [--metadata METADATA]\n                             model\n\nConvert a huggingface model to a GGML compatible file\n\npositional arguments:\n  model                 directory containing model file\n\noptions:\n  -h, --help            show this help message and exit\n  --vocab-only          extract only the vocab\n  --outfile OUTFILE     path to write to; default: based on input. {ftype}\n                        will be replaced by the outtype.\n  --outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}\n                        output format - use f32 for float32, f16 for float16,\n                        bf16 for bfloat16, q8_0 for Q8_0, tq1_0 or tq2_0 for\n                        ternary, and auto for the highest-fidelity 16-bit\n                        float type depending on the first loaded tensor type\n  --bigendian           model is executed on big endian machine\n  --use-temp-file       use the tempfile library while processing (helpful\n                        when running out of memory, process killed)\n  --no-lazy             use more RAM by computing all outputs before writing\n                        (use in case lazy evaluation is broken)\n  --model-name MODEL_NAME\n                        name of the model\n  --verbose             increase output verbosity\n  --split-max-tensors SPLIT_MAX_TENSORS\n                        max tensors in each split\n  --split-max-size SPLIT_MAX_SIZE\n                        max size per split N(M|G)\n  --dry-run             only print out a split plan and exit, without writing\n                        any new files\n  --no-tensor-first-split\n                        do not add tensors to the first split (disabled by\n                        default)\n  --metadata METADATA   Specify the path for an authorship metadata override\n                        file\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Adapter only","metadata":{}},{"cell_type":"code","source":"# Adapter only\nadp_dir = \"gemma_adapter\"\npeft_model.save_pretrained(adp_dir)\n\nbase_dir = \"gemma_base\"\nbase_model.save_pretrained(base_dir)","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:45:00.732136Z","iopub.execute_input":"2024-11-17T06:45:00.732757Z","iopub.status.idle":"2024-11-17T06:45:45.556470Z","shell.execute_reply.started":"2024-11-17T06:45:00.732695Z","shell.execute_reply":"2024-11-17T06:45:45.555101Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"!python llama.cpp/convert_lora_to_gguf.py $adp_dir\\\n  --base $base_dir\\\n  --outfile gemma-2-2b-jpn-it-adp-text-embedding-cft-q8_0.gguf\\\n  --outtype q8_0","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:45:53.085662Z","iopub.execute_input":"2024-11-17T06:45:53.086942Z","iopub.status.idle":"2024-11-17T06:45:59.098275Z","shell.execute_reply.started":"2024-11-17T06:45:53.086844Z","shell.execute_reply":"2024-11-17T06:45:59.096536Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Writing: 0.00byte [00:00, ?byte/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf $adp_dir $base_dir","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:46:16.820530Z","iopub.execute_input":"2024-11-17T06:46:16.821096Z","iopub.status.idle":"2024-11-17T06:46:19.698797Z","shell.execute_reply.started":"2024-11-17T06:46:16.821043Z","shell.execute_reply":"2024-11-17T06:46:19.697104Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"!ls -la","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:46:11.790740Z","iopub.execute_input":"2024-11-17T06:46:11.791347Z","iopub.status.idle":"2024-11-17T06:46:13.222592Z","shell.execute_reply.started":"2024-11-17T06:46:11.791290Z","shell.execute_reply":"2024-11-17T06:46:13.220773Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"total 28\ndrwxr-xr-x  6 root root 4096 Nov 17 06:45 .\ndrwxr-xr-x  5 root root 4096 Nov 17 06:16 ..\ndrwxr-xr-x  2 root root 4096 Nov 17 06:16 .virtual_documents\n-rw-r--r--  1 root root  544 Nov 17 06:45 gemma-2-2b-jpn-it-adp-text-embedding-cft-q8_0.gguf\ndrwxr-xr-x  2 root root 4096 Nov 17 06:45 gemma_adapter\ndrwxr-xr-x  2 root root 4096 Nov 17 06:45 gemma_base\ndrwxr-xr-x 24 root root 4096 Nov 17 06:36 llama.cpp\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Merge adapter and base model","metadata":{}},{"cell_type":"code","source":"# Merge LoRA weights into the base model\nmerged_dir = \"gemma_merged\"\nmerged_model = peft_model.merge_and_unload()\nmerged_model.save_pretrained(merged_dir)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained(out_dir)","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:47:40.755080Z","iopub.execute_input":"2024-11-17T06:47:40.755644Z","iopub.status.idle":"2024-11-17T06:48:23.831548Z","shell.execute_reply.started":"2024-11-17T06:47:40.755590Z","shell.execute_reply":"2024-11-17T06:48:23.829859Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"('gemma_merged/tokenizer_config.json',\n 'gemma_merged/special_tokens_map.json',\n 'gemma_merged/tokenizer.model',\n 'gemma_merged/added_tokens.json',\n 'gemma_merged/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"!python llama.cpp/convert_hf_to_gguf.py $merged_dir\\\n  --outfile gemma-2-2b-jpn-it-text-embedding-cft-q8_0.gguf\\\n  --outtype q8_0","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:48:34.660762Z","iopub.execute_input":"2024-11-17T06:48:34.661395Z","iopub.status.idle":"2024-11-17T06:49:57.822032Z","shell.execute_reply.started":"2024-11-17T06:48:34.661347Z","shell.execute_reply":"2024-11-17T06:49:57.820412Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Writing: 100%|██████████████████████████| 2.78G/2.78G [01:10<00:00, 39.5Mbyte/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf $merged_dir","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:50:05.686677Z","iopub.execute_input":"2024-11-17T06:50:05.687225Z","iopub.status.idle":"2024-11-17T06:50:08.273025Z","shell.execute_reply.started":"2024-11-17T06:50:05.687174Z","shell.execute_reply":"2024-11-17T06:50:08.270548Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"!ls -la","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:50:12.514206Z","iopub.execute_input":"2024-11-17T06:50:12.514740Z","iopub.status.idle":"2024-11-17T06:50:13.964010Z","shell.execute_reply.started":"2024-11-17T06:50:12.514688Z","shell.execute_reply":"2024-11-17T06:50:13.962344Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"total 2719260\ndrwxr-xr-x  4 root root       4096 Nov 17 06:50 .\ndrwxr-xr-x  5 root root       4096 Nov 17 06:16 ..\ndrwxr-xr-x  2 root root       4096 Nov 17 06:16 .virtual_documents\n-rw-r--r--  1 root root        544 Nov 17 06:45 gemma-2-2b-jpn-it-adp-text-embedding-cft-q8_0.gguf\n-rw-r--r--  1 root root 2784495136 Nov 17 06:49 gemma-2-2b-jpn-it-text-embedding-cft-q8_0.gguf\ndrwxr-xr-x 24 root root       4096 Nov 17 06:36 llama.cpp\n","output_type":"stream"}]},{"cell_type":"code","source":"# Trick to download file from kaggle draft section\nfrom IPython.display import FileLink\nFileLink(r'gemma-2-2b-jpn-it-text-embedding-cft-q8_0.gguf')","metadata":{"execution":{"iopub.status.busy":"2024-11-17T06:54:33.880691Z","iopub.execute_input":"2024-11-17T06:54:33.883475Z","iopub.status.idle":"2024-11-17T06:54:33.903677Z","shell.execute_reply.started":"2024-11-17T06:54:33.883249Z","shell.execute_reply":"2024-11-17T06:54:33.898681Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/gemma-2-2b-jpn-it-text-embedding-cft-q8_0.gguf","text/html":"<a href='gemma-2-2b-jpn-it-text-embedding-cft-q8_0.gguf' target='_blank'>gemma-2-2b-jpn-it-text-embedding-cft-q8_0.gguf</a><br>"},"metadata":{}}]}]}