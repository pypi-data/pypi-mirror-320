# -*- coding: utf-8 -*-
"""
Docs:

    https://pypi.org/project/text-dedup/


    https://chenghaomou.github.io/text-dedup/

    https://github.com/seomoz/simhash-py/tree/master


    pip install simhash-pybind

    import simhash
    v1="abc"
    v2="abc"

    a = simhash.compute(v1)
    b = simhash.compute(v2)
    simhash.num_differing_bits(a, b)


"""

import warnings
warnings.filterwarnings("ignore")
import os, pathlib, uuid, time, traceback
from typing import Any, Callable, Dict, List, Optional, Sequence, Union
from box import Box  ## use dot notation as pseudo class
import pandas as pd, numpy as np, torch
from copy import deepcopy


from utilmy import pd_read_file, os_makedirs, pd_to_file, date_now, glob_glob
from utilmy import log, log2


from box import Box
import os 




######################################################################################
def test_all():
    ### python engine2.py test_all
    pass




#####################################################################################
def date_get():
  y,m,d,h = date_now(fmt="%Y-%m-%d-%H").split("-")
  ts = date_now(fmt="%y%m%d_%H%M%S")
  return y,m,d,h,ts 




def pd_convert_to_parquet(dfm):
    """ Normalized for parquet.... 
        
    """ 
    x = dfm.iloc[0, :]    
    for col in dfm.columns: 
        x1 = x[col]
        if isinstance(x1, list ):
           if isinstance(x1[0],  tuple) or isinstance(x1[0], list)  :
               log("converting", col)
               dfm[col] = dfm[col].apply(lambda llist : [ str(xi) for xi in llist  ]  )
    return dfm



def torch_device(device=None):
    if device is None or len(str(device)) == 0:
        return os.environ.get("torch_device", "cpu")
    else:
        return device


def str_fuzzy_match(xstr:str, xlist:list, cutoff=70.0):
    from rapidfuzz import process, fuzz
    results = process.extract(xstr, xlist, scorer=fuzz.ratio, score_cutoff=cutoff)
    return [result[0] for result in results]




def pd_add_textid(df, coltext="text", colid="text_id"):
    df[colid] = df[coltext].apply(lambda x : hash_textid(x) )
    return df


def hash_textid(xstr:str, n_chars=1000, seed=123):
    """Generate a UNIQUE hash value for a given text string.
       2 same Exact texts with different lengths --> 2 different hash values.
    Args:
        xstr (str): The input text string.
        n_chars (int, optional): The number of characters to consider from the input string. Defaults to 1000.
        seed (int, optional): The seed value for the hash function. Defaults to 123.
    Returns:
        int: The hash value generated by the xxhash.xxh64_intdigest function.
    """
    import xxhash  
    xstr = str(xstr).strip()[:n_chars]
    unique_hash_per_text= xxhash.xxh64_intdigest(xstr, seed=seed) - len(xstr)
    return unique_hash_per_text




def hash_textid_v2(xstr: str, sep=" "):
    """ Applies the SimHash for text text tokens -->
        Similar text --> Similar hash        
        vv="Microsoft, Aurora"
        h1 = hash_textid_eng_v3(vv,)
        vv="Microsoft, Weather.."
        h2 = hash_textid_eng_v3(vv,)
    """
    from floc_simhash import SimHash
    return SimHash(n_bits=64, tokenizer=lambda x: x.lower().split(sep) ).inthash(xstr)


def hash_dist(a: int, b: int, f = 64):
    """ Calculate the hamming distance between int_a and int_b
        i.e. number of bits by which two inputs of the same length differ
    """
    x = (a ^ b) & ((1 << f) - 1)
    ans = 0
    while x:
        ans += 1
        x &= x - 1
    return ans

def pd_add_textid2(df, coltext="text", colid="text_id"):
    df[colid] = df[coltext].apply(lambda x : hash_textid_v2(x) )
    return df



def pd_to_file_split(df, dirout, ksize=1000):
    kmax = int(len(df) // ksize) + 1
    for k in range(0, kmax):
        log(k, ksize)
        dirouk = f"{dirout}/df_{k}.parquet"
        pd_to_file(df.iloc[k * ksize : (k + 1) * ksize, :], dirouk, show=0)



def np_str(v):
    return np.array([str(xi) for xi in v])



def pd_append(df:pd.DataFrame, rowlist:list)-> pd.DataFrame:
  df2 = pd.DataFrame(rowlist, columns= list(df.columns))
  df  = pd.concat([df, df2], ignore_index=True)
  return df

def uuid_int64():
    """## 64 bits integer UUID : global unique"""
    return uuid.uuid4().int & ((1 << 64) - 1)


def pd_fake_data(nrows=1000, dirout=None, overwrite=False, reuse=True) -> pd.DataFrame:
    from faker import Faker

    if os.path.exists(str(dirout)) and reuse:
        log("Loading from disk")
        df = pd_read_file(dirout)
        return df

    fake = Faker()
    dtunix = date_now(returnval="unix")
    df = pd.DataFrame()

    ##### id is integer64bits
    df["id"] = [uuid_int64() for i in range(nrows)]
    df["dt"] = [int(dtunix) for i in range(nrows)]

    df["title"] = [fake.name() for i in range(nrows)]
    df["body"] = [fake.text() for i in range(nrows)]
    df["cat1"] = np_str(np.random.randint(0, 10, nrows))
    df["cat2"] = np_str(np.random.randint(0, 50, nrows))
    df["cat3"] = np_str(np.random.randint(0, 100, nrows))
    df["cat4"] = np_str(np.random.randint(0, 200, nrows))
    df["cat5"] = np_str(np.random.randint(0, 500, nrows))

    if dirout is not None:
        if not os.path.exists(dirout) or overwrite:
            pd_to_file(df, dirout, show=1)

    log(df.head(1), df.shape)
    return df


def pd_fake_data_batch(nrows=1000, dirout=None, nfile=1, overwrite=False) -> None:
    """Generate a batch of fake data and save it to Parquet files.

    python engine.py  pd_fake_data_batch --nrows 100000  dirout='ztmp/files/'  --nfile 10

    """

    for i in range(0, nfile):
        dirouti = f"{dirout}/df_text_{i}.parquet"
        pd_fake_data(nrows=nrows, dirout=dirouti, overwrite=overwrite)








def dict_flatten(d: Dict[str, Any],/,*,recursive: bool = True,
    join_fn: Callable[[Sequence[str]], str] = ".".join,
) -> Dict[str, Any]:
    r"""Flatten dictionaries recursively."""
    result: dict[str, Any] = {}
    for key, item in d.items():
        if isinstance(item, dict) and recursive:
            subdict = dict_flatten(item, recursive=True, join_fn=join_fn)
            for subkey, subitem in subdict.items():
                result[join_fn((key, subkey))] = subitem
        else:
            result[key] = item
    return result


def dict_unflatten(d: Dict[str, Any],/,*, recursive: bool = True,
    split_fn: Callable[[str], Sequence[str]] = lambda s: s.split(".", maxsplit=1),
) -> Dict[str, Any]:
    r"""Unflatten dictionaries recursively."""
    result = {}
    for key, item in d.items():
        split = split_fn(key)
        result.setdefault(split[0], {})
        if len(split) > 1 and recursive:
            assert len(split) == 2
            subdict = dict_unflatten(
                {split[1]: item}, recursive=recursive, split_fn=split_fn
            )
            result[split[0]] |= subdict
        else:
            result[split[0]] = item
    return result



def dict_merge_into(dref:Dict, d2:Dict)->Dict:
    """ Merge d2 into dref, preserving original dref values if not exist in d2

    """
    import copy
    def merge_d2_into_d1(d1, d2):
        for key, value in d2.items():
            if isinstance(value, dict):
                if key in d1 and isinstance(d1[key], dict):
                    # Recursively merge value with value of d1
                    merge_d2_into_d1(d1[key], value)
                else:
                    d1[key] = value  ##### Overwrite existing key
            elif key not in d1:
                d1[key] = value

        return d1


    dnew = merge_d2_into_d1( copy.deepcopy(dref), d2)
    return dnew


################################################################################################
from utils.util_vars import *

def bgetback(x):
   import base64
   return base64.b64decode(x).decode()


def bgetfoward(x):
   import base64
   return base64.b64encode(x.encode()).decode()



################################################################################################
from utilmy import json_save, json_load
import requests
def get_data_v4(q, topk=10):
    global pppbrv
    pp = deepcopy(ppbrv)
    pp['par']['q'] = q 
    pp.uu = bgetback(ppbrv.uu2)
    try:
       resp = requests.get(pp.uu, headers=dict(pp.hh), params=dict(pp.par) )
       dd = resp.json()
       dd['qqid'] = q
       dd['qqts'] = time.time()
       y,m,d,h,ts = date_get()
       json_save(dd, f"ztmp/data/ext/brv/year={y}/monht={m}/day={d}/hour={h}/gnb_{ts}.json")

       ss = ""
       dref = []
       for ii, vv in enumerate(dd['web']['results']):
           if ii>topk: break
           ss = ss +  f"""*Title*: {vv['title']} \n*Text*: {vv['extra_snippets']} \n\n"""
           dref.append({'title':vv['title'], 'url': vv['url'] })

       return ss,dref 

    except Exception as e:
       log(e)
       return ""


def os_open_textfile(file_path):
    # Example usage
    # open_text_file("path/to/your/textfile.txt")

    import subprocess, platform

    # Determine OS
    try :
        OS = platform.system().lower()
        if "windows" in OS:
            editor = "notepad.exe"
        elif "darwin" in OS:
            subprocess.run(['open', '-a', '/Applications/Sublime\ Text.app', file_path])

        else:
            subprocess.run(['xdg-open',  file_path])
    except Exception as e :
        log(e)


def os_convert_open_to_csv(dirin:str="myfile.parquet",):
    """Converts a Parquet file to a CSV file with tab separation.
    
     In your .bashrc :
        export PYTHONPATH="$PYTHONPATH:$(pwd)"  ### relative import issue

        functon view() { 
            ###  view ztmp/data/myfile.parquet
            python utils/util_base.py convert_open_to_csv --dirin "$1"
        }

        import subprocess
        # Replace 'example.txt' with your file path
        file_path = 'example.txt'
        subprocess.run(['open', '-a', '/Applications/Sublime\ Text.app', file_path])


    """
    df = pd_read_file(dirin)

    df = df.iloc[:1000, :]

    suffix = dirin.split(".")[-1]
    dirout = dirin.replace(suffix, "csv")
    pd_to_file(df, dirout, sep='\t', index=False, show=0)
    log(dirout) 
    os_open_textfile(dirout)     



########################################################################################
def get_data_v3(q, topk=10, returnval='text'):
    global ppbng
    pp = deepcopy(ppbng)

    pp['uu'] = bgetback(pp.uu)
    pp['par']['q'] = q 
    try:
       resp = requests.get(pp.uu, headers=pp.hh, params=pp.par )
       dd = resp.json()
       dd['qqid'] = q
       dd['qqts'] = time.time()

       y,m,d,h,ts = date_get()
       json_save(dd, f"ztmp/data/ext/gnb/year={y}/monht={m}/day={d}/hour={h}/gnb_{ts}.json")

       if returnval=='text':
            ss   = ""
            dref = []
            for ii, vv in enumerate(dd['webPages']['value']):
                if ii>topk: break
                ss = ss + f"""*Title*: {vv['name']} \n*Text*: {vv['snippet']} \n\n"""
                dref.append({'title':vv['name'], 'url': vv['url'] })
            return ss,dref 
       else:
            dref=[]
            for ii, vv in enumerate(dd['webPages']['value']):
                if ii>topk: break
                dref.append({'title':vv['name'], 'url': vv['url'], 'text':  vv['snippet'], 
                             'date': vv["dateLastCrawled"] })
            dref = pd.DataFrame(dref)
            return dref 


    except Exception as e:
       log(e)
       return "" 








#########################################################################################
if __name__ == "__main__":
    import fire
    fire.Fire()



