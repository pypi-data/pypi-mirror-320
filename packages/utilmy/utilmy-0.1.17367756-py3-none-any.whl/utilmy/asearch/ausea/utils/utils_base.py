# -*- coding: utf-8 -*-
"""
Docs:

    https://pypi.org/project/text-dedup/


    https://chenghaomou.github.io/text-dedup/

    https://github.com/seomoz/simhash-py/tree/master


    pip install simhash-pybind

    import simhash
    v1="abc"
    v2="abc"

    a = simhash.compute(v1)
    b = simhash.compute(v2)
    simhash.num_differing_bits(a, b)


"""

import warnings
warnings.filterwarnings("ignore")
import os, pathlib, uuid, time, traceback
from typing import Any, Callable, Dict, List, Optional, Sequence, Union
from box import Box  ## use dot notation as pseudo class

import pandas as pd, numpy as np, torch


from utilmy import pd_read_file, os_makedirs, pd_to_file, date_now, glob_glob
from utilmy import log, log2


######################################################################################
def test_all():
    ### python engine2.py test_all
    pass




#####################################################################################
def pd_to_dict(df: pd.DataFrame, colkey: Union[str, List], colval: str = None, key_merge_sep="-") -> Dict:
    """ dataframe into Dict: key:val    key can be a merged-key with "-"
    :param df:
    :param colkey:
    :param colval:
    :return:
    """
    from utilmy import pd_read_file
    if isinstance(df, str):
        df = pd_read_file(df)

    if df is None or len(df) < 1:
        log("pd_to_dict, empty input dataframe df")
        return {}

    try:

        if isinstance(colkey, str):
            colkey2 = colkey
        else:
            colkey2 = key_merge_sep.join(colkey)
            df[colkey2] = df.apply(lambda x: key_merge_sep.join([str(x[ci]) for ci in colkey]), axis=1)
            log('using merged key: ', colkey2)
            # log(df)

        ##### value
        if isinstance(colval, str):
            df1 = df.set_index(colkey2)
            df1 = df1[colval].to_dict()  ### Direct Mapping: colkey2 --> val
            return df1

        else:  ### colkey --> { "col1": val1, "col2": val2 }
            df1 = df.set_index(colkey2)
            df1 = df1.to_dict(orient='index')
            return df1

    except Exception as e:
        log(" pd_to_dict cannot convert: ", e)
        return {}




def torch_getdevice(device=None):
    if device is None or len(str(device)) == 0:
        return os.environ.get("torch_device", "cpu")
    else:
        return device


def pd_add_textid(df, coltext="text", colid="text_id"):
    df[colid] = df[coltext].apply(lambda x : hash_textid(x) )
    return df


def hash_textid(xstr:str, n_chars=1000, seed=123):
    """Generate a UNIQUE hash value for a given text string.

       2 same Exact texts with different lengths --> 2 different hash values.


    Args:
        xstr (str): The input text string.
        n_chars (int, optional): The number of characters to consider from the input string. Defaults to 1000.
        seed (int, optional): The seed value for the hash function. Defaults to 123.

    Returns:
        int: The hash value generated by the xxhash.xxh64_intdigest function.

    Example:
        >>> hash_textid("example_text")
        123456789
    """
    import xxhash  
    xstr = str(xstr).strip()[:n_chars]
    unique_hash_per_text= xxhash.xxh64_intdigest(xstr, seed=seed) - len(xstr)
    return unique_hash_per_text






def pd_to_file_split(df, dirout, ksize=1000):
    kmax = int(len(df) // ksize) + 1
    for k in range(0, kmax):
        log(k, ksize)
        dirouk = f"{dirout}/df_{k}.parquet"
        pd_to_file(df.iloc[k * ksize : (k + 1) * ksize, :], dirouk, show=0)



def np_str(v):
    return np.array([str(xi) for xi in v])



def pd_append(df:pd.DataFrame, rowlist:list)-> pd.DataFrame:
  df2 = pd.DataFrame(rowlist, columns= list(df.columns))
  df  = pd.concat([df, df2], ignore_index=True)
  return df

def uuid_int64():
    """## 64 bits integer UUID : global unique"""
    return uuid.uuid4().int & ((1 << 64) - 1)


def pd_fake_data(nrows=1000, dirout=None, overwrite=False, reuse=True) -> pd.DataFrame:
    from faker import Faker

    if os.path.exists(str(dirout)) and reuse:
        log("Loading from disk")
        df = pd_read_file(dirout)
        return df

    fake = Faker()
    dtunix = date_now(returnval="unix")
    df = pd.DataFrame()

    ##### id is integer64bits
    df["id"] = [uuid_int64() for i in range(nrows)]
    df["dt"] = [int(dtunix) for i in range(nrows)]

    df["title"] = [fake.name() for i in range(nrows)]
    df["body"] = [fake.text() for i in range(nrows)]
    df["cat1"] = np_str(np.random.randint(0, 10, nrows))
    df["cat2"] = np_str(np.random.randint(0, 50, nrows))
    df["cat3"] = np_str(np.random.randint(0, 100, nrows))
    df["cat4"] = np_str(np.random.randint(0, 200, nrows))
    df["cat5"] = np_str(np.random.randint(0, 500, nrows))

    if dirout is not None:
        if not os.path.exists(dirout) or overwrite:
            pd_to_file(df, dirout, show=1)

    log(df.head(1), df.shape)
    return df


def pd_fake_data_batch(nrows=1000, dirout=None, nfile=1, overwrite=False) -> None:
    """Generate a batch of fake data and save it to Parquet files.

    python engine.py  pd_fake_data_batch --nrows 100000  dirout='ztmp/files/'  --nfile 10

    """

    for i in range(0, nfile):
        dirouti = f"{dirout}/df_text_{i}.parquet"
        pd_fake_data(nrows=nrows, dirout=dirouti, overwrite=overwrite)








def dict_flatten(d: Dict[str, Any],/,*,recursive: bool = True,
    join_fn: Callable[[Sequence[str]], str] = ".".join,
) -> Dict[str, Any]:
    r"""Flatten dictionaries recursively."""
    result: dict[str, Any] = {}
    for key, item in d.items():
        if isinstance(item, dict) and recursive:
            subdict = dict_flatten(item, recursive=True, join_fn=join_fn)
            for subkey, subitem in subdict.items():
                result[join_fn((key, subkey))] = subitem
        else:
            result[key] = item
    return result


def dict_unflatten(d: Dict[str, Any],/,*, recursive: bool = True,
    split_fn: Callable[[str], Sequence[str]] = lambda s: s.split(".", maxsplit=1),
) -> Dict[str, Any]:
    r"""Unflatten dictionaries recursively."""
    result = {}
    for key, item in d.items():
        split = split_fn(key)
        result.setdefault(split[0], {})
        if len(split) > 1 and recursive:
            assert len(split) == 2
            subdict = dict_unflatten(
                {split[1]: item}, recursive=recursive, split_fn=split_fn
            )
            result[split[0]] |= subdict
        else:
            result[split[0]] = item
    return result



def dict_merge_into(dref:Dict, d2:Dict)->Dict:
    """ Merge d2 into dref, preserving original dref values if not exist in d2

    """
    import copy
    def merge_d2_into_d1(d1, d2):
        for key, value in d2.items():
            if isinstance(value, dict):
                if key in d1 and isinstance(d1[key], dict):
                    # Recursively merge value with value of d1
                    merge_d2_into_d1(d1[key], value)
                else:
                    d1[key] = value  ##### Overwrite existing key
            elif key not in d1:
                d1[key] = value

        return d1


    dnew = merge_d2_into_d1( copy.deepcopy(dref), d2)
    return dnew




################################################################################################
def os_open_textfile(file_path):
    # Example usage
    # open_text_file("path/to/your/textfile.txt")

    import subprocess, platform

    # Determine OS
    try :
        OS = platform.system().lower()
        if "windows" in OS:
            editor = "notepad.exe"
        elif "darwin" in OS:
            subprocess.run(['open', '-a', '/Applications/Sublime\ Text.app', file_path])

        else:
            subprocess.run(['xdg-open',  file_path])
    except Exception as e :
        log(e)


def os_convert_open_to_csv(dirin:str="myfile.parquet",):
    """Converts a Parquet file to a CSV file with tab separation.
    
     In your .bashrc :
        export PYTHONPATH="$PYTHONPATH:$(pwd)"  ### relative import issue

        functon view() { 
            ###  view ztmp/data/myfile.parquet
            python utils/util_base.py convert_open_to_csv --dirin "$1"
        }

        import subprocess
        # Replace 'example.txt' with your file path
        file_path = 'example.txt'
        subprocess.run(['open', '-a', '/Applications/Sublime\ Text.app', file_path])


    """
    df = pd_read_file(dirin)

    df = df.iloc[:1000, :]

    suffix = dirin.split(".")[-1]
    dirout = dirin.replace(suffix, "csv")
    pd_to_file(df, dirout, sep='\t', index=False, show=0)
    log(dirout) 
    os_open_textfile(dirout)     



###################################################################################################
if __name__ == "__main__":
    import fire
    fire.Fire()



