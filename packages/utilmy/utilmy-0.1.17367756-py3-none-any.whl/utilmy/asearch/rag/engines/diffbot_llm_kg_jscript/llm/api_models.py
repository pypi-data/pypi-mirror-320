'''
Manually created based on the OpenAI OpenAPI specification:
https://github.com/openai/openai-openapi/blob/master/openapi.yaml

openapi generator cannot generate python code correctly for some types.
'''
from typing import Any, Dict, List, Optional, Literal, Union
from pydantic import BaseModel, Field

class ChatCompletionNamedToolChoiceFunction(BaseModel):
    name: str = Field(description="The name of the function to call.")

class ChatCompletionNamedToolChoice(BaseModel):
    type: Literal['function'] = Field(description="The type of the tool.", default="function")
    function: ChatCompletionNamedToolChoiceFunction = Field(description="The specific function to call.")

class ChatCompletionToolCallFunction(BaseModel):
    name: Optional[str] = Field(description="The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.")
    arguments: Optional[str] = Field(description="The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function." )

class ChatCompletionToolFunctionObject(BaseModel):
    description: Optional[str] = Field(default=None, description="A description of what the function does, used by the model to choose when and how to call the function.")
    name: str = Field(description="The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.")
    parameters: Dict[str, Any] = Field(description="The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/gpt/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.\n\nTo describe a function that accepts no parameters, provide the value `{\"type\": \"object\", \"properties\": {}}`." )

class ChatCompletionTool(BaseModel):
    type: Literal['function'] = Field(description="The type of the tool. Currently, only `function` is supported.", default="function")
    function: ChatCompletionToolFunctionObject = Field(description="function.")
    
class ChatCompletionToolCall(BaseModel):
    id: str = Field(description="The ID of the tool call.")
    type: Literal['function'] = Field(description="The type of the tool. Currently, only `function` is supported.", default="function")
    function: ChatCompletionToolCallFunction = Field(description="The function that the model called.") 

ChatCompletionMessageToolCall = ChatCompletionToolCall
class ChatCompletionMessageToolCallChunk(BaseModel):
    index: int = Field(description="index")
    id: Optional[str] = Field(description="The ID of the tool call.")
    type: Optional[Literal['function']] = Field(description="The type of the tool. Currently, only `function` is supported.", default="function")
    function: ChatCompletionToolCallFunction = Field(description="The function that the model called.") 
    
class ChatCompletionMessage(BaseModel):
    content: Optional[Union[str, List[Dict]]] = Field(description="The contents of the message. `content` is required for all messages, and may be null for assistant messages with function calls.")
    role: Optional[Literal["system", "user", "tool", "assistant", "function"]] = Field(description="The role of the messages author. One of `system`, `user`, `assistant`, or `function`.")
    tool_call_id: Optional[str] = Field(description="Tool call that this message is responding to. Only apply for role=tool")
    tool_calls: Optional[List[ChatCompletionMessageToolCall]] = Field(description="The tool calls generated by the model, such as function calls. Only apply for role=assistant")

ChatCompletionRequestMessage = ChatCompletionMessage
ChatCompletionResponseMessage = ChatCompletionMessage

ChatCompletionStreamResponseDeltaFunctionCall = ChatCompletionToolCall

class ChatCompletionStreamResponseDelta(BaseModel):
    content: Optional[str] = Field(description="The contents of the message. `content` is required for all messages, and may be null for assistant messages with function calls.", default=None)
    role: Optional[Literal["system", "user", "tool", "assistant", "function"]] = Field(description="The role of the messages author. One of `system`, `user`, `assistant`, or `function`.", default=None)
    tool_call_id: Optional[str] = Field(description="Tool call that this message is responding to. Only apply for role=tool", default=None)
    tool_calls: Optional[List[ChatCompletionMessageToolCallChunk]] = Field(description="The tool calls generated by the model, such as function calls. Only apply for role=assistant", default=None)

class CompletionUsage(BaseModel):
    completion_tokens: int = Field(description="Number of tokens in the generated completion.", default=0)
    prompt_tokens: int = Field(description="Number of tokens in the prompt.", default=0)
    total_tokens: int = Field(description="Total number of tokens used in the request (prompt + completion).", default=0)


class ChatCompletionResponseChoice(BaseModel):
    finish_reason: Literal["eos", "stop", "length", "tool_calls", "content_filter"]= Field(description="The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,`length` if the maximum number of tokens specified in the request was reached,`content_filter` if content was omitted due to a flag from our content filters, or `tool_calls` if the model called a tool.")
    index: int = Field(description="The index of the choice in the list of choices.")
    message: ChatCompletionResponseMessage = Field(alias="message", description="message")

class ChatCompletionResponse(BaseModel):
    id: str = Field(description="A unique identifier for the chat completion.")
    choices: List[ChatCompletionResponseChoice] = Field(description="A list of chat completion choices. Can be more than one if `n` is greater than 1.")
    created: int = Field(description="The Unix timestamp (in seconds) of when the chat completion was created.")
    model: str = Field(description="The model used for the chat completion.")
    object: str = Field(description="The object type, which is always `chat.completion`", default="chat.completion")
    system_fingerprint: Optional[str] = Field(description="This fingerprint represents the backend configuration that the model runs with.")
    usage: Optional[CompletionUsage] = Field(default=None, description="usage")

ChatCompletionFunctionResponse = ChatCompletionResponse

class ChatCompletionStreamResponseChoice(BaseModel):
    delta: Optional[ChatCompletionStreamResponseDelta] = Field(alias="delta")
    finish_reason: Optional[Literal["eos", "stop", "length", "tool_calls", "content_filter", ""]] = Field(default=None, description="finish reason")
    index: int = Field(description="The index of the choice in the list of choices.")

class ChatCompletionStreamResponse(BaseModel):
    id: str = Field(description="A unique identifier for the chat completion. Each chunk has the same ID.")
    choices: List[ChatCompletionStreamResponseChoice] = Field(description="A list of chat completion choices. Can be more than one if `n` is greater than 1.")
    created: int = Field(description="The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.")
    model: str = Field(description="The model to generate the completion.")
    system_fingerprint: Optional[str] = Field(description="This fingerprint represents the backend configuration that the model runs with.")
    object: Literal["chat.completion.chunk"]= Field(description="The object type, which is always `chat.completion.chunk`.", default="chat.completion.chunk")

class ChatCompletionResponseFormat(BaseModel):
    type: Literal['text', 'json_object']= Field(description="The format that the model must output. Must be one of `text` or `json_object`.", default='text')

class CreateChatCompletionRequest(BaseModel):
    messages: List[ChatCompletionRequestMessage] = Field(description="A list of messages comprising the conversation so far.")
    model: str = Field(description="ID of the model to use. e.g., gpt-3.5-turbo")
    frequency_penalty: Optional[float] = Field(default=0.0, ge=-2.0, le=2.0)
    logit_bias: Optional[Dict[str, int]] = Field(description="Modify the likelihood of specified tokens appearing in the completion.", default=None)
    max_tokens: Optional[int] = Field(description="The maximum number of [tokens](/tokenizer) to generate in the chat completion.")
    n: Optional[int] = Field(description="How many chat completion choices to generate for each input message.", default=1, ge=1, le=128)
    presence_penalty: Optional[float] = Field(description="", default=0, ge=-1, le=2)
    response_format: Optional[ChatCompletionResponseFormat] = Field(description="An object specifying the format that the model must output.")
    seed: Optional[int] = Field(description="If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.", default=None)
    stop: Optional[Union[str, List[str]]] = Field(description="Up to 4 sequences where the API will stop generating further tokens.", default=None)
    stream: Optional[bool] = Field(alias="stream", default=False)
    temperature: Optional[float] = Field(alias="temperature", default=0, ge=0, le=2)
    top_p: Optional[float] = Field(alias="top_p", default=1, ge=0, le=1)
    tools: Optional[List[ChatCompletionTool]] = Field(description="A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for", default=None)
    tool_choice: Optional[Union[Literal["none", "auto"], ChatCompletionNamedToolChoice]] = Field(default=None)
    user: Optional[str] = Field(alias="user", default=None)

class Error(BaseModel):
    code: str = Field(alias="code", description="The code of this Error.")
    message: str = Field(alias="message", description="The message of this Error.")
    param: Optional[str] = Field(alias="param", description="The param of this Error.")
    type: Optional[str] = Field(alias="type", description="The type of this Error.")

class ErrorResponse(BaseModel):
    error: Error = Field(description="The error of this ErrorResponse")

class LLMException(Exception):
    error: Error = None

    def __init__(self, *args: object, error: Error) -> None:
        super().__init__(*args)
        self.error = error

class ChunkRequest(BaseModel):
    text: str
    chunk_size: int
    chunk_overlap: int

class ChunkResponse(BaseModel):
    chunk_text: str
    offset: int

class ToolCall(BaseModel):
    function_name: str
    function_arguments: str
    tool_call_id: str

class ToolCalls(BaseModel):
    tool_calls: List[ToolCall]
    content: str

class RequestGlobals(BaseModel):
    usage: dict[str, CompletionUsage] = {}
    timings: dict[str, float] = {}
    diffbot_responses: list[dict[str, Any]] = []
    internal_request: dict[str, Any] = {}
    disable_tool_fallback: bool = False
    diffbot_token: str = ""