# -*- coding: utf-8 -*-
"""transf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L30WmsnAm05ADqugF_DdWS7MV0DteRzk
"""

import torch
from torch import nn
import torch.nn.functional as F

from fastai.callback.hook import Hooks

bs, seq_len, d_in = 5, 10, 32
x = torch.randn(bs, seq_len, d_in)
d_h = 32

attn = nn.MultiheadAttention(d_h, 4, bias=False, batch_first=True)

for n, p in attn.named_parameters():
    print(f"{n:<12} {p.shape}")

out, attn_weights = attn(x, x, x, average_attn_weights=False)

out.shape, attn_weights.shape

enc = nn.TransformerEncoderLayer(d_h, 4, 128, batch_first=True)

for n, p in enc.named_parameters():
    print(f"{n:<28} {p.shape}")

out = enc(x)
print(out.shape)

dec = nn.TransformerDecoderLayer(d_h, 4, batch_first=True)
for n, p in dec.named_parameters():
    print(f"{n:<32} {p.shape}")

dec

y = torch.randn(bs, seq_len-1, d_h)
out = dec(x, y)
print(out.shape)

enc = nn.TransformerEncoder(
    nn.TransformerEncoderLayer(d_h, 4, batch_first=True),
    2
)

out = enc(x)
print(out.shape)

enc

