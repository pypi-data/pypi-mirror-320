Metadata-Version: 2.3
Name: metricsgpt
Version: 0.3.0
Summary: MetricsGPT is a tool for generating PromQL queries from natural language queries. Talk to your metrics!
Home-page: https://github.com/saswatamcode/metricsgpt
License: Apache-2.0
Keywords: prometheus,promql,metrics,chat,llm,sre,observability
Author: Saswata Mukherjee
Author-email: saswataminsta@yahoo.com
Maintainer: Saswata Mukherjee
Maintainer-email: saswataminsta@yahoo.com
Requires-Python: >=3.12,<4.0
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Dist: aioconsole (>=0.8.1,<0.9.0)
Requires-Dist: colorama (>=0.4.6,<0.5.0)
Requires-Dist: fastapi (>=0.115.6,<0.116.0)
Requires-Dist: llama-index (>=0.12.10,<0.13.0)
Requires-Dist: llama-index-embeddings-azure-openai (>=0.3.0,<0.4.0)
Requires-Dist: llama-index-embeddings-ollama (>=0.5.0,<0.6.0)
Requires-Dist: llama-index-llms-azure-openai (>=0.3.0,<0.4.0)
Requires-Dist: llama-index-llms-gemini (>=0.4.3,<0.5.0)
Requires-Dist: llama-index-llms-ollama (>=0.5.0,<0.6.0)
Requires-Dist: llama-index-vector-stores-milvus (>=0.5.0,<0.6.0)
Requires-Dist: prometheus-api-client (>=0.5.5,<0.6.0)
Requires-Dist: pyyaml (>=6.0.2,<7.0.0)
Requires-Dist: uvicorn (>=0.34.0,<0.35.0)
Project-URL: Documentation, https://github.com/saswatamcode/metricsgpt
Project-URL: Repository, https://github.com/saswatamcode/metricsgpt
Description-Content-Type: text/markdown

# metricsGPT

Talk to your metrics.

<img src="./demo.png" alt="Demo" width="800" style="max-width: 100%;" />

> [!NOTE]
>
> This is a work in progress with no API guarantees. The current implementation needs work on scalability.
> Right now it will cause quite some load on your Prometheus API and take a while.

## Installation

Ensure you have Python 3.12+ and Node v20+ locally .

By default this tool uses [`llama3`](https://ollama.com/library/llama3) and [`nomic-embed-text`](https://ollama.com/library/nomic-embed-text).

```bash
ollama pull llama3
ollama pull nomic-embed-text
```

Have some prometheus up and running. You can use `make run-prom` to get one running in docker that scrapes itself.

You can choose to grab the CLI from https://pypi.org/project/metricsgpt/
```bash
pip3 install metricsgpt
metricsGPT --server --config=config.yaml
```

If building locally you can use Poetry,
```bash
poetry install
poetry run metricsGPT --server --config=config.yaml
```

and visit localhost:8081!

##Â Configuration

Edit [config.yaml](./config.yaml) to suit your own models/Prometheus/Thanos setups.

```yaml
# Prometheus Configuration
prometheus_url: "http://localhost:9090"
# prometheus_auth:
#   # Basic authentication
#   basic_auth:
#     username: "your_username"
#     password: "your_password"
  
#   # Or Bearer token
#   bearer_token: "your_token"
  
#   # Or custom headers
#   custom_headers:
#     Authorization: "Custom your_auth_header"
#     X-Custom-Header: "custom_value"
  
#   # TLS/SSL configuration
#   tls:
#     cert_file: "/path/to/cert.pem"
#     key_file: "/path/to/key.pem"
#     skip_verify: false  # Set to true to skip 

prom_external_url: null  # Optional external URL for links in the UI
query_lookback_hours: 1.0

# Storage Configuration
vectordb_path: "./data.db"
series_cache_file: "./series_cache.json"

# Server Configuration
refresh_interval: 900  # VectorDB Refresh interval in seconds 
server_host: "0.0.0.0"
server_port: 8081

# LLM Configuration
llm:
  provider: "ollama"
  model: "llama3.1"

embedding:
  provider: "ollama"  # or "openai"
  model: "nomic-embed-text"
  dimension: 768 # optional, defaults to this dimension

# For Azure OpenAI embeddings:
#embedding:
#  provider: "azure"
#  model: "text-embedding-ada-002"
#  deployment_name: "your-embedding-deployment"
#  api_key: "your-api-key"
#  endpoint: "your-azure-endpoint"
#  api_version: "2023-05-15"  
#  dimension: "dimensions of model"

# For Watson embeddings:
#embedding:
#  provider: "watsonx"
#  api_key: "your-api-key"
#  project_id: "your-project-id"
#  model_id: "google/flan-ul2"  # optional, defaults to this model
#  dimension: "dimensions of model"

# For OpenAI embeddings:
#embedding:
#  provider: "openai"
#  model: "text-embedding-ada-002"
#  api_key: "your-api-key"
#  dimension: "dimensions of model"

# Example configurations for different providers:

# For OpenAI:
#llm:
#  provider: "openai"
#  model: "gpt-4"
#  api_key: "your-api-key"

# For Ollama:
#llm:
#  provider: "ollama"
#  model: "metricsGPT"
#  timeout: 120.0

# For Azure:
#llm:
#  provider: "azure"
#  model: "gpt-4"
#  deployment_name: "your-deployment"
#  api_key: "your-api-key"
#  endpoint: "your-azure-endpoint"

# For Gemini:
#llm:
#  provider: "gemini"
#  model: "gemini-pro"
#  api_key: "your-api-key"

# For WatsonX:
#llm:
#  provider: "watsonx"
#  api_key: "your-api-key"
#  project_id: "your-project-id"
#  model_id: "your-model-id"
```

## TODOs:
- Much more efficient vectorDB ops
- Use other Prom HTTP APIs for more context
- Range queries
- Visualize
- Embed query results for better analysis
- Process alerts
