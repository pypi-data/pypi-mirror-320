---
hide:
    - navigation
    - toc
---
#
<div align='center'>
<img src="https://raw.githubusercontent.com/ScandEval/ScandEval/main/gfx/scandeval.png" width="517" height="217">
<h3>A Robust Multilingual Evaluation Framework for Language Models</h3>
</div>

--------------------------

ScandEval is a language model benchmarking framework that supports evaluating all types
of language models out there: encoders, decoders, encoder-decoders, base models, and
instruction tuned models. ScandEval has been battle-tested for more than three years and
are the standard evaluation benchmark for many companies, universities and organisations
around Europe.

Check out the [leaderboards](/leaderboards) to see how different language models perform
on a wide range of tasks in various European languages. The leaderboards are updated
regularly with new models and new results.

All benchmark results have been computed using the associated [ScandEval Python
package](/python-package), which you can use to replicate all the results. It supports
all models on the [Hugging Face Hub](https://huggingface.co/models), as well as models
accessible through 100+ different APIs, including models you are hosting yourself via,
e.g., [Ollama](https://ollama.com/) or [LM Studio](https://lmstudio.ai/).

ScandEval is maintained by researchers at [Alexandra
Institute](https://alexandra.dk) and [Aarhus University](https://au.dk), and is funded
by the EU project [TrustLLM](https://trustllm.eu/).

_The image used in the logo has been created by the amazing [Scandinavia and the
World](https://satwcomic.com/) team. Go check them out!_
