import os

class tests:
    def __init__(self):

        self.sklad = {1: ['1.	Наивное умножение матрицы на вектор и умножение матриц. ', 
[
'''
Ну хз че тут написать, умножение матрицы на вектор работает за O(n^2), матрицы на матрицу – O(n^3). Это долго собственно говоря. Вот алгос:
''',
'''
def multiply_matrix_vector(matrix, vector):
    # Проверка совместимости размеров
    rows = len(matrix)
    cols = len(matrix[0])
    if len(vector) != cols:
        raise ValueError("Число столбцов матрицы должно совпадать с размером вектора.")
    
    # Результирующий вектор
    result = [0] * rows
    
    # Умножение
    for i in range(rows):
        for j in range(cols):
            result[i] += matrix[i][j] * vector[j]
    
    return result

''',
'''
def multiply_matrices(matrix_a, matrix_b):
    # Проверка совместимости размеров
    rows_a = len(matrix_a)
    cols_a = len(matrix_a[0])
    rows_b = len(matrix_b)
    cols_b = len(matrix_b[0])
    if cols_a != rows_b:
        raise ValueError("Число столбцов первой матрицы должно совпадать с числом строк второй матрицы.")
    
    # Создаем результирующую матрицу размера rows_a x cols_b
    result = [[0] * cols_b for _ in range(rows_a)]
    
    # Умножение
    for i in range(rows_a):
        for j in range(cols_b):
            for k in range(cols_a):
                result[i][j] += matrix_a[i][k] * matrix_b[k][j]
    
    return result
''']],

2: ['2.	Иерархия памяти, план кеша и LRU, промахи в обращении к кешу.', 
[
'''
Иерархия памяти описывает структуру различных уровней памяти в компьютерной системе, организованных от самой быстрой и дорогостоящей к более медленным и дешевым. Эта иерархия необходима для достижения баланса между скоростью и стоимостью памяти. Она состоит из нескольких уровней:
1.	Регистр процессора — самый быстрый тип памяти, используется для хранения данных и инструкций, с которыми процессор работает непосредственно.
2.	Кэш-память (L1, L2, L3) — быстродействующая память, расположенная ближе к процессору. Существует несколько уровней кеша:
○	L1 (первичный кеш) — самый быстрый, но и самый маленький. Обычно разделяется на кеш данных и кеш инструкций.
○	L2 (вторичный кеш) — больше по размеру, но медленнее, чем L1.
○	L3 (третичный кеш) — может быть общим для нескольких ядер процессора, обычно медленнее, чем L2, но также значительно больше по объему.
3.	Оперативная память (RAM) — более медленная, чем кэш, но гораздо большая по объему. Используется для хранения активных данных и программ.
4.	Жесткий диск (HDD), твердотельный накопитель (SSD) — более медленные устройства хранения данных, которые используются для долговременного хранения данных.
''',
'''
План кеша (или политика кеширования) определяет, как данные должны быть записаны и извлечены из кеша. Рассмотрим несколько ключевых моментов:
1.	Стратегия записи в кеш:
○	Write-through: При записи данных в кеш, они немедленно записываются и в основной памяти.
○	Write-back: Данные записываются в кеш, а в основной памяти обновляются только при вытеснении данных из кеша.
2.	Стратегия вытеснения:
○	LRU (Least Recently Used): Стратегия вытеснения, при которой из кеша вытесняется тот блок данных, который не использовался дольше всех. Это предполагает, что если данные не использовались давно, то они, вероятно, не понадобятся в будущем.
○	FIFO (First In, First Out): Выталкивание первого элемента, который был загружен в кеш.
○	Random: Выталкивается случайный элемент кеша.
3.	Размер кеша: Он обычно ограничен, поэтому важно правильно выбрать стратегию замещения, чтобы минимизировать количество промахов.
''',
'''
LRU (Least Recently Used)— это алгоритм управления кешем, который вытесняет наименее используемые данные. Идея заключается в том, что если данные в кеше не использовались в течение определенного времени, они, вероятно, не понадобятся в будущем. Таким образом, для решения проблемы нехватки места в кеше, алгоритм LRU удаляет блоки данных, которые не использовались дольше всего.
Пример работы LRU:
1.	Если кэш полон, и нужно загрузить новый блок данных, вытесняется блок, который был использован раньше всего.
2.	Для реализации LRU часто используют структуры данных, такие как связные списки или очереди с приоритетами. В каждом цикле работы системы обновляется информация о времени последнего использования элементов кеша.
''',
'''
Промах в обращении к кешу (cache miss) возникает, когда требуемые данные не найдены в кеше, и требуется их загрузка из более медленной памяти (например, из оперативной памяти или жесткого диска). Промахи могут значительно замедлить выполнение программы, поэтому эффективное управление кешем критически важно для производительности.
Типы промахов:
1.	Compulsory (или cold miss): Это промах, который возникает, когда данные обращаются к кешу впервые. При первом доступе к данным они не могут быть в кеше, так как кеш еще не содержал этих данных.
2.	Capacity miss: Промах, который возникает, когда кеш переполняется. Например, если в кеше недостаточно места для хранения всех данных, которые требуются программе, то некоторые данные вытесняются, и при следующем доступе они снова не могут быть найдены в кеше.
3.	Conflict miss: Промах, который происходит из-за конфликта размещения данных в кеше. Это может произойти, если кеш имеет определенные ограничения, которые не позволяют эффективно разместить данные, даже если в кеше есть свободное место.
Как снизить промахи:
●	Увеличение объема кеша может помочь уменьшить количество capacity miss.
●	Использование алгоритмов замещения, таких как LRU, помогает эффективно управлять данными в кеше и минимизировать количество промахов.
●	Системы могут использовать сложные методы предсказания или предзагрузки данных, чтобы уменьшить вероятность промахов.
''']],
3: ['3.	Алгоритм Штрассена.', 
[
'''
Алгоритм Штрассена — это эффективный алгоритм умножения квадратных матриц, который уменьшает количество операций по сравнению с классическим методом. Вместо O(n^3) операций он использует O(n^log 2(7)) ~ O(n^{2.81}), что ускоряет вычисления для больших матриц.
Основные идеи алгоритма
1.	Разбиение матриц: Матрицы A и B размером n×n разбиваются на четыре подматрицы размером n/2×n/2:
2.	Рекурсивные вычисления: Вычисляются 7 промежуточных произведений:
M1=(A_11+A_22)(B_11+B_22), 
M2=(A_21+A_22)*B11, 
M3=A_11*(B_12−B_22), 
M4=A_22*(B_21−B_11), 
M5=(A_11+A_12)*B_22,
M6=(A_21−A_11)(B_11+B_12),
M7=(A_12−A_22)(B_21+B_22).
3.	Сборка результирующей матрицы: Итоговые подматрицы C_11,C_12,C_21,C_22 вычисляются как:
C11=M1+M4−M5+M7,
C12=M3+M5,
C21=M2+M4,
C22=M1−M2+M3+M6.
4.	Рекурсия: Если подматрицы достаточно малы (например, размер 2×2), выполняется обычное умножение.
''',
'''
import numpy as np


def pad_matrix_to_power_of_two(matrix):
    import math

    # Размеры входной матрицы
    rows = len(matrix)
    cols = len(matrix[0])

    # Определяем ближайшую степень двойки
    new_size = 2 ** math.ceil(math.log2(max(rows, cols)))

    # Создаем новую матрицу, заполненную нулями
    padded_matrix = [[0] * new_size for _ in range(new_size)]

    # Копируем исходную матрицу в новый массив
    for i in range(rows):
        for j in range(cols):
            padded_matrix[i][j] = matrix[i][j]

    return padded_matrix


def add_matrices(A, B):
    return [[A[i][j] + B[i][j] for j in range(len(A))] for i in range(len(A))]


def subtract_matrices(A, B):
    return [[A[i][j] - B[i][j] for j in range(len(A))] for i in range(len(A))]


def strassen_multiply(A, B):
    # Базовый случай: если размер матрицы 1x1
    n = len(A)
    if n == 1:
        return [[A[0][0] * B[0][0]]]

    # Разбиение матриц на подматрицы
    mid = n // 2
    A11 = [row[:mid] for row in A[:mid]]
    A12 = [row[mid:] for row in A[:mid]]
    A21 = [row[:mid] for row in A[mid:]]
    A22 = [row[mid:] for row in A[mid:]]

    B11 = [row[:mid] for row in B[:mid]]
    B12 = [row[mid:] for row in B[:mid]]
    B21 = [row[:mid] for row in B[mid:]]
    B22 = [row[mid:] for row in B[mid:]]

    # Вычисление промежуточных матриц
    M1 = strassen_multiply(add_matrices(A11, A22), add_matrices(B11, B22))
    M2 = strassen_multiply(add_matrices(A21, A22), B11)
    M3 = strassen_multiply(A11, subtract_matrices(B12, B22))
    M4 = strassen_multiply(A22, subtract_matrices(B21, B11))
    M5 = strassen_multiply(add_matrices(A11, A12), B22)
    M6 = strassen_multiply(subtract_matrices(A21, A11), add_matrices(B11, B12))
    M7 = strassen_multiply(subtract_matrices(A12, A22), add_matrices(B21, B22))

    # Сборка результирующих подматриц
    C11 = add_matrices(subtract_matrices(add_matrices(M1, M4), M5), M7)
    C12 = add_matrices(M3, M5)
    C21 = add_matrices(M2, M4)
    C22 = add_matrices(subtract_matrices(add_matrices(M1, M3), M2), M6)

    # Объединение подматриц в одну
    new_matrix = [[0] * n for _ in range(n)]
    for i in range(mid):
        for j in range(mid):
            new_matrix[i][j] = C11[i][j]
            new_matrix[i][j + mid] = C12[i][j]
            new_matrix[i + mid][j] = C21[i][j]
            new_matrix[i + mid][j + mid] = C22[i][j]

    return new_matrix
''',
'''
# Пример использования
A = [[1, 2],
     [5, 6],
     [9, 10],
     [13, 14]]

B = [[1, 0, 0, 1],
     [0, 1, 1, 0]]

result = strassen_multiply(pad_matrix_to_power_of_two(A), pad_matrix_to_power_of_two(B))
print(np.array(result))

''']],
4: ['4.	Собственные векторы, собственные значения (важность, Google PageRank). (Витя)', 
[
'''
Пусть A —действительная числовая квадратная матрица размеров (n × n). Ненулевой вектор Х размеров (n × 1), удовлетворяющий условию AX= λX называется собственным вектором матрицы A. 
Число λ в равенстве называется собственным значением. Говорят, что собственный вектор X соответствует (принадлежит) собственному значению λ. 
Последнее равенство равносильно однородной относительно X системе: 
(A – λE) X = 0, (X ≠ 0). Е - единичная матрица
Система имеет ненулевое решение для вектора X (при известном λ) при условии 
|A– λE| = 0. Это равенство есть характеристическое уравнение:
 |A – λE| = Pn(λ) = 0 где Pn() — характеристический многочлен n-й степени. 
Корни λ1, λ2, ..., λn характеристического уравнения являются собственными (характеристическими) значениями матрицы A, а соответствующие каждому собственному значению λi, i = 1, ..., n, ненулевые векторы Xi, удовлетворяющие системе AXi = λi Xi или (A – λiE) Xi = 0, i = 1, 2, ..., n, являются собственными векторами. Требуется найти собственные значения и собственные векторы заданной матрицы. Поставленная задача часто именуется второй задачей линейной алгебры

''',
]],
5: ['5.	Разложение Шура и QR-алгоритм. ', 
[
'''
Разложение Шура — это важная теорема линейной алгебры, утверждающая, что любую квадратную матрицу A можно разложить на произведение унитарной матрицы и верхнетреугольной матрицы.
Если мы приведем матрицу A к верхнетреугольному виду T с помощью унитарной матрицы U: U*AU = T, мы решим задачу. Умножая слева и справа на U и U* соответственно, получим нужное нам разложение: A = UTU*
''',
'''
QR алгоритм был предложен в 1961 г. независимо В. Н. Кублановской и J. Francis’ом. 
QR алгоритм и QR разложение – разные вещи. QR разложение – это представление матрицы в виде произведения двух матриц, QR алгоритм использует QR разложение для вычисления разложения Шура. Рассмотрим выражение A = QTQ∗ и перепишем его в виде QT=AQ. Слева замечаем QR разложение матрицы AQ.
''',
'''
Шаг 1: Инициализация
Начинаем с исходной матрицы:
$$A_0$$ = $$A$$
Шаг 2: QR-разложение
Для каждой итерации kkk:
$$A_k$$ = $$Q_k R_k$$
где:
●	$$Q_k$$ — унитарная матрица ($$Q_k^*$$ $$Q_k$$ = $$I$$ ),
●	$$R_k$$ — верхнетреугольная матрица.
Шаг 3: Обновление матрицы
После разложения вычисляем новую аппроксимацию:
$$A_{k+1}=R_k Q_k$$
Эта операция сохраняет подобие: $$A_{k+1}$$ ∼ $$A_k$$, то есть $$A_{k+1}$$ и $$A_k$$ имеют одинаковый спектр собственных значений.
Шаг 4: Критерий остановки
Продолжаем итерации, пока поддиагональные элементы Ak не станут достаточно малыми (их евклидова норма должна быть меньше заданного порога ε).
''',
'''
import math
#Нулевая матрица
def zeros(n, m):
    return [[0] * m for i in range(n)]
#Единичная матрица
def eye(n):
    return [[1 if i == j else 0 for j in range(n)] for i in range(n)]
# Умножение двух квадратных матриц n x n
def matmul(A, B):
    n = len(A)
    result = zeros(n, n)
    for i in range(n):
        for j in range(n):
            for k in range(n):
                result[i][j] += A[i][k] * B[k][j]
    return result

# Транспонирование матрицы
def transpose(matrix):
    return [[matrix[j][i] for j in range(len(matrix))] for i in range(len(matrix))]

#Вычисление евклидовой нормы
def norm(x):
    result = 0
    for element in x:
        result += element ** 2
    return math.sqrt(result)

#Внешнее произведение вектора v
def outer_product(v):
    n = len(v)
    outer_matrix =  zeros(n, n)
    for i in range(n):
        for j in range(n):
            outer_matrix[i][j] = v[i] * v[j]
    return outer_matrix

#Норма поддиагональных элементов матрицы A
def under_diagonal_norm(A):
    n = len(A)
    result = 0
    for i in range(1, n):
        for j in range(i):
            result += A[i][j] ** 2
    return math.sqrt(result)

def QR(A):
    n = len(A)
    Q = eye(n)
    R = [row[:] for row in A]
    for i in range(n - 1):
        # Вектор для отражения Хаусхолдера (начиная с i-й строки)
        x = [R[j][i] for j in range(i, n)]
        # Вектор e (целевой вектор, норма которого совпадает с x)
        e = [0] * len(x)
        e[0] = norm(x)  # норма
        # Вектор u = x - e
        u = [x[j] - e[j] for j in range(len(x))]
        # Вектор Хаусхолдера v (нормализация вектора u)
        if norm(u) != 0:
            v = [u[j] / norm(u) for j in range(len(u))]
        else:
            v = u
        H = eye(n)
        outer_vv = outer_product(v)  # вычисляем внешнее произведение
        for row in range(i, n):
            for col in range(i, n):
                H[row][col] -= 2 * outer_vv[row - i][col - i] / sum(v[k] * v[k] for k in range(len(v)))  # формула отражения
        # Обновляем R и Q
        R = matmul(H, R)
        Q = matmul(Q, transpose(H))
    return Q, R
def QR_algorithm_with_U(A, tol=1e-12, max_iter=1000):
    n = len(A)
    Ak = [row[:] for row in A]  # A0 = A
    U = eye(n)  # Начальная матрица U (единичная)

    for _ in range(max_iter):
        Q, R = QR(Ak)

        # Обновляем итоговую унитарную матрицу U
        U = matmul(U, Q)

        # Обновляем аппроксимацию A_{k+1} = R_k * Q_k
        Ak_new = zeros(n, n)
        for i in range(n):
            for j in range(n):
                Ak_new[i][j] = sum(R[i][k] * Q[k][j] for k in range(n))

        # Проверяем сходимость QR-алгоритма (норму поддиагональных элементов)
        if under_diagonal_norm(Ak_new) < tol:
            break

        # Обновляем текущую матрицу
        Ak = Ak_new

    return U, Ak  # U -- унитарная матрица, Ak -- треугольная
A = [[6, 9, -7],
     [511, 11, -3],
     [12, 6, -2]]

U, T = QR_algorithm_with_U(A)
U_star = transpose(U)

# Проверка: A ≈ U @ T @ U*
A_reconstructed = matmul(matmul(U, T), U_star)

print("Исходная матрица A:")
for row in A:
    print(row)

print("\nМатрица U (унитарная):")
for row in U:
    print(row)

print("\nМатрица T (верхнетреугольная):")
for row in T:
    print(row)

print("\nРеконструированная матрица A (U @ T @ U*):")
for row in A_reconstructed:
    print(row)
''']],
6: ['6.	Степенной метод', 
[
'''
Степенной метод – простейший метод вычисления максимального по модулю собственного значения. Метод прост в реализации и основан на итеративном процессе, что делает его полезным в приложениях, где требуется вычислить только доминирующее собственное значение.
Степенной метод дает оценку для максимального по модулю собственного числа или спектрального радиуса матрицы  
Одна итерация требует одного умножения матрицы на вектор. Если можно умножить вектор на матрицу за O(n) (например, она разреженная), тогда степенной метод можно использовать для больших n
Сходимость может быть медленной
Для грубой оценки максимального по модулю собственного значения и соответствующего вектора достаточно небольшого числа итераций
Вектор решения лежит в Крыловском подпространстве {x0 , A x0 , …, Ak x0 } и имеет вид μAk x0 , где μ – нормировочная постоянная.
''',
'''
import numpy as np

def power_method(A, num_iterations=100, tol=1e-6):
    n = A.shape[0]
    # Начальный вектор (можно выбрать случайный)
    b_k = np.random.rand(n)

    for _ in range(num_iterations):
        # Новый вектор - результат умножения A на текущий вектор
        b_k1 = np.dot(A, b_k)

        # Нормализация нового вектора
        b_k1_norm = np.linalg.norm(b_k1)
        b_k1 = b_k1 / b_k1_norm

        # Проверка сходимости
        if np.linalg.norm(b_k1 - b_k) < tol:
            break

        # Переход к следующей итерации
        b_k = b_k1

    # Приближённое собственное значение
    eigenvalue = np.dot(b_k, np.dot(A, b_k))
    # Приближенный собственный вектор
    eigenvector = b_k

    return eigenvalue, eigenvector

# Пример использования
A = np.array([[4, 1],
              [2, 3]])

eigenvalue, eigenvector = power_method(A)
print("Наибольшее по модулю собственное значение:", eigenvalue)
print("Соответствующий собственный вектор:", eigenvector)
''',
]],
7: ['7.	Круги Гершгорина.', 
[
'''
Круги Гершгорина — это геометрическое представление, которое позволяет оценить расположение собственных значений квадратной матрицы в комплексной плоскости. Этот метод основан на теореме Гершгорина, которая устанавливает, что все собственные значения матрицы лежат внутри определённых кругов, называемых кругами Гершгорина.
Теорема Гершгорина:
Все собственные значения $$\lambda_i$$ , i = 1, 2, ..., n находятся внутри объединения кругов Гершгорина Ci , где Ci – окружность на комплексной плоскости с центром в aii и радиусом $$r_i = \sum_{j \neq i} |a_{ij}|$$ 
Более того, если круги не пересекаются, то они содержат по одному собственному значению внутри каждого круга.
''',
'''
import numpy as np
import matplotlib.pyplot as plt
n = 3
fig, ax = plt.subplots(1, 1)
a = [[5, 1, 1], [1, 0, 0.5], [2, 0, 10]]
a = np.array(a)
a = a + 2 * np.random.randn(n, n)

xg = np.diag(a).real
yg = np.diag(a).imag
rg = np.zeros(n)
ev = np.linalg.eigvals(a)

for i in range(n):
  rg[i] = np.sum(abs(a[i, :])) - np.abs(a[i,i])
  crc = plt.Circle((xg[i], yg[i]), radius = rg[i], fill = False)
  ax.add_patch(crc)

plt.scatter(ev.real, ev.imag, color = 'r', label = 'Eigvalues')
plt.legend()
''',
]],
8: ['8.	Разложение Шура, теорема Шура. ', 
[
'''
Разложение Шура — это важная теорема линейной алгебры, утверждающая, что любую квадратную матрицу A можно разложить на произведение унитарной матрицы и верхнетреугольной матрицы.
Если мы приведем матрицу A к верхнетреугольному виду T с помощью унитарной матрицы U: U*AU = T, мы решим задачу. Умножая слева и справа на U и U* соответственно, получим нужное нам разложение: A = UTU* 
Использование унитарных матриц приводит к устойчивым алгоритмам, таким образом собственные значения вычисляются очень точно. Разложение Шура показывает, почему нам нужны матричные разложения: они представляют матрицу в виде произведения трёх матриц подходящей структуры.
Теорема Шура:
Каждая матрица A принадлежит Cn×n может быть представлена в виде формы Шура: A = UTU* где U унитарная, а T верхнетреугольная, при этом элементы на главной диагонали T являются собственными значениями матрицы A.
''',
]],
9: ['9.	Нормальные матрицы, эрмитовы матрицы, унитарно диагонализуемые матрицы, верхне-гессенбергова форма матриц. ', 
[
'''
Теорема: A – нормальная матрица, тогда и только тогда, когда A = UΛU* , где U унитарна и Λ диагональна.

Примеры нормальных матриц: эрмитовы матрицы, унитарные матрицы. 

 Любая нормальная матрица – унитарно диагонализуема. Это означает, что она может быть приведена к диагональному виду с помощью унитарной матрицы U. Другими словами, каждая нормальная матрица имеет ортогональный базис из собственных векторов.

Матрица 𝐴 имеет верхне-гессенбергову форму, если 𝑎𝑖𝑗 = 0, при 𝑖 >= 𝑗 + 2. 
H = 
x x x x x
x x x x x
0 x x x x
0 0 x x x
0 0 0 x x
''',
'''
С помощью отражений Хаусхолдера можно привести любую матрицу к верхнегессенберговой форме: U* AU = H. Сложность такого приведения O(n^3 ) операций. Если матрица приведена к верхне-гессенберговой форме, то одна итерация QR алгоритма имеет сложность O(n^2 ) операций. Также верхне-гессенбергова форма матрицы сохраняется после выполнения одной итерации QR алгоритма. Если матрица 𝐴 симметричная (эрмитова), то 𝐴=𝐴∗ , тогда 𝐻=𝐻∗ и верхне-гессенбергова форма оказывается трёхдиагональной матрицей.  Будем говорить только о симметричном трёхдиагональном виде верхне-гессенберговой формы.  Любая эрмитова матрица может быть приведена к трёхдиагональной форме с помощью отражений Хаусхолдера. 
''',
]],
10: ['10.	Спектр и псевдоспектр.', 
[
'''
Спектром матрицы A∈C^(n×n) называют множество всех её собственных значений:
spec(A)={λ∈C: det(A−λI)=0} 
Для динамических систем с матрицей A, спектр может много сообщить о поведении системы (например, о её устойчивости). Однако для не нормальных матриц, спектр может быть неустойчивым относительно малых возмущений матрицы. Для измерения подобных возмущений было разработана концепция псевдоспектра.

Теорема: A – нормальная матрица, тогда и только тогда, когда A = UΛU* , где U унитарна и Λ диагональна. Любая нормальная матрица – унитарно диагонализуема. Это означает, что она может быть приведена к диагональному виду с помощью унитарной матрицы U. Другими словами, каждая нормальная матрица имеет ортогональный базис из собственных векторов.
''',
'''
Псевдоспектр матрицы A можно определить следующим образом:
Λε(A)={λ∈C ∣ ∃x, x!= 0: (A−λI+E)x=0, ∥E∥≤ε}.
Для малых E и нормальных A это круги вокруг собственных значений, для не нормальных матриц, структура может сильно отличаться.
Пример:
Нормальная матрица: Пусть A=diag(1,2,3). Спектр: {1,2,3} а псевдоспектр состоит из окружностей радиуса ε центрированных в этих точках.

Различают полную и частичную проблему собственных значений, когда необходимо найти весь спектр (все собственные значения) и собственные век& торы либо часть спектра, например: p(A) = max_i |λ_i(A)| и p(A) = min_i |λ_i(A)| . Величина (A) называется спектральным радиусом. 
3. Симметрическая матрица имеет полный спектр действительных собственных значений; k-кратному корню характеристического уравнения симметрической матрицы соответствует ровно k линейно независимых собственных векторов. 
4. Положительно определенная симметрическая матрица имеет полный спектр действительных положительных собственных значений.
''',
]],
11: ['11.	Неявный QR алгоритм (со сдвигами). ', 
[
'''
Вычисление QR разложения более сложное, но возможно вычислить 𝐴_(𝑘+1) напрямую без вычисления 𝑄𝑘 . Это называется неявный QR-шаг.
Все реализации неявного QR алгоритма основаны на следующей теореме:
 Теорема. Пусть 𝑄∗𝐴𝑄 = 𝐻 верхне-гессенбергова форма матрицы. Тогда первый столбец матрицы 𝑄 определяет все остальные её столбцы. Он может быть найден из следующего уравнения: 𝐴𝑄 = 𝑄𝐻.

''',
'''
Сходимость такого алгоритма линейная с фактором
A_k - s_k * I = Q_k * R_k, A_(k+1) = R_k*Q_k + s_k * I и | (𝜆_(m+1)  -  s_k) / (𝜆_m - s_k) |

 и где 𝜆_m – 𝑚-ое большее по модулю собственное значение. Если сдвиг близок к собственному вектору, сходимость более быстрая. 
▪ Существуют различные стратегии выбора сдвигов. 
▪ Использование сдвигов – это общий подход к ускорению сходимости итерационных методов вычисления собственных значений.
''',
'''
import math
#Нулевая матрица
def zeros(n, m):
    return [[0] * m for i in range(n)]
#Единичная матрица
def eye(n):
    return [[1 if i == j else 0 for j in range(n)] for i in range(n)]
# Умножение двух квадратных матриц n x n
def matmul(A, B):
    n = len(A)
    result = zeros(n, n)
    for i in range(n):
        for j in range(n):
            for k in range(n):
                result[i][j] += A[i][k] * B[k][j]
    return result

# Транспонирование матрицы
def transpose(matrix):
    return [[matrix[j][i] for j in range(len(matrix))] for i in range(len(matrix))]

#Вычисление евклидовой нормы
def norm(x):
    result = 0
    for element in x:
        result += element ** 2
    return math.sqrt(result)

#Внешнее произведение вектора v
def outer_product(v):
    n = len(v)
    outer_matrix =  zeros(n, n)
    for i in range(n):
        for j in range(n):
            outer_matrix[i][j] = v[i] * v[j]
    return outer_matrix

#Норма поддиагональных элементов матрицы A
def under_diagonal_norm(A):
    n = len(A)
    result = 0
    for i in range(1, n):
        for j in range(i):
            result += A[i][j] ** 2
    return math.sqrt(result)
''',
'''
def QR(A):
    n = len(A)
    Q = eye(n)
    R = [row[:] for row in A]
    for i in range(n - 1):
        # Вектор для отражения Хаусхолдера (начиная с i-й строки)
        x = [R[j][i] for j in range(i, n)]
        # Вектор e (целевой вектор, норма которого совпадает с x)
        e = [0] * len(x)
        e[0] = norm(x)  # норма
        # Вектор u = x - e
        u = [x[j] - e[j] for j in range(len(x))]
        # Вектор Хаусхолдера v (нормализация вектора u)
        if norm(u) != 0:
            v = [u[j] / norm(u) for j in range(len(u))]
        else:
            v = u
        H = eye(n)
        outer_vv = outer_product(v)  # вычисляем внешнее произведение
        for row in range(i, n):
            for col in range(i, n):
                H[row][col] -= 2 * outer_vv[row - i][col - i] / sum(v[k] * v[k] for k in range(len(v)))  # формула отражения
        # Обновляем R и Q
        R = matmul(H, R)
        Q = matmul(Q, transpose(H))
    return Q, R
def QR_algorithm2(A, tol=1e-12, max_iter=1000):
    def manual_dot(x, y): #Скалярное произведение векторов
        return sum(xi * yi for xi, yi in zip(x, y))

    def mv(A, x): #Умножение матрицы на вектор
        return [sum(A[i][j] * x[j] for j in range(len(x))) for i in range(len(A))]

    n = len(A)
    Ak = [row[:] for row in A]  # инициализируем A0 = A

    x = [random.random() for _ in range(n)]
    norm_x = norm(x)
    x = [xi / norm_x for xi in x]  # нормализуем вектор

    for i in range(max_iter):
        #параметр сдвига
        Ax = mv(Ak, x)  # умножаем матрицу Ak на вектор x
        lambda_k = manual_dot(Ax, x) / manual_dot(x, x)  # параметр сдвига
        #матрица с сдвигом
        A_shifted = [[Ak[i][j] - lambda_k * (1 if i == j else 0) for j in range(n)] for i in range(n)]
        A_inv = inv(A_shifted)  # A^(-1)

        # Обновляем вектор x
        x = mv(A_inv, x)  # обратная итерация
        norm_x = norm(x)
        x = [xi / norm_x for xi in x]  # нормализуем вектор

        # QR разложение для матрицы со сдвигом
        Q, R = QR([[Ak[i][j] - lambda_k * (1 if i == j else 0) for j in range(n)] for i in range(n)])

        # Обновляем матрицу с обратным сдвигом
        Ak_new = matmul(R, Q)
        Ak_new = [[Ak_new[i][j] + lambda_k * (1 if i == j else 0) for j in range(n)] for i in range(n)]

        # Проверка на сходимость
        if under_diagonal_norm(Ak_new) < tol:
            break
        Ak = Ak_new

    return Ak
A = [[6, 9, -7],
     [511, 11, -3],
     [12, 6, -2]]

A_upper = QR_algorithm2(A)
print("Треугольная матрица после QR алгоритма:")
for i in range(len(A_upper)):
    print(A_upper[i])
eigenvalues = [A_upper[i][i] for i in range(len(A_upper))]
print('---------------------------')
print('Собственные значения')
print(eigenvalues)
''']],
12: ['12.	Алгоритм на основе стратегии "разделяй и властвуй".', 
[
'''
Метод «разделяй и властвуй» является эффективным алгоритмом для вычисления собственных значений и собственных векторов симметричных трёхдиагональных матриц.
Он особенно полезен при необходимости нахождения всех собственных значений и векторов для матриц порядка 
𝑛, начиная с примерно 26; точное значение этого порога зависит от конкретной вычислительной системы.

''',
'''
1. Разделение матрицы: Исходная матрица разбивается на две меньшие подматрицы, добавляется возмущение ранга 1.

''',
'''
2. Рекурсивное вычисление: Собственные значения и векторы подматриц находятся рекурсивно.
''',
'''
3. После нахождения собственных значений и векторов подматриц задача сводится к решению задачи для матрицы с возмущением ранга 1. Это достигается путём решения соответствующего характеристического уравнения и обновления собственных векторов.
''',
'''
Преимущества метода:

Высокая эффективность: Метод «разделяй и властвуй» является одним из самых быстрых для вычисления всех собственных значений и векторов трёхдиагональных матриц большого порядка.

Параллелизация: Алгоритм хорошо адаптируется к параллельным вычислениям, что позволяет значительно ускорить процесс на современных многопроцессорных системах.
'''
]],
13: ['13.	Разреженные матрицы, форматы хранения разреженных матриц, прямые методы для решения больших разреженных систем. (олег)', 
[
'''
– Разреженной называют матрицу, имеющую малый процент ненулевых элементов. 
– Матрица размера N x N называется от разреженной, если количество ее ненулевых элементов есть O(N).
Существует несколько популярных форматов хранения разреженных матриц. Выбор формата зависит от характера данных и предполагаемых операций.
1. COO (Coordinate Format):
    Сохраняет ненулевые элементы и их индексы.
    Подходит для построения матрицы (удобно добавлять элементы).
    Структура:
        Значения (data): массив ненулевых элементов.
        Строки (row): массив индексов строк.
        Столбцы (col): массив индексов столбцов.
2. CSR (Compressed Sparse Row):
    Сохраняет данные по строкам, сжатие происходит за счёт хранения индексов начала каждой строки.
    Эффективен для матрично-векторных операций.
    Структура:
        Значения (data): массив ненулевых элементов.
        Индексы столбцов (col_indices): массив номеров столбцов для каждого элемента.
        Смещения строк (row_ptr): массив, указывающий начало каждой строки в data.
''',
'''
3. CSC (Compressed Sparse Column):
    Аналогично CSR, но сжатию подвергаются столбцы.
    Удобен для операций с столбцами, например, транспонирования.
    Структура:
        Значения (data): массив ненулевых элементов.
        Индексы строк (row_indicesr): массив номеров строк для каждого элемента.
        Смещения столбцов (col_ptr): массив, указывающий начало каждого столбца в data.
4. DIA (Diagonal Storage Format):
    Используется для матриц, где большинство ненулевых элементов лежат на диагоналях.
    Эффективно хранит диагонали в виде массивов.
    Структура:
        Диагонали (data): двумерный массив с ненулевыми элементами диагоналей.
        Смещения (offsets): массив, указывающий индексы диагоналей относительно главной.
5. LIL (List of Lists):
    Сохраняет ненулевые элементы и их индексы в строках.
    Подходит для построения и изменения матриц (удобно добавлять или удалять элементы).
    Структура:
        Строки (rows): массив списков, где каждый список содержит индексы столбцов ненулевых элементов строки.
        Значения (data): массив списков, где каждый список хранит значения соответствующих ненулевых элементов строки.
''',
'''
Прямые методы обеспечивают точное решение линейных систем Ax=b, но для разреженных матриц необходимо учитывать особенности их структуры.
Метод Гаусса:
Классический метод неэффективен из-за заполнения нулями (заполнение ненулевыми значениями в процессе вычислений).
Для разреженных систем используются модификации, такие как разреженный метод Гаусса.
Метод LU-разложения:
Представление A=LU, где L — нижняя треугольная матрица, U — верхняя треугольная матрица.
Реализовано в библиотеках, например, в scipy.sparse.linalg.splu.
Проблемы: заполнение (fill-in) ненулевыми элементами при разложении.
QR-разложение:
Представление A=QR, где Q — ортогональная матрица, R — верхняя треугольная.
Обычно используется для численно устойчивых решений.
''',
'''
import numpy as np
import scipy.sparse as sp

# 1. Создадим разреженную матрицу в формате CSR (Compressed Sparse Row)
n = 5  # Размерность матрицы
data = np.array([4, 5, 7, 9])  # Ненулевые элементы
row_indices = np.array([0, 2, 3, 4])  # Индексы строк, где находятся ненулевые элементы
col_indices = np.array([0, 1, 2, 4])  # Индексы столбцов, где находятся ненулевые элементы

# Создание разреженной матрицы 5x5
A = sp.csr_matrix((data, (row_indices, col_indices)), shape=(n, n))
print("Матрица A (в CSR формате):\n", A)

# 2. Умножим разреженную матрицу на вектор
rhs = np.array([1, 2, 3, 4, 5])  # Вектор правой части
result = A.dot(rhs)  # Умножение разреженной матрицы на вектор
print("\nРезультат умножения A на вектор rhs:\n", result)

# 3. Преобразуем разреженную матрицу в формат COO (Coordinate List)
A_coo = A.tocoo()
print("\nМатрица A в COO формате:\n", A_coo)

# 4. Преобразуем разреженную матрицу в формат CSC (Compressed Sparse Column)
A_csc = A.tocsc()
print("\nМатрица A в CSC формате:\n", A_csc)

# 5. Добавление элементов в разреженную матрицу
B = A.copy()
B[0, 4] = 10  # Добавим элемент в строку 0, столбец 4
print("\nМатрица B после добавления нового элемента:\n", B)

# 6. Операция сложения разреженных матриц
C = A + B  # Сложение разреженных матриц
print("\nМатрица C (сумма A и B):\n", C)

# 7. Преобразование разреженной матрицы в плотную
A_dense = A.todense()
print("\nПлотная версия матрицы A:\n", A_dense)
''']],
14: ['14.	Обыкновенные дифференциальные уравнения, задача Коши. (Антон)', 
[
'''
Обыкновенные дифференциальные уравнения (ОДУ) - это уравнения, содержащие одну или несколько производных от искомой функции: F( x, у, у', у", y"…, y(")) = 0
где х- независимая переменная, у = у(х) - искомая функция.
Наивысший порядок производной n, входящей в предыдущее уравнение, называют порядком дифференциального уравнения
Рассмотрим систему обыкновенных дифференциальных уравнений (ОДУ) первого порядка, записанную в виде:y’(x) = f(x, y(x))
Решение: любая функция у(х), которая удовлетворяет уравнению. Решением ОДУ на интервале (a,b) называется функция у = ф(х), которая при ее подстановке в исходное уравнение обращает его в тождество на (а, b)
Решение ОДУ в неявном виде Ф(х, у) = 0, называется интегралом ОДУ.

''',
'''
Существует множество возможных решений. Для одного уникального решения необходимо указать независимые условия (для системы размером n).
Например, когда п условий заданы для одной точки. 
y(0) = y_0
Это задача Коши (задача с начальными условиями). Либо дифференциальная задача.
Дифференциальная задача: Lu = f (1), где L - дифференциальный оператор, y - точное решение, f - правая часть
Разностная задача: L_h*u_h=f_h (2), где L_h - разностная функция, y_h - приближ. решение (сеточная функция), f_h - проекция правой части (на сетку).

''',
'''
Опр 1: Результат постановки сеточной проекции точного решения (1) в разностную схему (2) называется невязкой r_h = L_h*[u]_h - f_h
Опр 2: Если ||r_n||_(h->0) -> 0, то разностная задача (2) аппроксимирует дифференциальную задачу (1). Если ||r_n|| <= Ch^p, то аппроксимация порядка р
Опр 3: Будем говорить, что решение разностной схемы устойчиво, если для решения двух возмущенных задач L_n*u^1_h = f_n + epsilon^1_h и L_n*u^2_h = f_n + epsilon^2_h справедлива оценка |u^1_h - u^2_h| <= C_1(||epsilon^1_h|| + ||epsilon^2_h||)
Опр 4: Будем говорить, что решение разностной задачи сходится к решению дифференциальной, если ||u_h - [u]_h|| <= C_2*h^p, где p - порядок сходимости
Теорема (Лакса-Рябенького-Филипова):
Пусть семейство разностных схем (2) аппроксимирует дифференциальную задачу (1) и устойчиво, тогда решение задачи (2) сходится к решению задачи (1). Причем, если аппроксимация имела порядок р, то и сходимость будет иметь порядок р.
В большинстве случаев поиск аналитического решения ОДУ первого порядка оказывается невозможным – приходится решать эту задачу численными методами.

''']],
15: ['15.	Локальная, глобальная ошибки. ', 
[
'''
Второе название метода Эйлера (про метод Эйлера см. Метод Эйлера) – «метод ломаных» обусловлено графической интерпретацией данного метода. Искомая функция y(x) заменяется ломаной линией с узлами в точках x0 , x1 ,..., xn . Метод Эйлера характеризуется достаточно высокой погрешностью вычисления: ∆ ≈ O(h). В дополнение, данный метод во многих случаях оказывается неустойчивым – малая ошибка (к примеру, заложенная в исходных данных) накапливается с увеличением x.
Точность метода Эйлера повышается при уменьшении размера шага вычислений h. Здесь также стоит отметить, что чрезмерно малое значение величины h приводит к снижению производительности вследствие увеличения количества вычислений: чем меньше шаг вычислений – тем больше итераций необходимо выполнить.
''',
'''
Локальные погрешности – погрешности, образовавшиеся на каждом шаге
Глобальная (накопленная) погрешность – погрешность, образовавшаяся за несколько шагов. Порядок глобальной погрешности относительно шага интегрирования на единицу ниже, чем порядок локальной погрешности. Таким образом, глобальная погрешность метода Эйлера имеет порядок p = 1: g_1 = C ∙ h, где C – некоторая постоянная. 
Порядок численного метода для решения ОДУ определяется порядком его глобальной погрешности. Он может быть также определен, как количество вычислений значения производной f(x, y) искомой функции на каждом шаге. В соответствии с этим метод Эйлера является методом первого порядка.

''']],
16: ['16.	Метод центральной разности. (Юра вроде норм, но вроде мало хз)', 
[
'''
Метод центральных разностей — это численный метод, который используется для аппроксимации производных функций на основе значений функции в соседних точках. Этот метод часто применяется в задачах численного решения дифференциальных уравнений, вычислительной физике, обработке сигналов и других областях, где требуется аппроксимация производных.
''',
'''
Рассмотрим функцию, которая гладкая в некоторой окрестности точки x. Производные можно приближать, используя значения функции в заданных узловых точках. Если шаг сетки равен h, то узловые точки: ..., x−3hx-3hx−3h, x−2hx-2hx−2h, x−hx-hx−h, xxx, x+hx+hx+h, x+2hx+2hx+2h, x+3hx+3hx+3h, ...
Используя теорему Тейлора, получаем:
f(x+h)=f(x)+f′(x)h+1/2 * f′′(ξ2)h^2
Приближение производной методом прямых разностей имеет вид:
f′(x)≈f(x+h)−f(x),
а ошибка составляет O(h)

''',
'''
Приближение производной методом центральных разностей более точно для гладких функций. Расширяя разложение Тейлора, получаем:
f(x+h)=f(x)+f′(x)h+1/2 * f′′(x)h^2+⅙ * f′′′(ξ3)h^3,
f(x−h)=f(x)−f′(x)h+1/ 2 * f′′(x)h^2−1/6 * f′′′(ξ3′)h^3
Приближение производной методом центральных разностей:
f′(x)≈( f(x+h)−f(x−h) ) / (2h)
а ошибка усечения составляет O(h^2).
Так как метод центральных разностей превосходит метод прямых разностей с точки зрения ошибки усечения, почему он не всегда используется? В некоторых случаях, например, в уравнениях конвекции-диффузии, использование центральных разностей может приводить к численной неустойчивости. 

''']],
17: ['17.	Метод Эйлера. ', 
[
'''
(Предысторию см. в “Обыкновенные дифференциальные уравнения, задача Коши.”)
Результатом решения ОДУ численными методами является таблица значений y = phi(x) (как русская ф нахуй) на некотором множестве значений аргумента х. Поэтому при постановке задачи численного решения ОДУ первого порядка наряду с начальными условиями x_0 , y_0 необходимо задать область решения – отрезок [a ; b] и шаг изменения аргумента h. 
Таким образом, численное решение ОДУ представляет собой таблицу значений искомой функции yi для заданной последовательности значений аргумента x_(i+1) = x_(i+h) , i = 0, 1, …, n, где h = x_(i+1)–x_i называется шагом интегрирования.
''',
'''
Разложим функцию y(x) в окрестности точки x_0 в ряд Тейлора:
y(x) = y(x_0) + (x - x_0)y'(x_0) + ((x - x_0)^2) / 2 * y''(x_0) + ...
который применяется для приближенного определения значения искомой функции y(x). В точке x_0 + h при малых значениях h достаточно использовать только два слагаемых ряда, получим:
y(x) = y(x_0 + h) = y(x_0) + y'(x_0) * Δx + O(h^2),
где O(h^2) — бесконечно малая величина порядка h^2. Преобразуем:
y(x_0 + h) ≈ y(x_0) + h * f(x_0).
Теперь приближенное решение в точке x_1 = x_0 + h может быть вновь рассмотрено как начальное условие, следовательно, используя формулу, можно найти значение искомой функции в следующей точке x_2 = x_1 + h. Таким образом, был получен простой алгоритм решения задачи Коши, называемый методом Эйлера или методом ломаных.
В общем виде формула Эйлера:
y_i  = y_(i-1) + h*f(x_(i-1),y_(i-1)); x_i = x_(i-1) + h

''',
'''
# Код метод Эйлера:
import math

def func(x, y):
    return y / (math.cos(x) ** 2) # dy/dx = y/(cosx)^2

def eiler(func, x0, xf, y0, h):
    count = int((xf - x0) / h) + 1
    y = [y0]
    x = x0
    for i in range(1, count):
        y.append(y[i - 1] + h * func(x, y[i - 1]))
        x += h
    return y

print(eiler(func, 0, 1, 2.7183, 0.1))

''']],
18: ['18.	Метод предиктора-корректора. ', 
[
'''
Рассмотрим еще одно семейство многошаговых методов, которые используют неявные схемы, – метод прогноза и коррекции (они называются также методами предиктор-корректор). Суть этих методов состоит в следующем. На каждом шаге вводятся два этапа, использующих многошаговые методы: 
1) с помощью явного метода (предиктора) по известным значениям функции в предыдущих узлах находится начальное приближение 𝑦_(𝑖+1) = 𝑦_(𝑖+1)^(0) в новом узле. 
2) используя неявный метод (корректор), в результате итераций находятся приближения y_(i+1) ^ (1) , y_(i+1) ^ (2) 

''',
'''
Один из вариантов метода прогноза и коррекции может быть получен на основе метода Адамса четвертого порядка: 
на этапе предиктора y_(i+1) = y_i + h/24 * (55 * f_i - 59 * f_(i-1) + 37 * f_(i - 2) - 9 * f_(i - 3)) 
на этапе корректора y_(i+1) = y_i + h/24 * (9 * f_(i+1) + 19 * f_i - 5*f_(i-1) + f_(i-2))
Явная схема используется на каждом шаге один раз, а с помощью неявной схемы строится итерационный процесс вычисления y_(i+1) , поскольку это значение входит в правую часть выражения f_(i+1) = f(x_(i+1) , y_(i+1)). Расчет по этому методу может быть начат только со значения y4 . Необходимые при этом y1 , y2 , y3 находятся по методу Рунге-Кутта, y0 задается начальным условием.

''']],
19: ['19.	Метод Рунге-Кутты 1-4 порядков.', 
[
'''
Метод Рунге-Кутты является широко используемым при интегрировании обыкновенных дифференциальных уравнений. По умолчанию речь идет о методе Рунге-Кутты четвертого порядка точности – из-за большой распространенности данного метода, указания на тип и порядок зачастую опускаются. При этом существуют еще методы первого, второго и третьего порядка точности.
''',
'''
Если нужен вывод блядской формулы, напишите его на чит листе
y_(k+1) = y_k + h * [(1-alpha) * f(x_k, y_k) + alpha * f (x_k + h / (2 * alpha), y_k + f(x_k, y_k) * 2 * h / alpha )

''',
'''
При α = 0 получаем как частный случай уже известную схему Эйлера 
y_(k+1) = y_k + h * f(x_k, y_k) - Схема 1 порядка

''',
'''
При α = 0,5 получаем как частный случай метод Рунге-Кутта второго порядка / схему Эйлера с пересчетом.
K1 = f(x_k, y_k)
K2 = f(x_k + h, y_k + h * K1)
y_(k+1) = y_k + h/2 * (K1 + K2)

''',
'''
Локальная погрешность метода Рунге–Кутты 2–го порядка e2 = C∙h3 , где C – некоторая постоянная, и пропорциональна кубу шага интегрирования: при уменьшении шага в 2 раза локальная погрешность уменьшится в 8 раз.

''',
'''
Схема третьего порядка:


K1 = f(x_k, y_k)
K2 = f(x_k + h/2, y_k + h/2 * K1)
K3 = f(x_k + h, y_k - h * K1 + 2 * h * K2)
y_(k+1) = y_k + h/6 * (K1 + 4 * K2 + K3)
''',
'''
Схема 4 порядка:
K1 = f(x_k, y_k)
K2 = f(x_k + h/2, y_k + h/2 * K1)
K3 = f(x_k + h / 2, y_k +  h/2 * K2)
K4 = f(x_k + h, y_k + h * K3)
y_(k+1) = y_k + h/6 * (K1 + 2 * K2 + 2*K3 + K4)

'''
]],
20: ['20.	Методы Адамса-Мултона, методы Адамса-Бэшфорта. (Маркус (из лекции))', 
[
'''
Широко распространенным семейством многошаговых методов являются методы Адамса. (Простейший из них, получающийся при k = 1, совпадает с рассмотренным ранее методом Эйлера первого порядка точности.) В практических расчетах чаще всего используется вариант метода Адамса 4-го порядка. Именно его обычно и называют методом Адамса. Рассмотрим этот метод.

Явный метод Адамса:
Использует предыдущие значения 𝑦_𝑛, 𝑦_𝑛−1, … для аппроксимации следующего значения 𝑦_𝑛+1. Формула для трехшагового метода Адамса-Башфорта:
y_n+1 = y_n + h/12 * (23*f(t_n, y_n) - 16*f(t_n-1) + 5*f(t_n-2, y_n-2))
 
Неявный метод Адамса:
Использует текущие и будущие значения для более точного результата. Формула для трехшагового метода Адамса-Мултона:
y_n+1 = y_n + h/12 * ((5*f(t_n+1, y_n+1) + 8*f(t_n, y_n) - f(t_n-1, y_n-1)))

''',
'''
Явные методы
Преимущества: Простота реализации и вычислительная эффективность.
Недостатки: Ограниченная стабильность, особенно для жестких систем.
Неявные методы
Преимущества: Более высокая стабильность, подходящие для жестких систем.
Недостатки: Более сложная реализация и необходимость решения нелинейных уравнений на каждом шаге.

''',
'''
Дальше идёт описание этого метода, ваш выбор, писать его или нет
Пусть найдены значения y_i-3, y_i-2, y_i-1, y_i в четырех последовательных узлах (k = 4). При этом имеются также вычисленные ранее значения правой части уравнения
f_i-3 = f (x_i-3, y_i-3);
f_i-2 = f (x_i-2, y_i-2);
f_i-1 = f (x_i-1, y_i-1);
f_i = f (x_i, y_i)
В качестве интерполяционного P_3 (x) можно взять многочлен Ньютона. В случае постоянного шага h конечные разности для правой части в узле x_i имеют вид:
(delta – треугольничек)
delta f_i = f_i – f_i-1
delta^2 f_i = f_i – 2*f_i-1 + f_i-2
delta^3 f_i = f_i – 3*f_i-1 + 3*f_i-2 – f_i-3

Разностная схема четвертого порядка метода Адамса записывается в виде:
y_i+1 = y_i + h*f_i + (h^2)/2 *delta f_i + (5*h^3)/12 *delta^2 f_i + (3*h^4)/8 *delta^3 f_i
Сравнивая метод Адамса с методом Рунге-Кутта той же точности, отмечаем его экономичность, поскольку он требует вычисления лишь одного значения правой части на каждом шаге (метод Рунге-Кутта – четырех).
При этом, метод Адамса неудобен тем, что невозможно начать счет по одному лишь известному значению y_0. Расчет может быть начат лишь с узла x_3.
 
Значения y_1, y_2, y_3, необходимые для вычисления y_4, нужно получить каким-либо другим способом (например, методом Рунге-Кутта), что существенно усложняет алгоритм.
Кроме того, метод Адамса не позволяет (без усложнения формул) изменить шаг h в процессе счета; этого недостатка лишены одношаговые методы.
Метод Адамса легко распространяется на системы дифференциальных уравнений.

''']],
21: ['21.	Метод Милна. (Маркус) (Всё отсюда https://www.bottomscience.com/milnes-method-python/)', 
[
'''
Метод Милна – это численный метод, используемый для решения обыкновенных дифференциальных уравнений (ОДУ) вида:
 y′(x) = f(x, y(x)),
 где y(x) – неизвестная функция, а f(x,y(x)) – заданная функция.
Это метод предиктор-корректор, что означает, что он использует начальное приближение ("предиктор") для оценки значения неизвестной функции в новой точке, а затем использует эту оценку для получения более точного значения ("корректор").

''',
'''
Метод основывается на разложении функции в ряд Тейлора и включает следующие шаги:
1.	Задано начальное значение y(x_0) = y_0 и шаг h. Используя одношаговый метод (например, метод Эйлера), вычисляется приближённое значение y(x_0 + h) = y_1.
2.	Используя y_1, вычисляется разложение функции y(x) в ряд Тейлора вокруг x_0, включая вторую производную:
y(x) = y_0 + y'(x_0)(x – x_0) + (1/2)y”(x_0)(x – x_0)^2 + O((x – x_0)^3)
3.	С помощью y_1 уточняется значение функции в точке x_0 + h, обозначаемое как y_2.
4.	Этот процесс повторяется для вычисления значений y(x_0 + 2h), y(x_0 + 3h) и так далее.

''',
'''
Метод Милна является методом четвёртого порядка, что означает, что глобальная ошибка метода имеет порядок O(h^5). Это делает его более точным по сравнению с методом Эйлера, у которого порядок ошибки O(h^2).
Стоит отметить, что метод Милна используется на практике реже, чем другие методы, такие как метод Рунге-Кутты или метод Адамса-Башфорта.

''',
'''
# Реализация в Python (https://www.bottomscience.com/milnes-method-python/):
import numpy as np

def milnes_method(f, y0, x0, x_end, h):
    # Инициализация массива для хранения решений
    x = np.arange(x0, x_end+h, h)
    y = np.zeros(len(x))
    y[0] = y0

    # Метод Милна [By Bottom Science]

    # Итерация по шагам
    for i in range(1, len(x)):
        # Предсказание значения (predictor)
        y_pred = y[i-1] + h*f(x[i-1], y[i-1])
        # Коррекция значения (corrector)
        y_corrected = y[i-1] + (h/4)*(3*f(x[i], y_pred) - f(x[i-1], y[i-1]))
        y[i] = y_corrected

    return x, y

# Определение функции ODE (дифференциального уравнения)
def f(x, y):
    return -y

# Задание начальных условий и шага
y0 = 1  # Начальное значение y(0) = 1
x0 = 0  # Начальное значение x
x_end = 10  # Конечное значение x
h = 0.1  # Шаг

# Решение ОДУ
x, y = milnes_method(f, y0, x0, x_end, h)

# Вывод результатов
for i in range(len(x)):
    print("x = ", x[i], " y = ", y[i])

''']],
22: ['22.	Согласованность, устойчивость, сходимость, условия устойчивости.', 
[
'''
Согласованность, устойчивость, сходимость и условия устойчивости — важные понятия в математическом анализе, теории численных методов и решении дифференциальных уравнений. Рассмотрим их более подробно:

1. Согласованность (Consistency)
Согласованность метода численного решения означает, что метод правильно приближает решение задачи, когда шаги в сетке (или шаги времени) стремятся к нулю. В контексте численных методов согласованность гарантирует, что ошибки, связанные с приближением (например, ошибка аппроксимации производных или интегралов), стремятся к нулю, когда шаг сетки становится все меньше.

Формально: метод численного решения задачи согласован, если погрешность на каждом шаге стремится к нулю, когда шаг сетки (или времени) стремится к нулю.
Пример: численное решение дифференциального уравнения с использованием метода Эйлера будет согласованным, если ошибка аппроксимации производной стремится к нулю при уменьшении шага времени.
''',
'''
2. Устойчивость (Stability)
Устойчивость метода численного решения означает, что погрешности, возникающие из-за ошибок округления или аппроксимации, не приводят к экспоненциальному росту ошибки в процессе вычислений. Иными словами, численный метод должен обеспечивать, чтобы ошибки не накапливались слишком быстро, не приводя к неверным результатам.

В контексте численных методов устойчивость часто определяется через анализ поведения ошибок в процессе итераций (или шагов времени). Если ошибки сохраняются в пределах разумных значений, то метод считается устойчивым.
Пример: в методах интегрирования, таких как метод Эйлера, если шаг времени слишком велик, то решение может стать неустойчивым, приводя к большим отклонениям от истинного решения.
''',
'''
3. Сходимость (Convergence)
Сходимость численного метода означает, что при уменьшении шага сетки (или шага времени) результат численного решения стремится к точному решению задачи. Метод численного решения сходится, если ошибка между численным решением и точным решением стремится к нулю, когда шаг сетки или времени стремится к нулю.

Формально: метод численного решения задачи сходится, если его ошибка стремится к нулю при стремлении шага к нулю.
Пример: метод Эйлера для дифференциальных уравнений с правильным выбором шага времени будет сходиться к точному решению, если шаг времени достаточно мал.
''',
'''
4. Условия устойчивости (Stability conditions)
Условия устойчивости описывают критерии, которые должны быть выполнены для того, чтобы численный метод был устойчивым. Эти условия зависят от используемой схемы (метода) и задачи, которую решают. Например, для метода Эйлера для решения дифференциальных уравнений существует условие, при котором шаг времени должен быть достаточно малым, чтобы метод оставался устойчивым.

Условия устойчивости могут включать ограничения на шаги по времени или пространству. Для линейных методов, таких как метод Эйлера или метод Кранка-Николсона, эти условия часто выражаются через спектральный радиус оператора.
Пример: для метода Эйлера условие устойчивости состоит в том, чтобы шаг времени был меньше, чем определенное значение, связанное с максимальным значением собственных чисел матрицы системы дифференциальных уравнений.
''']],
23: ['23.	Моделирование волны с использованием математических инструментов (амплитуда, период, длина волны, частота, Герц, дискретизация, частота дискретизации, фаза, угловая частота).', 
[
'''
Волны описываются математическими уравнениями, которые зависят от параметров, таких как амплитуда, частота, длина волны и другие. Гармоническая волна, наиболее часто используемая в теоретических и прикладных задачах, выражается в виде:
y(t)=A⋅sin⁡(ωt+ϕ)
где:
A — амплитуда, максимальная величина колебаний.
ω=2πf — угловая частота, скорость изменения фазы.
f — частота волны (в Герцах, Hz), количество циклов в секунду.
ϕ — фаза, смещение волны относительно начала координат.
t — время (или координата, если рассматривать пространственные волны).
''',
'''
Важные соотношения:
f = 1/T  (частота и период)
ω = 2πf  (угловая частота)
λ = v/f  (длина волны)
E ∝ A²   (энергия)
fs > 2fmax (теорема Котельникова-Найквиста)
Теоретическое применение:
Физика: волны используются для описания электромагнитных и акустических явлений, например, свет, звук, радиоволны.
Обработка сигналов: анализ временных рядов, компрессия данных, шумоподавление.
Инженерия: проектирование антенн, анализ вибраций и нагрузок.
Математика: моделирование сложных процессов с помощью теории Фурье и спектрального анализа.
''']],
24: ['24.	Дискретное преобразование Фурье, обратное дискретное преобразование Фурье их ограничения, симметрии в дискретном преобразовании Фурье.', 
[
'''
Разновидность преобразования Фурье, специально предназначенная для работы с дискретными сигналами. 
• Лежит в основе различных технологий спектрального анализа случайных процессов.
 В результате вычисления ДПФ случайного процесса (сигнала) получается лишь спектр его единственной (одной из возможных) реализаций, что обычно не представляет большого интереса. Для спектрального анализа случайных сигналов необходимо использовать усреднение спектра
 Методы спектрального анализа, в которых после усреднения сигнала используется только информация, извлеченная из самого входного сигнала, называются непараметрическими. Если при проведении усреднения случайного сигнала определена некоторая его статистическая модель, спектральный анализ будет также решать задачи определения параметров этой модели. Такие методы называются параметрическими.

''',
'''
Рассмотрим периодическую последовательность отсчетов {x(k)} с периодом N:
 𝑥(𝑘 +𝑁)= 𝑥(𝑘) для любого k. 
Поставим в соответствие этой последовательности сигнал из смещенных по времени дельта-функций: 
𝑠(𝑡) =sum((k = −∞; ∞) 𝑥(𝑘)𝛿(𝑡−𝑘𝑇)) также периодический с периодом NT. 
Спектр дискретного сигнала s(t) периодический с периодом 2, а поскольку и сигнал периодический, то его спектр дискретен с расстояниями между гармониками 2N..
 Применив к дискретному периодическому сигналу s(t) разложение в ряд Фурье, получим дискретное преобразование Фурье, т.е. разложение сигнала по гармоникам: 
X(n) = sum((k = 0; N-1) x(k)exp(-j * (2Pink/N)))
Обратное дискретное преобразование Фурье: 
x(k) = 1/N * sum((n = 0; N-1) X(n)exp(j * (2Pink/N)))

''',
'''
Ограничения ДПФ
Конечная длина сигнала: ДПФ применяется только к конечным последовательностям. Если сигнал длиннее, его приходится разрезать на сегменты.
Эффект наложения спектров: ДПФ предполагает периодичность сигнала, что может вызывать искажения (эффект наложения спектров или алиасинг) при недостаточной дискретизации.
Ограниченная разрешающая способность: Разрешение в частотной области зависит от длины N. Увеличение N улучшает частотное разрешение, но увеличивает вычислительные затраты.
Краевые эффекты: При отсутствии оконных функций могут возникать побочные лепестки в спектре.

''',
'''
Симметрии в ДПФ
ДПФ обладает важными свойствами симметрии, особенно для вещественных сигналов:
Герметичность (сопряженная симметрия): Если x[n]— вещественный сигнал, то:
X[k] =▁(X[N - k]) 
  где X[N−k]— комплексно сопряженное число для частоты N−k
Четные и нечетные компоненты:
Амплитудный спектр ∣X[k]|— четная функция.
Фазовый спектр — нечетная функция.
Периодичность:
X[k] = X[k+N]

''',
'''
import numpy as np

def dft(signal):
    N = len(signal)
    result = []
    for k in range(N):
        summation = 0
        for n in range(N):
            angle = -2j * np.pi * k * n / N
            summation += signal[n] * np.exp(angle)
        result.append(summation)
    return result

# Пример использования
signal = [1, 2, 3, 4]
result = dft(signal)
print("DFT:", result)

''']],
25: ['25.	Быстрое преобразование Фурье, его принципы, фильтрация сигнала с использованием быстрого преобразования Фурье.', 
[
'''
Быстрое преобразование Фурье (БПФ)
Быстрое преобразование Фурье (FFT) — это эффективный алгоритм вычисления дискретного преобразования Фурье (ДПФ) и его обратного преобразования. Оно значительно уменьшает количество вычислений, необходимых для выполнения ДПФ, особенно для длинных последовательностей.
Принципы работы БПФ
Основная идея БПФ заключается в использовании симметрии и периодичности экспоненциальных функций (комплексных коэффициентов) в формуле ДПФ. БПФ разбивает исходную последовательность на меньшие части, применяет к ним преобразование и объединяет результаты.
Ключевые принципы:
1.	Декомпозиция сигнала: Исходная последовательность разбивается на четные и нечетные элементы.
2.	Рекурсивное применение: БПФ применяет ДПФ к каждой подгруппе рекурсивно.
3.	Объединение результатов: Частичные результаты комбинируются с использованием "бабочек" (алгебраических операций с экспонентами).

''',
'''
Фильтрация сигнала с использованием БПФ
Фильтрация с использованием БПФ позволяет эффективно удалять шум или выделять определенные частоты.
Этапы фильтрации:
1.	Прямое преобразование:
○	Сначала применяется БПФ к входному сигналу x[n], чтобы получить спектр X[k]
2.	Модификация спектра:
○	Применяется фильтр к спектральным компонентам X[k] Это может быть:
■	Удаление высокочастотного шума (низкочастотный фильтр),
■	Удаление низкочастотных составляющих (высокочастотный фильтр),
■	Выделение определенного диапазона частот (полосовой фильтр).
3.	Обратное преобразование:
○	Применяется обратное быстрое преобразование Фурье (iFFT), чтобы восстановить временной сигнал x′[n]
Пример:
●	Для подавления шума высоких частот можно обнулить компоненты X[k]соответствующие частотам выше порогового значения.

''',
'''
import numpy as np

def fft(signal):
    N = len(signal)
    if N <= 1:
        return signal

    if N % 2 != 0:
        raise ValueError("Длина сигнала должна быть степенью двойки")

    # Разделяем сигнал на четные и нечетные элементы
    even_part = fft(signal[0::2])
    odd_part = fft(signal[1::2])

    # Вычисляем значения БПФ с использованием "бабочек"
    T = [np.exp(-2j * np.pi * k / N) * odd_part[k] for k in range(N // 2)]
    return [even_part[k] + T[k] for k in range(N // 2)] + \
           [even_part[k] - T[k] for k in range(N // 2)]

# Пример использования
signal = [1, 2, 3, 4, 0, 0, 0, 0]  # Длина сигнала = 8 (степень двойки)
result = fft(signal)
print("FFT:", result)

''']],
26: ['26.	Операции свёртки, связь с быстрым преобразованием Фурье, операции дискретной свёртки.', 
[
'''
Операции свёртки и их связь с быстрым преобразованием Фурье
Свёртка — это операция, широко используемая в цифровой обработке сигналов. Она вычисляет, как одна функция изменяет форму другой, и применяется для фильтрации, сглаживания и анализа сигналов.
Свёртка двух сигналов
Пусть x[n] — входной сигнал, а h[n] — импульсная характеристика фильтра. Свёртка y[n] вычисляется по формуле:
y[n]=∑_(m = 0)^(N - 1)   x[m] * h[N - m],n=0,1,…,N−1
Связь свёртки с БПФ
Свойство свёртки в частотной области упрощает вычисления:
	Теорема свёртки: Свёртка в временной области эквивалентна умножению спектров в частотной области. y[n]=DFT^−1(DFT(x[n]) * DFT(h[n]))

	Это позволяет использовать БПФ для ускорения вычисления свёртки:
	Выполняется БПФ для x[n] и h[n]
	Спектры перемножаются.
Применяется обратное БПФ для получения результата.

''',
'''
def discrete_convolution(x, h):
    N = len(x)
    M = len(h)
    result = [0] * (N + M - 1)

    for n in range(len(result)):
        for m in range(M):
            if 0 <= n - m < N:
                result[n] += x[n - m] * h[m]

    return result

# Пример использования
x = [1, 2, 3, 4]
h = [0.2, 0.5, 0.8]
result = discrete_convolution(x, h)
print("Convolution result:", result)

''']],
27: ['27.	Дискретная свёртка и Тёплицевы матрицы (Ганкелевы матрицы).', 
[
'''
Дискретная свёртка — операция, определяющаяся на двух дискретных функциях или последовательностях. Она часто используется в обработке сигналов, анализе временных рядов и машинном обучении.
Формально, для двух последовательностей f[n] и g[n], дискретная свёртка h[n] определяется как:
h[n]=(f∗g)[n] = \sum_{k=-\infty}^{\infty} f[k] * g[n-k]
На практике, для конечных последовательностей длины N и M, свёртка вычисляется как:
h[n]=  \sum_{k=0}^{N-1} f[k] \cdot g[n-k]
где индекс n−k берётся циклически, если используются граничные условия с циклическим продолжением (например, в случае круговой свёртки), или сводится к нулю для n−k вне допустимого диапазона.
Тёплицева матрица — это квадратная матрица, у которой все элементы на диагоналях, параллельных главной диагонали, одинаковы. Это удобно для представления свёртки как матричного умножения.
- Структура: ti,j = t(i-j)
- Пример:
[t0  t-1 t-2]
[t1  t0  t-1]
[t2  t1  t0]
Ганкелева матрица отличается от Тёплицевой тем, что элементы на антидиагоналях (пересекающих главную диагональ под углом 90°) одинаковы. Это часто используется в задачах интерполяции, анализа сигналов и решения систем линейных уравнений.
- Структура: hi,j = h(i+j)
- Пример:
[h0  h1  h2]
[h1  h2  h3]
[h2  h3  h4]
''',
'''
Связь с дискретной свёрткой:
- Умножение Тёплицевой матрицы на вектор эквивалентно дискретной свёртке
- T·x = h * x, где h - последовательность, определяющая Тёплицеву матрицу


Применения:
- Цифровая обработка сигналов
- Фильтрация
- Решение разностных уравнений
- Быстрые алгоритмы умножения


Вычислительные аспекты:
- Быстрое умножение на вектор (O(n log n) с FFT)
- Эффективное хранение (O(n) вместо O(n²))
- Специальные алгоритмы для систем уравнений
''',
'''
import numpy as np

# Входные данные
f = np.array([1, 2, 3, 4])  # Сигнал
g = np.array([0.22, 0.5, 1, 0.5, 0.11])  # Ядро свёртки

# Длина результата
n = len(f) + len(g) - 1

# Создание Тёплицевой матрицы вручную
toeplitz_matrix = np.zeros((n, len(f)))
for i in range(n):
    for j in range(len(f)):
        if 0 <= i - j < len(g):
            toeplitz_matrix[i, j] = g[i - j]

# Вычисление свёртки через матричное умножение
convolution_result = toeplitz_matrix @ f

# Результат
print("Сигнал f:", f)
print("Ядро g:", g)
print("Результат свёртки:", convolution_result)
''']],
28: ['28.	Циркулянтные матрицы. Матрицы Фурье. (Дамир)', 
[
'''
Циркулянтные матрицы
Определение:
Циркулянтная матрица — это квадратная матрица C, каждая строка которой является циклическим сдвигом предыдущей. Если первая строка имеет элементы [c0,c1,…,cn−1], то матрица C задается как:
	[c0,c1,…,cn−1]
	[cn-1,c1,…,cn−2]
C = 	[cn-2,cn-1,…,cn−3]
		…
		[c1,c2,…,c0]
Свойства:
1.	Коммутативность: Произведение циркулянтных матриц коммутативно. C_1*C_2=C_2*C_1
2.	Диагонализация через матрицы Фурье: Любая циркулянтная матрица диагонализуема, и ее собственные значения можно вычислить с помощью дискретного преобразования Фурье (ДПФ).
3.	Применение в свертках: В цифровой обработке сигналов циркулянтные матрицы представляют свертки сигналов.

''',
'''
Матрицы Фурье
Определение:
Матрица Фурье F используется для вычисления ДПФ. Она имеет размер n×n, и ее элементы задаются формулой:
F = (1 / sqrt(n)) * e ^ (-2Пi*(j*k/n)),     j,k=0,1,…,n−1.
Или в явной форме:
1   1            1              …  1
1   w           w^2          …  w^(n-1)
1   w^2       w^4          …  w^2(n-1)
……………………………………….
1   w^(n - 1) w^2(n-1) … w^(n-1)(n-1)
Где w = e^(-2Пi/n)
Свойства:
1.	Унитарность: Матрица F унитарна, то есть F*F†=I, где F† — эрмитово-сопряженная матрица F.
2.	Диагонализация циркулянтных матриц: Если C — циркулянтная матрица, то существует представление: C=F†*Λ*F,где Λ — диагональная матрица, элементы которой — собственные значения C, вычисляемые с помощью ДПФ.
3.	Прямое и обратное ДПФ:
●	Прямое преобразование: x_hat = F*x.
●	Обратное преобразование: x = F†*x_hat.

''',
'''
Связь между циркулянтными матрицами и матрицами Фурье
1.	Диагонализация: Матрица Фурье F используется для диагонализации циркулянтной матрицы C. Собственные значения циркулянтной матрицы можно найти, применяя ДПФ к ее первой строке:
Λ=diag(FFT(c0,c1,…,cn−1)).
2.	Спектральное представление: Циркулянтные матрицы удобно описывать в частотной области через их собственные значения, которые соответствуют спектру сигнала.
3.	Быстрые вычисления: Использование матриц Фурье позволяет эффективно выполнять операции с циркулянтными матрицами (например, умножение на вектор) с помощью алгоритма FFT с вычислительной сложностью O(nlog⁡n) вместо O(n^2).

''']],
29: ['29.	Быстрый матвек с циркулянтом. ', 
[
'''
Циркулянтная матрица — это специальный тип матрицы, в которой каждая строка является циклическим сдвигом предыдущей. Использование циркулянтной матрицы позволяет оптимизировать вычисления благодаря их структуры и свойствам, связанных с преобразованием Фурье.
Свойства циркулянтных матриц
Если матрица C является циркулянтной, то:
1.	Умножение C*x можно свести к дискретному преобразованию Фурье (DFT), что ускоряет вычисления до O(nlog⁡n).
2.	Спектральное разложение позволяет выразить матрицу через собственные значения, которые вычисляются через DFT от первого столбца.

''',
'''
Быстрое умножение циркулянтной матрицы на вектор
Для циркулянтной матрицы C и вектора x:
1.	Вычисляется DFT от первого столбца матрицы C.
2.	Вычисляется DFT от вектора x.
3.	Выполняется покоординатное умножение спектров.
4.	Выполняется обратное DFT для получения результата.

''',
'''
import cmath
import numpy as np
import math

def fft(signal):
	"""
	Быстрое преобразование Фурье (FFT) с автоматическим дополнением до степени двойки.
    
	:param signal: список комплексных чисел (входной сигнал)
	:return: список комплексных чисел (спектр сигнала)
	"""
	# Дополнение сигнала до степени двойки
	n = len(signal)
	if n <= 1:  # Базовый случай
    	return signal
    
	# Разделяем на чётные и нечётные элементы
	even_part = fft(signal[0::2])
	odd_part = fft(signal[1::2])
    
	# Вычисляем FFT
	result = [0] * n
	for k in range(n // 2):
    	t = cmath.exp(-2j * cmath.pi * k / n) * odd_part[k]
    	result[k] = even_part[k] + t
    	result[k + n // 2] = even_part[k] - t
    
	return np.array(result)

def ifft(spectrum):
	"""
	Обратное быстрое преобразование Фурье (IFFT).
    
	:param spectrum: список комплексных чисел (спектр сигнала)
	:return: список комплексных чисел (восстановленный сигнал)
	"""
	n = len(spectrum)
	if n <= 1:  # Базовый случай
    	return spectrum
    
	# Разделяем на чётные и нечётные элементы
	even_part = ifft(spectrum[0::2])
	odd_part  = ifft(spectrum[1::2])
    
	# Вычисляем IFFT
	result = [0] * n
	for k in range(n // 2):
    	t = cmath.exp(2j * cmath.pi * k / n) * odd_part[k]
    	result[k] = even_part[k] + t
    	result[k + n // 2] = even_part[k] - t
    
	return np.array([x for x in result])

def circulant_multiply(c_first_col, x):
	"""
	Быстрое умножение циркулянтной матрицы на вектор.
    
	:param c_first_col: первый столбец циркулянтной матрицы
	:param x: вектор, на который нужно умножить
	:return: результат умножения (вектор)
	"""
	# Размер матрицы и вектора
	n = len(c_first_col)
	if len(x) != n:
    	raise ValueError("Длина вектора должна совпадать с размером матрицы.")
    
	# Вычисление спектров
	c_spectrum = fft(c_first_col)  # DFT от первого столбца матрицы
	x_spectrum = fft(x)        	 # DFT от вектора

	# Покоординатное умножение в спектральной области
	result_spectrum = c_spectrum * x_spectrum
    
	# Обратное преобразование для получения результата
	result = ifft(result_spectrum).real / n # Извлекаем действительную часть и нормализуем
    
	return result

# Пример использования
c_first_col = [1, 2, 3, 4]  # Первый столбец циркулянтной матрицы
x = [5, 6, 7, 8]            # Вектор, на который умножаем

result = circulant_multiply(c_first_col, x)
print("Результат умножения:", result)

''',
'''

''']],

            
        
        }

        self.themes = [{values[0]:keys} for keys,values in self.sklad.items()]
    
    def search(self, text):
        ress = []
        for theme in self.themes:
            for key, value in theme.items():
                if text in key:
                    ress += [f"{key} : {value}"]
        return ress

