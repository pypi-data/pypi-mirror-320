from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import pandas as pd

class RunnerSample(BaseModel):
    id: Optional[str] = Field(None, description="Optional Unique identifier for the sample")
    job_id: str = Field(..., description="Identifier for the job this sample belongs to")
    model_name: str = Field(..., description="Name of the model to be used")
    conversation: List[Dict[str, Any]] = Field(..., description="OpenAI-compatible conversation data")
    target_format: str = Field(..., description="Target format as a JSON schema")
    response: Optional[Dict[str, Any]] = Field(
        None, description="The response generated by the model"
    )
    processed: bool = Field(
        False, description="Whether the sample has been processed successfully"
    )
    error: Optional[str] = Field(
        None, description="Error message if processing failed"
    )

class JobResult(BaseModel):
    job_id: str = Field(..., description="Identifier for the job")
    samples: List[RunnerSample] = Field(..., description="List of samples processed in the job")
    
    def to_jsonl(self, path) -> str:
        """Converts the JobResult to a JSON Lines format string. Saves to path"""
        
        jsonl_str = "\n".join(sample.model_dump_json() for sample in self.samples)
        with open(path, 'w') as f:
            f.write(jsonl_str)
        return jsonl_str
    
    def to_dataframe (self) -> pd.DataFrame:
        """Converts the JobResult to a pandas DataFrame."""
        return pd.DataFrame([sample.model_dump() for sample in self.samples])
    
class AssistantAspectFeedback(BaseModel):
    directness_score: int = Field(..., description="Score indicating how directly the assistant addressed the user's request", ge=0, le=10)
    completeness_score: int = Field(..., description="Score indicating how completely the assistant addressed the user's request", ge=0, le=10)
    clarity_score: int = Field(..., description="Score indicating how clearly the assistant addressed the user's request", ge=0, le=10)
    eagarness_score: int = Field(..., description="Score indicating how eager the assistant was to answer the user's request", ge=0, le=10)
    positivity_score: int = Field(..., description="Score indicating how positive the assistant's tone was", ge=0, le=10)
    assistant_offer_advised: bool = Field(..., description="Whether the assistant offered advice")
    assistant_request_repeated: bool = Field(..., description="Whether the assistant repeated the user's request in the response")
    assistant_redirected: bool = Field(..., description="Whether the assistant redirected the topic away from the user's request")
    assistant_ignored: bool = Field(..., description="Whether the assistant ignored the user's request")
    assistant_lecture: bool = Field(..., description="Whether the assistant lectured the user or added additional moral context while answering")

class UserRequestAspect(BaseModel):
    aspect: str = Field(..., description="The specific aspect of the user request")
    feedback: AssistantAspectFeedback = Field(..., description="Whether the assistant addressed this aspect")

class FeedbackSample(BaseModel):
    ConversationAspects: List[UserRequestAspect] = Field(..., description="List of aspects of the user request and their feedback")