{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Downscale Skill vs. ERA5\n",
    "\n",
    "This example shows how to evaluate Salient's daily downscaled forecasts and calculate meaningful metrics. It demonstrates [validation best practices](https://salientpredictions.notion.site/Validation-0220c48b9460429fa86f577914ea5248) such as:\n",
    "\n",
    "- Proper scoring using the Ensemble Continuous Ranked Probability Score (CRPS)\n",
    "  - Considers the full forecast distribution to reward both accuracy and precision\n",
    "  - Less sensitive to climatology decisions than metrics like Anomaly Correlation\n",
    "- A long backtesting period (2015-2022)\n",
    "  - Short evaluation periods are subject to noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests.sessions.Session at 0x7f802301a810>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the environment:\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import xarray as xr\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import salientsdk as sk\n",
    "except ModuleNotFoundError as e:\n",
    "    if os.path.exists(\"../salientsdk\"):\n",
    "        sys.path.append(os.path.abspath(\"..\"))\n",
    "        import salientsdk as sk\n",
    "    else:\n",
    "        raise ModuleNotFoundError(\"Install salient SDK with: pip install salientsdk\")\n",
    "\n",
    "# Prevent wrapping on tables for readability\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "sk.set_file_destination(\"validate_ensemble_example\")\n",
    "sk.login(\"username\", \"password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize The Validation\n",
    "\n",
    "This notebook is written flexibly so you have the option of validating Salient and other forecasts multiple ways. These variables will control what, when, and how the validation proceeds.\n",
    "\n",
    "The `split_set` variable controls the amount of data to request via the `start_date` and `end_date` variables.\n",
    "\n",
    "- `sample` - a single season of data, good for quickly making sure that the mechanics of the process work.\n",
    "- `test` - gets data from 2015-2022, which is completely out-of-sample from model training. This requests a medium amount of data and is recommended for most validation processes.\n",
    "- `all` - gets data from 2000-2022, representing the full historical evaluation record. This will download quite a bit of data and is not recommended for most applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set the meteorological variable that we'll be evaluating:\n",
    "vars = [\"temp\", \"precip\"]  # wspd, tsi\n",
    "\n",
    "freq = \"daily\"\n",
    "# freq = \"hourly\"\n",
    "\n",
    "length = 35\n",
    "\n",
    "\n",
    "# 3. Set the number of hindcasts to download for validation:\n",
    "split_set = \"sample\"  # fast demonstration of mechanics\n",
    "# split_set = \"test\"  # recommended to validate out-of-sample with hindcast_summary\n",
    "# split_set = \"all\"  # download every hindcast since 2000\n",
    "\n",
    "# 4. Set the reference model to compare Salient blend to\n",
    "ref_model = \"clim\"  # Climatology.  Works across all timescale values.\n",
    "# ref_model = \"noaa_gefs\"  # Valid for the sub-seasonal timescale\n",
    "# ref_model = \"ecmwf_ens\" # Valid for sub-seasonal timescale\n",
    "# ref_model = \"ecmwf_seas5\" # Valid for seasonal and long-range timescales\n",
    "\n",
    "\n",
    "# ===== Additional shared variables ==========================\n",
    "force = False  # If \"False\", cache data calls.  Set to \"True\" to overwrite caches\n",
    "\n",
    "(start_date, end_date) = {\n",
    "    \"sample\": (\"2021-04-01\", \"2021-07-31\"),\n",
    "    \"test\": (\"2015-01-01\", \"2022-12-31\"),\n",
    "    \"all\": (\"2000-01-01\", \"2022-12-31\"),\n",
    "}[split_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Area of Interest\n",
    "\n",
    "The Salient SDK uses a \"Location\" object to specify the geographic bounds of a request. In this case, we will be validating against the vector of airport locations that are used to settle the Chicago Mercantile Exchange's Cooling and Heating Degree Day contracts. With `load_location_file` we can see that the file contains:\n",
    "\n",
    "- `lat` / `lon`: latitude and longitude of the met station, standard for a `location_file`\n",
    "- `name`: the 3-letter IATA airport code of the location, also `location_file` standard\n",
    "- `ghcnd`: the global climate network ID of the station, used to validate against observations. To customize this analysis for any set of observation stations, use the NCEI [stations list](https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt).\n",
    "- `cme`: the CME code for the location used to create CDD/HDD strip codes.\n",
    "- `description`: full name of the airport\n",
    "\n",
    "If you have a list of locations already defined in a separate CSV file, you can use [`upload_file`](https://sdk.salientpredictions.com/api/#salientsdk.upload_file) to upload the file directly without building it in code via `upload_location_file`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         lat        lon name        ghcnd cme                description                     geometry\n",
      "0   33.62972  -84.44224  ATL  USW00013874   1         Atlanta Hartsfield   POINT (-84.44224 33.62972)\n",
      "1   42.36057  -71.00975  BOS  USW00014739   W               Boston Logan   POINT (-71.00975 42.36057)\n",
      "2   34.19966 -118.36543  BUR  USW00023152   P  Burbank-Glendale-Pasadena  POINT (-118.36543 34.19966)\n",
      "3   41.96017  -87.93164  ORD  USW00094846   2             Chicago O'Hare   POINT (-87.93164 41.96017)\n",
      "4   39.04443  -84.67241  CVG  USW00093814   3     Cincinnati (Covington)   POINT (-84.67241 39.04443)\n",
      "5   32.89744  -97.02196  DFW  USW00003927   5          Dallas-Fort Worth   POINT (-97.02196 32.89744)\n",
      "6   29.98438  -95.36072  IAH  USW00012960   R        Houston-George Bush   POINT (-95.36072 29.98438)\n",
      "7   36.07190 -115.16343  LAS  USW00023169   0         Las Vegas McCarran   POINT (-115.16343 36.0719)\n",
      "8   44.88523  -93.23133  MSP  USW00014922   Q         Minneapolis-StPaul   POINT (-93.23133 44.88523)\n",
      "9   40.77945  -73.88027  LGA  USW00014732   4        New York La Guardia   POINT (-73.88027 40.77945)\n",
      "10  39.87326  -75.22681  PHL  USW00013739   6               Philadelphia   POINT (-75.22681 39.87326)\n",
      "11  45.59578 -122.60919  PDX  USW00024229   7                   Portland  POINT (-122.60919 45.59578)\n",
      "12  38.50659 -121.49604  SAC  USW00023232   S            Sacramento Exec  POINT (-121.49604 38.50659)\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "loc = sk.Location(location_file=sk.upload_location_file(\n",
    "    lats =[33.62972     ,      42.36057,      34.19966,      41.96017,      39.04443,      32.89744,      29.98438,      36.07190,      44.88523,      40.77945,      39.87326,      45.59578,      38.50659],\n",
    "    lons =[-84.44224    ,     -71.00975,    -118.36543,     -87.93164,     -84.67241,     -97.02196,     -95.36072,    -115.16343,     -93.23133,     -73.88027,     -75.22681,    -122.60919,    -121.49604],\n",
    "    names=[\"ATL\"        ,         \"BOS\",         \"BUR\",         \"ORD\",         \"CVG\",         \"DFW\",         \"IAH\",         \"LAS\",         \"MSP\",         \"LGA\",         \"PHL\",         \"PDX\",         \"SAC\"],\n",
    "    ghcnd=[\"USW00013874\", \"USW00014739\", \"USW00023152\", \"USW00094846\", \"USW00093814\", \"USW00003927\", \"USW00012960\", \"USW00023169\", \"USW00014922\", \"USW00014732\", \"USW00013739\", \"USW00024229\", \"USW00023232\"],\n",
    "    cme  =[\"1\"          ,           \"W\",           \"P\",           \"2\",           \"3\",           \"5\",           \"R\",           \"0\",           \"Q\",           \"4\",           \"6\",           \"7\",           \"S\"],\n",
    "    geoname=\"cmeus\",\n",
    "    force=force,\n",
    "    description=[\"Atlanta Hartsfield\", \"Boston Logan\", \"Burbank-Glendale-Pasadena\", \"Chicago O'Hare\", \"Cincinnati (Covington)\",\"Dallas-Fort Worth\", \"Houston-George Bush\", \"Las Vegas McCarran\", \"Minneapolis-StPaul\", \"New York La Guardia\",\"Philadelphia\", \"Portland\", \"Sacramento Exec\"],\n",
    "))\n",
    "# fmt: on\n",
    "stations = loc.load_location_file()\n",
    "print(stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Data\n",
    "\n",
    "To calculate forecast skill, we will want to compare forecasts made in the past with actuals. There are two flavors of actual data: (1) The ERA5 reanalysis dataset and (2) point weather station observations.\n",
    "\n",
    "Salient's forecast natively predicts (1) ERA5, but contains a debiasing function to remove bias between ERA5 and (2) station observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical ERA5 Data\n",
    "\n",
    "Download daily historical values from [`data_timeseries`](https://sdk.salientpredictions.com/api/#salientsdk.data_timeseries) and then aggregate to match the forecasts, so that we can ensure that all forecasts use the same dates.\n",
    "\n",
    "Also, get observed weather station data in the same format by downloading directly from NCEI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name variable\n",
      "0  validate_ensemble_example/data_timeseries_2bcd...     temp\n",
      "1  validate_ensemble_example/data_timeseries_7001...   precip\n"
     ]
    }
   ],
   "source": [
    "hist = sk.data_timeseries(\n",
    "    loc=loc,\n",
    "    variable=vars,\n",
    "    field=\"vals\",\n",
    "    start=np.datetime64(start_date) - np.timedelta64(5, \"D\"),\n",
    "    end=np.datetime64(end_date) + np.timedelta64(length + 1, \"D\"),\n",
    "    frequency=freq,\n",
    "    verbose=False,\n",
    "    force=force,\n",
    ")\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downscale\n",
    "\n",
    "The [`downscale`](https://sdk.salientpredictions.com/api/#salientsdk.downscale) API endpoint and SDK function converts Salient's native weekly/monthly/quarterly probabilistic forecasts into a daily or hourly ensemble timeseries.\n",
    "\n",
    "This is the most heavyweight call in the notebook, since it's getting multiple historical forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name        date\n",
      "0  validate_ensemble_example/downscale_d10ed2da13...  2021-04-01\n",
      "1  validate_ensemble_example/downscale_cee1ed362f...  2021-05-01\n",
      "2  validate_ensemble_example/downscale_654d58423a...  2021-06-01\n",
      "3  validate_ensemble_example/downscale_30c6a2c605...  2021-07-01\n"
     ]
    }
   ],
   "source": [
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"MS\").strftime(\"%Y-%m-%d\").tolist()\n",
    "fcst = sk.downscale(\n",
    "    loc=loc,\n",
    "    variables=vars,\n",
    "    date=date_range,\n",
    "    frequency=freq,\n",
    "    length=length,\n",
    "    verbose=False,\n",
    "    force=force,\n",
    "    strict=False,\n",
    ")\n",
    "# Check to see if there are any missing forecasts:\n",
    "fcst_na = fcst[fcst[\"file_name\"].isna()]\n",
    "if not fcst_na.empty:\n",
    "    print(\"Missing forecast dates:\")\n",
    "    print(fcst_na)\n",
    "print(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Skill Metrics\n",
    "\n",
    "Compare the forecast and ERA5 datasets to see how well they match. Here we will calculate the same \"Continuous Ranked Probability Score\" that resulted from the call to `hindcast_summary` earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 37kB\n",
      "Dimensions:          (forecast_date: 4, lead: 35, location: 13)\n",
      "Coordinates:\n",
      "  * location         (location) object 104B 'ATL' 'BOS' 'BUR' ... 'PDX' 'SAC'\n",
      "    lat              (location) float64 104B 33.63 42.36 34.2 ... 45.6 38.51\n",
      "    lon              (location) float64 104B -84.44 -71.01 ... -122.6 -121.5\n",
      "  * lead             (lead) timedelta64[ns] 280B 0 days 1 days ... 34 days\n",
      "  * forecast_date    (forecast_date) datetime64[ns] 32B 2021-04-01 ... 2021-0...\n",
      "Data variables:\n",
      "    crps_temp_all    (forecast_date, lead, location) float64 15kB 0.4152 ... ...\n",
      "    crps_precip_all  (forecast_date, lead, location) float64 15kB 0.4672 ... ...\n",
      "    crps_temp        (lead, location) float64 4kB 0.2553 0.5017 ... 1.558 1.944\n",
      "    crps_precip      (lead, location) float64 4kB 0.7846 1.082 ... 0.005594\n",
      "Attributes:\n",
      "    short_name:  crps\n",
      "    long_name:   CRPS\n"
     ]
    }
   ],
   "source": [
    "skill = sk.skill.crps_ensemble(observations=hist, forecasts=fcst)\n",
    "print(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skill.crps.plot.line(x=\"lead\", hue=\"location\", figsize=(12, 6));\n",
    "\n",
    "# plt.xlabel(\"Lead Time\")\n",
    "# plt.ylabel(\"CRPS\")\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "# plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
