Metadata-Version: 2.3
Name: fruitstand
Version: 0.3.0
Summary: A library for regression testing llm prompts
Author: gareth@deckarddesigns.com
Requires-Python: >=3.13,<4
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.13
Requires-Dist: argparse (>=1.4.0,<2.0.0)
Requires-Dist: numpy (>=2.2.1,<3.0.0)
Requires-Dist: openai (>=1.59.6,<2.0.0)
Description-Content-Type: text/markdown

# Why is this library called fruitstand?

Because we’re comparing apples and oranges! Most testing involves checking that a value returned from a function is the same as what is expected. This doesn’t work when working with LLMs, as they are nondeterministic. Therefore, you need to check that there is a threshold of similarity.

# Why would I want to use this library?

If you’re using a particular LLM model prompt to determine an action based on the response, you may want to ensure that a response has a certain similarity when transitioning between models. This library allows you to set a baseline with the current model and verify that upgrading or changing models maintains the same behavior.

For example, you are using an LLM to do intent detection for your chatbot. You have a prompt like this:

```
Based on the provided user prompt, determine if the user wanted to:
1. Change their address
2. Change their name
3. Cancel their subscription

User Prompt:
I would like to update my subscription.
```

Using fruitstand will ensure that the LLM routes to the correct intent as you upgrade/change your llm/model.

