import base64
import json
import typing as pt
from dataclasses import asdict, dataclass
from typing import Any

from py4j.protocol import Py4JError

from bodo_iceberg_connector.catalog_conn import (
    gen_file_loc,
    normalize_data_loc,
    normalize_loc,
    parse_conn_str,
)
from bodo_iceberg_connector.puffin import StatisticsFile
from bodo_iceberg_connector.py4j_support import (
    convert_dict_to_java,
    convert_list_to_java,
    get_bodo_arrow_schema_utils_class,
    get_bodo_iceberg_handler_class,
    get_catalog,
)
from bodo_iceberg_connector.schema_helper import (
    arrow_schema_j2py,
    arrow_to_iceberg_schema,
    convert_arrow_schema_to_large_types,
)

if pt.TYPE_CHECKING:
    import pyarrow as pa


@dataclass
class DataFileInfo:
    """
    Python Representation of the DataFileInfo class on Java's side
    Used for communicating between Python and Java by transforming
    objects into JSON form
    """

    path: str
    file_size_in_bytes: int
    metrics: dict[str, Any]


class BytesEncoder(json.JSONEncoder):
    """
    JSON Encoder for bytes objects for lower/upper bound.
    """

    def default(self, o):
        if isinstance(o, bytes):
            return base64.b64encode(o).decode("ascii")
        else:
            return super().default(o)


def process_file_infos(
    fnames: list[str],
    file_size_bytes: list[int],
    metrics: list[dict[str, Any]],
    table_loc: str,
    db_name: str,
    table_name: str,
):
    """
    Process file name and metrics to a Java objects that can be passed
    to the connector.

    Args:
        fnames: List of file paths (possibly relative or absolute)
        metrics: Metrics about written data to include in commit
        table_loc: Warehouse location of data/ path (and files)
        db_name: Namespace / Database schema containing Iceberg table
        table_name: Name of Iceberg table

    Returns:
        JSON String Representing DataFileInfo objects
    """

    fnames = [gen_file_loc(table_loc, db_name, table_name, name) for name in fnames]
    file_infos = [
        asdict(DataFileInfo(fname, size, count))
        for fname, size, count in zip(fnames, file_size_bytes, metrics)
    ]
    return json.dumps(file_infos, cls=BytesEncoder)


def get_schema_with_init_field_ids(schema: "pa.Schema") -> "pa.Schema":
    """
    Python wrapper around BodoIcebergHandler.getInitSchema.
    See the docstring of that function for more details about
    why this is required.

    Args:
        schema (pa.Schema): Original schema created by Bodo. This
            must have Iceberg Field IDs embedded in the metadata
            of the fields.

    Returns:
        pa.Schema: Same schema, except the field IDs (in the field
        metadata) would be as if generated by Iceberg Java Library in
        the final metadata during commit.
    """
    handler_class = get_bodo_iceberg_handler_class()
    bodo_arrow_schema_utils_class = get_bodo_arrow_schema_utils_class()
    # Get the modified schema in Iceberg Schema format:
    init_iceberg_schema = handler_class.getInitSchema(arrow_to_iceberg_schema(schema))
    # Convert the Iceberg schema to a Arrow schema:
    init_arrow_schema_jvm = bodo_arrow_schema_utils_class.convert(init_iceberg_schema)
    # Convert the JVM Arrow schema to a PyArrow schema:
    pyarrow_schema = arrow_schema_j2py(init_arrow_schema_jvm)
    return convert_arrow_schema_to_large_types(pyarrow_schema)


def start_write(
    conn_str: str,
    db_name: str,
    table_name: str,
    table_loc: str,
    iceberg_schema_id: int,
    create_table_info,
    pa_schema: "pa.Schema",
    partition_spec: str | None,
    sort_order: str | None,
    mode: str,
):
    """
    Start a write action in an Iceberg catalog
        conn_str: Connection string to catalog
        db_name: Namespace containing the table written to
        table_name: Name of table written to
        table_loc: Warehouse location of data/ path (and files)
        iceberg_schema_id: Known Schema ID when files were written
        create_table_info: meta information about table and column comments
        pa_schema: Arrow Schema of written data. In the create/replace
            case, make sure to use 'get_schema_with_init_field_ids'
            to generate the correct field IDs for the fields before
            writing the parquet files.
        partition_spec: Iceberg-based partitioning schema of data
        sort_order: Iceberg-based sorting of data
        mode: Method of Iceberg write (`create`, `replace`, `append`)
    returns:
    txn_id: Transaction ID of the write action
    table_loc: Updated warehouse location of data/ path (and files)
    """
    catalog_type, _ = parse_conn_str(conn_str)
    handler = get_catalog(conn_str, catalog_type)

    table_comment = None
    column_comments = None
    table_properties = None

    if create_table_info is not None:
        table_comment = create_table_info.table_comment
        column_comments = create_table_info.column_comments
        table_properties = create_table_info.table_properties

    additional_properties = {}
    if table_comment is not None:
        additional_properties["comment"] = table_comment
    if table_properties is not None:
        for key_value in table_properties:
            additional_properties[key_value[0]] = key_value[1]

    if mode == "create":
        assert (
            iceberg_schema_id == -1
        ), "bodo_iceberg_connector Internal Error: Should never create existing table"
        try:
            txn_id = handler.startCreateOrReplaceTable(
                db_name,
                table_name,
                arrow_to_iceberg_schema(pa_schema, column_comments),
                False,
                convert_dict_to_java(additional_properties),
            )
        except Py4JError as e:
            print("Error during Iceberg table creation: ", e)
            return None

    elif mode == "replace":
        try:
            txn_id = handler.startCreateOrReplaceTable(
                db_name,
                table_name,
                arrow_to_iceberg_schema(pa_schema, column_comments),
                True,
                convert_dict_to_java(additional_properties),
            )
        except Py4JError as e:
            print("Error during Iceberg table replace: ", e)
            return None
    else:
        assert (
            mode == "append"
        ), "bodo_iceberg_connector Internal Error: Unknown write mode. Supported modes: 'create', 'replace', 'append'."
        assert iceberg_schema_id is not None

        try:
            txn_id = handler.startAppendTable(
                db_name, table_name, convert_dict_to_java(additional_properties)
            )
        except Py4JError as e:
            print("Error during Iceberg table append: ", e)
            return None

    # Refresh the table location.
    table_loc = normalize_data_loc(handler.getTransactionTableLocation(txn_id))

    return (
        txn_id,
        table_loc,
    )


def commit_write(
    txn_id: int,
    conn_str: str,
    db_name: str,
    table_name: str,
    table_loc: str,
    fnames: list[str],
    file_size_bytes: list[int],
    metrics: list[dict[str, Any]],
    iceberg_schema_id: int | None,
    mode: str,
):
    """
    Register a write action in an Iceberg catalog

    Args:
        txn_id: Transaction ID of the write action
        conn_str: Connection string to catalog
        db_name: Namespace containing the table written to
        table_name: Name of table written to
        table_loc: Warehouse location of data/ path (and files)
        fnames: Names of Parquet file that need to be committed in Iceberg
        file_size_bytes: Size of each file in bytes.
        metrics: Metrics about written data to include in commit
        iceberg_schema_id: Known Schema ID when files were written
        mode: Method of Iceberg write (`create`, `replace`, `append`)

    Returns:
        bool: Whether the action was successfully committed or not
    """
    catalog_type, _ = parse_conn_str(conn_str)
    handler = get_catalog(conn_str, catalog_type)
    file_info_str = process_file_infos(
        fnames, file_size_bytes, metrics, table_loc, db_name, table_name
    )

    if mode in ("create", "replace"):
        try:
            handler.commitCreateOrReplaceTable(txn_id, file_info_str)
        except Py4JError as e:
            print("Error during Iceberg table create/replace commit: ", e)
            return False

    else:
        assert (
            mode == "append"
        ), "bodo_iceberg_connector Internal Error: Unknown write mode. Supported modes: 'create', 'replace', 'append'."
        assert iceberg_schema_id is not None

        try:
            handler.commitAppendTable(txn_id, file_info_str, iceberg_schema_id)
        except Py4JError as e:
            print("Error during Iceberg table append: ", e)
            return False
    return True


def remove_transaction(
    transaction_id: int,
    conn_str: str,
    db_name: str,
    table_name: str,
):
    """Indicate that a transaction is no longer
    needed and can be remove from any internal state.
    This DOES NOT finalize or commit a transaction.

    Args:
        transaction_id (int): Transaction ID to remove.
        conn_str (str): Connection string for indexing into our object list.
        db_name (str): Name of the database for indexing into our object list.
        table_name (str): Name of the table for indexing into our object list.
    """
    catalog_type, _ = parse_conn_str(conn_str)
    handler = get_catalog(conn_str, catalog_type)
    handler.removeTransaction(transaction_id)


def fetch_puffin_metadata(
    transaction_id: int,
    conn_str: str,
    db_name: str,
    table_name: str,
):
    """Fetch the puffin file metadata that we need from the committed
    transaction to write the puffin file. These are the:
        1. Snapshot ID for the committed data
        2. Sequence Number for the committed data
        3. The Location at which to write the puffin file.

    Args:
        transaction_id (int): Transaction ID to remove.
        conn_str (str): Connection string for indexing into our object list.
        db_name (str): Name of the database for indexing into our object list.
        table_name (str): Name of the table for indexing into our object list.

    Returns:
        tuple[int, int, str]: Tuple of the snapshot ID, sequence number, and
        location at which to write the puffin file.
    """
    catalog_type, _ = parse_conn_str(conn_str)
    handler = get_catalog(conn_str, catalog_type)
    snapshot_id = handler.getTransactionSnapshotID(transaction_id)
    sequence_number = handler.getTransactionSequenceNumber(transaction_id)
    location = normalize_loc(
        handler.getTransactionStatisticFileLocation(transaction_id)
    )
    return snapshot_id, sequence_number, location


def commit_statistics_file(
    conn_str: str,
    db_name: str,
    table_name: str,
    snapshot_id: int,
    statistic_file_info: StatisticsFile,
):
    catalog_type, _ = parse_conn_str(conn_str)
    handler = get_catalog(conn_str, catalog_type)
    # Json encode the statistics file info
    statistic_file_info_str = json.dumps(asdict(statistic_file_info))
    handler.commitStatisticsFile(
        db_name, table_name, snapshot_id, statistic_file_info_str
    )


def commit_merge_cow(
    conn_str: str,
    db_name: str,
    table_name: str,
    table_loc: str,
    old_fnames: list[str],
    new_fnames: list[str],
    file_size_bytes: list[int],
    metrics: list[dict[str, Any]],
    snapshot_id: int,
):
    """
    Commit the write step of MERGE INTO using copy-on-write rules

    Args:
        conn_str: Connection string to Iceberg catalog
        db_name: Namespace / Database schema of table
        table_name: Name of Iceberg table to write to
        table_loc: Location of data/ folder for an Iceberg table
        old_fnames: List of old file paths to invalidate in commit
        new_fnames: List of written files to replace old_fnames
        file_size_bytes: Size of each file in bytes.
        metrics: Iceberg metrics for new_fnames
        snapshot_id: Expected current snapshot ID

    Returns:
        True if commit succeeded, False otherwise
    """

    catalog_type, _ = parse_conn_str(conn_str)
    old_fnames_java = convert_list_to_java(old_fnames)
    new_file_info_str = process_file_infos(
        new_fnames,
        file_size_bytes,
        metrics,
        table_loc,
        db_name,
        table_name,
    )

    try:
        handler = get_catalog(conn_str, catalog_type)
        handler.mergeCOWTable(
            db_name, table_name, old_fnames_java, new_file_info_str, snapshot_id
        )
    except Py4JError as e:
        # Note: Py4JError is the base class for all types of Py4j Exceptions.
        print("Error during Iceberg MERGE INTO COW:", e)
        return False

    return True


def delete_table(conn_str: str, db_name: str, table_name: str, purge: bool = True):
    """
    Delete an Iceberg table.
    args:
        conn_str: Iceberg connection string
        db_name: Database name
        table_name: Table name
        purge: If true, delete data/metadata files as well
    return: True if successful, False otherwise
    """
    catalog_type, _ = parse_conn_str(conn_str)
    handler = get_catalog(conn_str, catalog_type)
    try:
        return handler.deleteTable(db_name, table_name, purge)
    except Py4JError as e:
        print("Error during Iceberg table delete: ", e)
        return False


def get_table_metadata_path(conn_str: str, db_name: str, table_name: str):
    """
    Get the path to the current metadata file.
    """
    catalog_type, _ = parse_conn_str(conn_str)
    handler = get_catalog(conn_str, catalog_type)
    return normalize_loc(handler.getTableMetadataPath(db_name, table_name))
